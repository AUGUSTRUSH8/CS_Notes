# 集成学习方法

- [1Bagging模型](#1Bagging模型并行训练)
- [2boosting加权训练](#2boosting通过加权进行训练 )
- [3stacking聚合](#3stacking聚合多个分类或回归模型)

# 集成学习方法 Ensemble learning
- 集成学习通过建立几个模型组合来解决单一预测问题。他的工作原理就是生成多个分类器/模型。各自独立地学习和作出预测。这些预测最后结合成组合预测，因此优于任何一个单分类的作出预测
- 目的: 让机器学习效果更好

## 1Bagging模型并行训练
- bootstrap aggregation 并行训练一堆分类器
- 最典型代表: **随机森林 random forest simplified**
- 随机: 数据采样随机，特征选择随机
- 森林: 很多个决策树并行放在一起
- bagging: 训练多个分类器取平均   
![集成算法bagging](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E9%9B%86%E6%88%90%E7%AE%97%E6%B3%95bagging.png)

- 前人的经验，现在提到集成模型，可以默认为是**树模型**
- 理论上越多数的效果会越好，但是实际上基本超过一定数量就差不多上下浮动了

### 1.1为什么采用bootstrap抽样
随机有放回抽样 这样每一个训练集都是独有的
- 为什么要随机抽样训练集
    - 如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的
- 为什么要有放回的抽样
    - 如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是有偏的，都是绝对片面的，每棵树训练出来都是有很大的差异，而随机森林最后分类取决于多棵树(弱分类器)的投票表决

## 2boosting通过加权进行训练  
![集成算法boosting](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E9%9B%86%E6%88%90%E7%AE%97%E6%B3%95boosting.png)  
- 串行
- 典型代表: AdaBoost，Xgboost
- Adaboost会根据前一次的分类效果调整数据权重
    - 解释: 如果某个数据在这次分错了，那么在下一次会给他更大的权重
    - 最终的结果: 每个分类器根据自身的准确性来确定各自的权重，再合体

## 3stacking聚合多个分类或回归模型
- 可以分阶段来做
- 堆叠
- 可以堆叠各种各样的分类器(KNN,SVM,RF等等)
- 分阶段：第一阶段得出各自的结果，第二阶段再用前一阶段结果训练
- 堆叠在一起的确能使得准确率提升，但是很消耗时间

