# 朴素贝叶斯算法

## 1概率基础
- 概念定义为一件事情发生的可能性
- P(X):取值在[0,1]

## 2联合概率，条件概率与互相独立
- 联合概率:包含多个条件，且所有条件同时成立的概率
    - 记作:P(A,B)
- 条件概率:就是事件A在另外一个事件B已经发生的条件下发生的概率
    - 记作:P(A|B)
- 相互独立: 如果P(A,B)=P(A)P(B),则称事件A与事件B相互独立

## 3贝叶斯公式

![贝叶斯公式](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F.png)

## 4什么是朴素贝叶斯分类方法
- 朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法
    - **朴素**: 特征和特征之间是**相互独立** 贝叶斯: 贝叶斯公式
- 贝叶斯分类是一系列分类算法的总称,这类算法均以贝叶斯定理为基础,故统称为贝叶斯分类。朴素贝叶斯算法（Naive Bayesian) 是其中应用最为广泛的分类算法之一
- 朴素贝叶斯分类器基于一个简单的假定：给定目标值时属性之间**相互条件独立**,特征和特征之间是**相互独立**的。

## 5应用场景
- 文本分类
    - 单词作为特征，词和词之间是相互独立的
- 公式可以理解为:  
    - P(C|F1,F2...)=P(F1,F2...|C)P(C)/P(F1,F2...)
    - P(C):每个文档类别的概率(某文档类别数/总文档数量)
    - P(W|C):给定类别下特征(被预测文档中出现的词)的概率
        - 计算方法: P(F1|C)=Ni/N(训练文档中出现的次数)
        - N为所属类别C下的文档所有词出现的次数和
    - P(F1,F2...)预测文档中每个词的频率
- 如果计算两个类别概率比较:
    - 所以我们只要比较前面的大小就可以，得出谁的概率大

```
        文档ID   文档中的词                  属于c=China类
训练集    1      Chinese Beijing Chinese        yes
          2      Chinese Chinese Shanghai       yes
          3      Chinese Macao                  yes
          4      Tokyo Japan Chinese            No
测试集    5      Chinese Chinese Chinese Tokyo Janpan ？
P(C)=3/4  
P(非C)=1/4
P(C|Chinese，Chinese，Chinese，Tokyo，Japan) = P(Chinese，Chinese，Chinese，Tokyo，Japan|C)P(C)/P(Chinese，Chinese，Chinese，Tokyo，Japan)
## 因为贝叶斯算法下，特征和特征之间是相互独立的
## 出现0，样本量太少，需要引入拉普拉斯平滑系数
P(Chinese|C) = 5/8  P(Tokyo|C) = 0/8 P(Japan|C) =0/8
P(Chinese，Chinese，Chinese，Tokyo，Japan|C) = P(Chinese|C)^3*P(Tokyo|C)*P(Japan|C) = 

P(非C|Chinese，Chinese，Chinese，Tokyo，Japan) =  P(Chinese，Chinese，Chinese，Tokyo，Japan|非C)P(非C)/P(Chinese，Chinese，Chinese，Tokyo，Japan)

分母一样，只要比较分子
```
## 6**拉普拉斯平滑系数**
- 目的: 防止计算出的分类概率为0
- 原来公式为: P(F1|C)=(Ni)/(N)
- 拉普拉斯平滑系数之后: P(F1|C)=(Ni+α)/(N+αm)
    - α为指定系数一般为1，m为训练集文档中统计出的特征词个数
```
P(Chinese|C) = 5/8  P(Tokyo|C) = 0/8 P(Japan|C) =0/8
then
P(Chinese|C) = (5+1)/(8+1*6)=3/7  
P(Tokyo|C) = (0+1)/(8+1*6)=1/14 
P(Japan|C) =(0+1)/(8+1*6)=1/14
```
## 7API
- **sklearn.navie_bayes.MultinomialNB(alpha=)**
    - 朴素贝叶斯分类
    - alpha: l拉普拉斯平滑系数 默认1.0

## 8模型比较理论
- 最大似然: 最符合观测数据的(即p(D|h)最大的)最有优势
- 抛掷一个硬币，观察到的是'正'，根据最大似然估计，我们应该猜测这枚硬币抛出正的概率是1，因为这个才是能最大化p(D|h)的那个猜测
- 如果平面有N个点，我们可以用直线来拟合，也可以用二阶多项式拟合，也可以用三阶多项式，用N-1项多项式能够保证完美通过N个数据点
- 奥卡姆剃刀: p(h)较大的模型有较大的优势 
- 奥卡姆剃刀: 越是高阶的多项式越是不常见


## 9案例二十类新闻分类
- 获取数据  
    - 获取的是sklearn的数据，所以我们不需要处理
    - 新闻组数据集是大约20000个新闻组文档的集合，平均分布在20个不同的新闻组中。数据被组织成20个不同的新闻组，每个新闻组对应不同的主题。一些新闻组彼此之间关系密切，而另外一些新闻组则非常不相关
- 划分数据集
    - 特征值
    - 目标值
- 特征工程
    - 文本特征抽取
    - TFIDF进行的特征抽取
        - 将文章字符串进行单词抽取
- 朴素贝叶斯预估器预测
- 模型评估

```
获取数据
from sklearn.datasets import fetch_20newsgroups
# 划分数据集
from sklearn.model_selection import train_test_split
# 文本数据抽取 Tfidf
from sklearn.feature_extraction.text import TfidfVectorizer
# 朴素贝叶斯算法
from sklearn.naive_bayes import MultinomialNB

def nb_news():
    '''
    用朴素贝叶斯算法对新闻进行分类
    :return:
    '''
    # 获取数据
    # subset 默认是train，获取训练集；subset = all 获取所有数据
    news = fetch_20newsgroups(subset= "all")
    # 划分数据集
    x_train,x_test,y_train,y_test = train_test_split(news.data,news.target)
    # 特征工程：文本特征抽取 TFIDF方式
    transfer = TfidfVectorizer()
    x_train = transfer.fit_transform(x_train)
    x_test = transfer.transform(x_test)
    # 朴素贝叶斯预估器预测,默认的alpha为1.0
    estimator = MultinomialNB()
    estimator.fit(x_train,y_train)
    # 模型评估
    y_predict = estimator.predict(x_test)
    print("y_predict:\n", y_predict)
    print("对比真实值和预测值：", y_predict == y_test)
    score = estimator.score(x_test, y_test)
    print("准确率：\n", score)
    return None
    
output:
y_predict:
 [ 7 18 14 ..., 11 18 13]
直接比对真实值和预测值:
 [False  True  True ...,  True  True  True]
准确率为：
 0.850594227504

```
## 朴素贝叶斯算法总结
- 优点：
    - 朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率
    - 对缺失数据不太敏感，算法比较简单，常用于文本分类
    - 分类准确度高，速度快
- 缺点：
    - 由于使用类样本属性独立性的假设，**特征和特征之间独立**，所以如果特征属性有关联时其效果不好



