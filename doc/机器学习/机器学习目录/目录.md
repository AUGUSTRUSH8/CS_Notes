# 思维导图

![机器学习](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%AE%E5%BD%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.png)

![机器学习算法](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%AE%E5%BD%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AE%97%E6%B3%95.png)

# 目录

# 1[机器学习概述](https://github.com/mayu1031/CS_Notes/blob/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0.md)
# 2[数据集](https://github.com/mayu1031/CS_Notes/blob/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%95%B0%E6%8D%AE%E9%9B%86.md)
## 2.1sklearn包含的内容
- classification 分类
- regression 回归
- clustering 聚类
- dimensionality reduction 降维
- model selection 模型选择
- preprocessing 特征工程  

## 2.2sklearn数据集API
- sklearn.datasets
    - 加载获取流行数据集
- datasets.load_name()
    - 获取小规模数据集,数据包含在datasets里面
- datasets.fetch_name(data_home=None)
     - 获取大规模数据集，需要从网络上下载，函数第一个参数是data_home，表示数据集下载的目录，默认是~/scikit_learn_data/
    - 调用cd~
    - cd scikit_learn_data/
```python
from sklearn.datasets import load_iris
iris = load_iris()
```
## 2.3数据集划分API
- sklearn.model_selection.trian_test_split(arrays,options)
    - x 数据集的特征值
        - 训练集特征值:x_train
        - 测试集特征值:x_test
    - y 数据集的标签值
        - 训练集目标值:y_train
        - 测试集目标值:y_test
    - test_size测试集的大小，一般为float
    - random_state随机数种子，不同的种子会造成不同的随机采样结果。相同的种子采样结果相同
    - return:训练集特征值，测试集特征值，训练集目标值，测试集目标 
```python
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(iris.data,iris.target,test_size=0.2,random_state=22) # 默认是0.25
print('训练集的特征值：', x_train)
```

# 3[特征工程介绍](https://github.com/mayu1031/CS_Notes/blob/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%BB%8B%E7%BB%8D/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%BB%8B%E7%BB%8D.md)
# 4[特征抽取](https://github.com/mayu1031/CS_Notes/blob/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E6%8A%BD%E5%8F%96/%E7%89%B9%E5%BE%81%E6%8A%BD%E5%8F%96.md)

## 4.1特征提取API
- sklearn.feature_extraction 是一个类
- feature 特征
- extraction 提取

## 4.2字典特征提取API
- sklearn.feature_extraction.DictVectorizer(spars=True...) 默认sparse为True
    - vector 向量 矢量 可以用一维数组来存储向量
        - 矩阵matrix 计算机中用二维数组存储 矩阵可以看成由向量构成
    - Dict Vectorizer 字典vectorizer转换成向量的形式，告诉计算机把字典转出成数值了，可以把每一个样本理解为一个向量，n个样本就是n个向量，可以看成是一个二维数组，也可以理解为矩阵
    - 系数矩阵: 默认sparse=True 节省空间，节省内存，提高加载运行效率
    - 如果需要构造二维矩阵 data.toarray()
- 字典特征提取
    - 数据离散化效果
    - 对于特征当中存在类别信息的我们都会做one-hot编码处理，哑变量

## 4.3文本特征提取
### 4.3.1CountVectorizer API
- sklearn.feature_extraction.text.CountVectorizer(stop_words=[])
    - 返回词频矩阵 统计每个样本特征词出现的次数
    - stop_words 停用词，我们觉得某些词对最终分类没有用处 is/to，以列表的形式传递

### 4.3.2TfidfVectorizer API
- sklearn.feature_extraction.text.TfidfVectorizer(stop_words=None...)
    - 返回词的权重矩阵
        - TfidfVectorizer.fit_transform(x)
        - x:文本或者包含文本字符串的可迭代对象
        - 返回值: 返回sparse矩阵
    - TfidfVectorizer.inverse_transform(x)
        - x: array数组或者sparse矩阵
        - 返回值: 转换之前数据格式
    - TfidfVectorizer.get_feature_names()
        - 返回值：单词列表

# 5[特征预处理](https://github.com/mayu1031/CS_Notes/blob/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E9%A2%84%E5%A4%84%E7%90%86/%E7%89%B9%E5%BE%81%E9%A2%84%E5%A4%84%E7%90%86.md)

## 5.1特征预处理API
- sklearn.preprocessing

### 5.1.1归一化API
- sklearn.preprocessing.MinMaxScaler(feature_range=(0,1)...)
    - 处理之后，对每列来说，所有数据都聚集在feature_range之间
    - MinMaxScaler.fit_transform(x)
        - x: numpy array格式的数据[n_samples,n_features]
    - 返回值: 转换后的形状相同的array

### 5.1.2标准化API
- sklearn.preprocessing.StandardScaler()
    - 处理之后，对每列来说，所有数据都聚集在均值为0附近，标准差为1
    - StandardScaler.fit_transform(x)
        - x: numpy array格式的数据[n_samples,n_features]
    - 返回值: 转换后的形状相同的array

# 6[特征降维](https://github.com/mayu1031/CS_Notes/blob/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4PCA/%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4.md)
## 6.1特征选择
### 6.1.1Filter(过滤式)
#### 1低方差特征过滤API
- sklearn.feature_selection.VarianceThreshold(threshold=0.0)
    - threshold=0.0 设置一个临界值，低于临界值就删除，默认方差为0
    - 删除所有低方差特征

#### 2相关系数API
- from scipy.stats import pearsonr
    - x:(N,) array_like
    - y:(N,) array_like Return:(Pearson's correlation coefficient,p-value)

### 6.1.2Embedded(嵌入式)

## 6.2主成分分析PCA API
- sklearn.decomposition.PCA(n_components=None)
    - 将数据分解为较低维数空间
    - n_components:
        - 小数：表示保留百分之多少的信息
        - 整数：减少到多少特征

# 7[sklearn转换器和估计器](https://github.com/mayu1031/CS_Notes/blob/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/sklearn%E8%BD%AC%E6%8D%A2%E5%99%A8%E5%92%8C%E4%BC%B0%E8%AE%A1%E5%99%A8.md)

## 7.1转换器transformer

## 7.2预估器estimator

# 8[k-近邻算法 KNN算法](https://github.com/mayu1031/CS_Notes/blob/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/k%E8%BF%91%E9%82%BB/k-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%20KNN%E7%AE%97%E6%B3%95.md)

## 8K-近邻 API
- sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, algorithm='auto')
    - n_neighbors: int,可选(默认=5)，k值，k_neighbors 查询默认使用的邻居数
    - algorithm: {'auto','ball_tree','kd_tree','brute'},可选用于计算最近邻居的算法
        - ball_tree 使用BallTree
        - kd_tree 使用KDTree
        - auto 将尝试根据传递给fit方法的值来决定最合适的算法 不同现实方式影响效率

# 9[模型选择和调优cross validation/Grid Search](https://github.com/mayu1031/CS_Notes/blob/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E5%92%8C%E8%B0%83%E4%BC%98.md)
## 9.1GridSearchCV API
- sklearn.model_selection.GridSearchCV(estimator,param_grid=None,cv=None)
- metric='minkowski' 这里是距离调用: 明可夫斯基距离
    - p=1是曼哈顿距离，p=2是欧式距离
- 对估计器的指定参数进行详尽搜索
    - estimator:估计器对象
    - param_grid:估计器参数(dict)("n_neighbors":[1,3,5]) 准备好的k的取值，以字典的 形式传进来
    - cv:指定几折交叉验证 经过几次交叉验证 一般是10折
        - cv=3 训练集分成三分，其中一份测试，一共训练三次
    - fit():输入训练数据
        - estimator.fit(x_train,y_train)
    - predict():预测测试集结果
        - y_predict = estimator.predict(x_test)
        - print("y_predict:\n", y_predict)
    - score():准确率 准确率是test测试集中的结果
        - score = estimator.score(x_test,y_test)
        - print("准确率：\n",score)
- 结果分析:
    - 此最佳结果是训练集中验证集当中的结果
    - 最佳参数: best_params_
    - 最佳结果: best_score_
    - 最佳估计器: best_estimator_
    - 交叉验证结果: cv_results_

# 10[朴素贝叶斯算法](https://github.com/mayu1031/CS_Notes/blob/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95.md)
## 10.1朴素贝叶斯 API
- sklearn.navie_bayes.MultinomialNB(alpha=)
    - 朴素贝叶斯分类
    - alpha: l拉普拉斯平滑系数 默认1.0

# 11[决策树](https://github.com/mayu1031/CS_Notes/blob/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/%E5%86%B3%E7%AD%96%E6%A0%91.md)
## 11.1决策树API
- from sklearn import tree
- sklearn.tree.DecisionTreeClassifier(criterion='gini',max_depth=None,random_state=None)
决策树分类器
    - criterion:决策树的划分依据, 默认是'gini'系数，也可以选择信息增益的熵'entropy'
    - max_depth:数的深度大小                             
        - (如果分的过细，很有可能泛化能力比较差，在训练集上表现的很好，但是在测试集上表现就没那么好，此时可以设置下数的深 度大小，提高准确率)
    - random_state:随机数种子，是在任意带有随机性的类或函数里作为参数来控制随机模式。
- 1.criterion: gini or entropy决策树的划分依据, 默认是'gini'系数，也可以选择信息增益的熵'entropy'

- 2.splitter: best or random 前者是在所有特征中找最好的切分点 后者是在部分特征中（数据量大的时候）默认best

- 3.max_features: 默认是None（即所有），log2，sqrt，N 特征小于50的时候一般使用所有的

- 4.max_depth: 深度，数据少或者特征少的时候可以不管这个值，如果模型样本量多，特征也多的情况下，可以尝试限制下 max_depth=2 只选择最好的两个特征来建立模型

- 5.min_samples_split: 叶子节点样本数，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分；如果样本量不大，不需要管这个值，如果样本量数量级非常大，则推荐增大这个值

- 6.min_samples_leaf: 这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝，如果样本量不大，不需要管这个值，大些如10W可是尝试下5

- 7.min_weight_fraction_leaf 这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了

- 8.max_leaf_nodes 通过限制最大叶子节点数，可以防止过拟合，默认是"None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制具体的值可以通过交叉验证得到

- 9.class_weight 指定样本各类别的的权重，主要是为了防止训练集某些类别的样本过多导致训练的决策树过于偏向这些类别。这里可以自己指定各个样本的权重如果使用“balanced”，则算法会自己计算权重，样本量少的类别所对应的样本权重会高

- 10.min_impurity_split 这个值限制了决策树的增长，如果某节点的不纯度(基尼系数，信息增益，均方差，绝对差)小于这个阈值则该节点不再生成子节点。即为叶子节点

- 11.n_estimators:要建立树的个数

## 11.2决策树可视化API
- sklearn.tree.export_graphviz() 该函数能导出DOT格式
    - tree.export_graphviz(estimator,out_file='tree.dot',feature_names=[","])

```python
export_graphviz(estimator, out_file="iris_tree.dot", feature_names=iris.feature_names)
```
# 12[随机森林](https://github.com/mayu1031/CS_Notes/blob/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.md)
## 12.1随机森林API
- sklearn.ensemble.RandomForestClassifier(n_estimators=10,criterion='gini',max_depth=None,bootstrap=True,random_state=None,min_sample_split=2)
- n_estimators: integer,optional(default=10) 森林里面数目的数量
- criterion:string，可选(default='gini') 分割特征的测量方法
- max_depth: integer或None，可选(默认=无) 树的最大深度 5，8，15，25，30
- bootstrap: boolean,optional(default=True) 是否在构建树是放回抽样
- max_features='auto',每个决策树最大特征数量 m
    - if auto, then max_features=sqrt(n_features)
    - if sqrt, then max_features(n_features) same as auto
    - if log2 then max_features=log2(n_features)
    - if None, then max_features=n_features m=M 达不到降维的效果，每棵树的时间很长
- min_samples_split:节点划分最少样本数
- min_samples_leaf:叶子节点的最小样本数
- 超参数:n_estimators,max_depth,min_samples_split,min_samples_leaf

# 13[集成学习方法 Ensemble learning](https://github.com/mayu1031/CS_Notes/blob/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%20Ensemble%20learning.md)

# 14[线性回归](https://github.com/mayu1031/CS_Notes/blob/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.md)

## 14.1API优化算法
- from sklearn.linear_model

### 14.1.1LinearRegression正规方程优化
- sklearn.linear_model.LinearRegression(fit_intercept=True)
    - fit_intercept: 是否计算偏置
- LinearRegression.coef_:回归系数
- LinearRegression.intercept:偏重

### 14.1.2SGDRegressor随机梯度下降学习
- sklearn.linear_model.SGDRegressior(loss='squared_loss',fit_intercept=True,learning_rate='invscaling',eta0=0.01)
- 支持不同的loss函数和正则化惩罚来拟合线性回归模型
- loss:损失类型
    - loss='squared_loss':普通最小二乘法
- fit_intercept:是否计算偏值
- learning_rate:string,optional 默认为'invscaling'
    - 学习率填充/步长
    - 'constant':eta=eta() 默认的是0.01 学习率保持在一个数据不变
    - 'optimal':eta=1.0/(alpha*(t+t0))[default]
    - 'invscaling':eta=eta0/pow(t,power_t) 默认下降的步长invscaling
        - power_t=0.25:存在于父类当中
    - 对于一个常数值的学习率来说，可以使用learning_rate='constant', 并使用eta0来指定学习率
- SGDRegressor.coef_: 回归系数
- SGDRegressor.intercept_:偏置

# 15[模型评估:均方误差](https://github.com/mayu1031/CS_Notes/blob/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE.md)

## 15.1均方误差API
- sklearn.metrics.mean_squared_erroy(y_true,y_pred)
    - 均方误差回归损失
    - y_true:真实值
    - y_pred:预测值
    - return:浮点数结果

# 16[欠拟合和过拟合](https://github.com/mayu1031/CS_Notes/blob/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88.md)
## 16.1正则化类别

# 17[岭回归](https://github.com/mayu1031/CS_Notes/blob/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%B2%AD%E5%9B%9E%E5%BD%92/%E5%B2%AD%E5%9B%9E%E5%BD%92.md)

## 17.1岭回归API
- from sklearn.linear_model
- sklearn.linear_model.Ridge(alpha=1.0,fit_intercept=True,solver='auto',normalize=False)
    - 具有L2正则化的线性回归
    - alpha: 惩罚项系数，正则化力度，也叫λ
        - λ取值:0~1 1~10
    - fit_intercept=True选择是否添加偏置，添加使得模型更准确
    - solver:会根据数据自动化选择择优方式
        - SAG: 如果数据集，特征都比较大，选择该随机梯度下降优化
        - SGD：随机梯度下降 Stochastic gradient descent 一次迭代时只考虑一个训练样本，高效，容易实现，但是需要很多超参数，对于特征标准化是敏感的
        - Ridge方法相当于SGDRegressor(penalty='L2',loss='squared_loss'),只不过SGDRegressor实现了一个普通的随机梯度下降学习，带L2的线性回归，推荐使用Rideg(可以自动化实现类SAG)
            - 即线性回归添加了L2惩罚项，loss是最小二乘法
    - normalize:数据是否进行标准化
        - 如果是True，和在特征工程中先进行StandarScaler标准化数据是一样的，设置成True就不用再做标准化
        - 默认normalize=False: 可以在fit之前调用preprocessing.StandarScaler标准化数据
    - max_iter:迭代次数
- Ridge.coef_:回归权重
- Ridge.intercept_:回归偏置
- sklearn.linear_model.RidgeCV(BaseRidegCV,RegressorMixin)
    - 具有L2正则化的线性回归，可以进行交叉验证
    - coef_:回归系数

# 18[逻辑回归](https://github.com/mayu1031/CS_Notes/blob/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E4%BA%8C%E5%88%86%E7%B1%BB.md)

## 18.1逻辑回归API
- sklearn.linear_model.LogisticRegression(solver='liblinear',penalty='L2',C=1.0)
- 也在linear_model线性模型里面，我们对他进一步加工
    - solver: 优化求解方式(默认开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数)
    - sag:根据数据集自动选择，随机平均梯度下降SAG
    - pennality: 正则优化的种类 防止过拟合
    - C: 正则化力度
- 默认将类别数量少的当做正例
- LogisticRegression方法相当于SGDClassifier(loss='log',penalty='')(Classifier分类器)(线性回归随机梯度是SGDRegressor),SGDClassifier实现一个普通的随机梯度下降学习，也支持平均随机梯度下降法(ASGD),可以通过设置average=True，使用LogisticRegression可以直接实现了SAG，solver='sag'

# 19[模型评估:精确率 召回率 ROC曲线 AUC指标](https://github.com/mayu1031/CS_Notes/blob/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%B2%BE%E7%A1%AE%E7%8E%87%20%E5%8F%AC%E5%9B%9E%E7%8E%87%20%20ROC%E6%9B%B2%E7%BA%BF%20AUC%E6%8C%87%E6%A0%87.md)
## 19.1分类评估API
- sklearn.metrics.classification_report(y_ture,y_pred,label=[],target_names=None) (classification_report分类结果报告)
    - y_true:真实目标值
    - y_pred:估计器预测目标值
    - labels:指定类别对应的数字 一般正例为1，负例为0
    - target_names:目标类别名称 比如:[良性，恶性]
    - return:每个目标精确率和召回率

## 19.2AUC计算API
- from sklearn.metrics import roc_auc_score
- sklearn.metrics.roc_auc_score(y_true,y_score)
    - 计算ROC曲线面积，即AUC值
    - y_true: 每个样本的真实类别，必须为0(反例)，1(正例)标记
    - y_score：预测得分，可以是正类的预计概率，置信值或者分类器方法的返回值

# 20[Kmeans](https://github.com/mayu1031/CS_Notes/blob/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%81%9A%E7%B1%BB/kmeans.md)

## 20.1K-meanAPI
- sklearn.cluster.KMeans(n_clusters=8,init='k-means++')
- k-means聚类
- n_cluster:开始的聚类中心数量 k值
- init:初始化方法，默认为'k-means++'
- labels_:默认标记的类型，可以和真实值对比(不是值对比)

# 21[模型评估轮廓系数](https://github.com/mayu1031/CS_Notes/blob/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E8%BD%AE%E5%BB%93%E7%B3%BB%E6%95%B0.md)
## 21.1轮廓系数API
- sklearn.metrics.silhouette_score(X,labels)
    - 计算所有样本的平均轮廓系数
    - X:特征值
    - label:被聚类标记的目标值


# 22[关于python中的随机种子](https://github.com/mayu1031/CS_Notes/blob/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%85%B3%E4%BA%8Epython%E4%B8%AD%E7%9A%84%E9%9A%8F%E6%9C%BA%E7%A7%8D%E5%AD%90random_state.md)

# 23[模型保存和加载](https://github.com/mayu1031/CS_Notes/blob/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD.md)
## 23.1klearn模型的保存和加载API
- from sklearn.externals import joblib
    - 保存: joblib.dump(rf,'test_pkl')
        - dump序列化 将模型序列化到本地
    - 加载: estimator = joblib.load('test_pkl')
