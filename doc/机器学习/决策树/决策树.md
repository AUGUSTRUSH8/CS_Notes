## 决策树
### 认识决策树
- 决策树思想的来源非常朴素，程序设计中的条件分支结构就是if-else结构，最早的决策树就是利用这类结构分割数据的一种分类学习
- 如何高效的进行决策
- 确定快速的确定特征的先后顺序

### 决策树分类原理详解
- 信息论
    - 信息: 消除随机不定性的东西
    - 信息是物质、能量、信息及其属性的标示。
    - 信息是确定性的增加。
    - 信息是事物现象及其属性标识的集合。
- 信息熵
    - 来衡量我们消除的不确定的程度
    - H的专业术语称为信息熵，单位为比特bit
     - 公式:
     
![公式][1] 公式
  [1]:
https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/%E4%BF%A1%E6%81%AF%E7%86%B5.png
- 信息熵性质
    - 单调性，即发生概率越高的事件，其所携带的信息熵越低。极端案例就是“太阳从东方升起”，因为为确定事件，所以不携带任何信息量。从信息论的角度，认为这句话没有消除任何不确定性。
    - 非负性，即信息熵不能为负。这个很好理解，因为负的信息，即你得知了某个信息后，却增加了不确定性是不合逻辑的。
    - 累加性，即多随机事件同时发生存在的总不确定性的量度是可以表示为各事件不确定性的量度的和。

- 决策树的划分依据之一 信息增益
    - 特征A对训练数据集D的信息增益g(D,A), 定义为集合D的信息熵H(D)与特征A给定条件下D的信息条件熵H(D|A)之差。
    - 信息增益表示得知特征X的信息从而信息的不确定性减少的程度，使得总的信息熵减少的程度
    - g(D|A) = H(D) - H(D|A)
    - 公式:  
![公式][2]  
[2]:
https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/%E6%9D%A1%E4%BB%B6%E7%86%B5.png

### 银行贷款案例
- ![银行贷款案例][3]  
[3]:
https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/%E9%93%B6%E8%A1%8C%E8%B4%B7%E6%AC%BE%E6%95%B0%E6%8D%AE.png
- g(D|A) = H(D) - H(D|A)
         = H(D) - (5/15H(青年)+5/15H(中年)+5/15H(老年))
- H(D) = -(6/15*log(6/15)+9/15*log(9/15))
- H(青年) = -(3/5log(3/5)+2/5log(2/5))
- H(中年) = -(3/5log(3/5)+2/5log(2/5))
- H(老年) = -(4/5log(3/5)+1/5log(2/5))
- 以A1,A2,A3,A4代表年龄，工作，房子和贷款情况。最终结果g(D|A1)=0.313 g(D|A2)=0.324 g(D|A3)=0.420 g(D|A4)=0.363 从而我们选择A3房子作为划分的第一个特征。

当然决策树的原理不止信息增益这一种，还有其他方法
- ID3
    - 信息增益，最大准则
- C4.5
    - 信息增益比 最大准则
- CART
    - 分类树: 基尼系数 最小准则 在sklearn中可选择划分的默认原则
    - 优势: 划分更加细致
    

### 决策树API
- sklearn.tree.DecisionTreeClassifier(criterion='gini',max_depth=None,random_state=None)
    - 决策树分类器
    - criterion:决策树的划分依据, 默认是'gini'系数，也可以选择信息增益的熵'entropy'
    - max_depth:数的深度大小 (如果分的过细，很有可能泛化能力比较差，在训练集上表现的很好，但是在测试集上表现就没那么好，此时可以设置下数的深度大小，提高准确率)
    - random_state:随机数种子

```
# 导入数据集
from sklearn.datasets import load_iris
# 划分数据集
from sklearn.model_selection import train_test_split
# 决策树分类器
from sklearn.tree import DecisionTreeClassifier

def tree_iris():
    '''
    用决策树对鸢尾花进行分类
    :return:
    '''
    # 获取数据
    iris = load_iris()
    # 划分数据
    x_train,x_test,y_train,y_test = train_test_split(iris.data,iris.target,random_state=22)
    # 使用决策树预估器进行分类
    estimator = DecisionTreeClassifier(criterion="entropy")
    estimator.fit(x_train,y_train)
    # 模型评估
    estimator.predict(x_test)
    print("y_predict:\n", y_predict)
    print("直接比对真实值和预测值:\n", y_test == y_predict)

    # 方法2：计算准确率
    score = estimator.score(x_test, y_test)
    print("准确率为：\n", score)
    return None
    
output:
y_predict:
 [0 2 1 2 1 1 1 1 1 0 2 1 2 2 0 2 1 1 1 1 0 2 0 1 2 0 1 2 2 1 0 0 1 1 1 0 0
 0]
    tree_iris()
直接比对真实值和预测值:
 [ True  True  True  True  True  True  True False  True  True  True  True
  True  True  True  True  True  True False  True  True  True  True  True
  True  True False  True  True False  True  True  True  True  True  True
  True  True]
准确率为：
 0.894736842105
```
- 决策树模型计算出来的准确率为89.47%, 而之前用knn算法算出来的结果准确率为97%。实际上不同的算法有不同的应用环境，决策树更适合用于数据量比较大的数据集

### 决策树可视化API
- sklearn.tree.export_graphviz() 该函数能导出DOT格式
    - tree.export_graphviz(estimator,out_file='tree.dot',feature_names=[","])
```
export_graphviz(estimator, out_file="iris_tree.dot", feature_names=iris.feature_names)
```

```
digraph Tree {
node [shape=box] ;
0 [label="petal width (cm) <= 0.75\nentropy = 1.584\nsamples = 112\nvalue = [39, 37, 36]"] ;
1 [label="entropy = 0.0\nsamples = 39\nvalue = [39, 0, 0]"] ;
0 -> 1 [labeldistance=2.5, labelangle=45, headlabel="True"] ;
2 [label="petal width (cm) <= 1.75\nentropy = 1.0\nsamples = 73\nvalue = [0, 37, 36]"] ;
0 -> 2 [labeldistance=2.5, labelangle=-45, headlabel="False"] ;
3 [label="petal length (cm) <= 5.05\nentropy = 0.391\nsamples = 39\nvalue = [0, 36, 3]"] ;
2 -> 3 ;
4 [label="sepal length (cm) <= 4.95\nentropy = 0.183\nsamples = 36\nvalue = [0, 35, 1]"] ;
3 -> 4 ;
5 [label="petal width (cm) <= 1.35\nentropy = 1.0\nsamples = 2\nvalue = [0, 1, 1]"] ;
4 -> 5 ;
6 [label="entropy = 0.0\nsamples = 1\nvalue = [0, 1, 0]"] ;
5 -> 6 ;
7 [label="entropy = 0.0\nsamples = 1\nvalue = [0, 0, 1]"] ;
5 -> 7 ;
8 [label="entropy = 0.0\nsamples = 34\nvalue = [0, 34, 0]"] ;
4 -> 8 ;
9 [label="sepal length (cm) <= 6.05\nentropy = 0.918\nsamples = 3\nvalue = [0, 1, 2]"] ;
3 -> 9 ;
10 [label="entropy = 0.0\nsamples = 1\nvalue = [0, 1, 0]"] ;
9 -> 10 ;
11 [label="entropy = 0.0\nsamples = 2\nvalue = [0, 0, 2]"] ;
9 -> 11 ;
12 [label="petal length (cm) <= 4.85\nentropy = 0.191\nsamples = 34\nvalue = [0, 1, 33]"] ;
2 -> 12 ;
13 [label="entropy = 0.0\nsamples = 1\nvalue = [0, 1, 0]"] ;
12 -> 13 ;
14 [label="entropy = 0.0\nsamples = 33\nvalue = [0, 0, 33]"] ;
12 -> 14 ;
}
```
- 网站显示结构
    - webgraphviz.com/

### 决策树总结
- 优点: 简单的理解和解释，树木可视化
- 缺点:
    - 决策树学习者可能创建不能够很好的用来推广数据的过于复杂的数，这样会被称为过拟合
- 改进:
    - 减技cart算法(决策树API当中已经实现，随机森林参数调优有相关介绍)
    - 随机森林
    - 企业重要决策，由于决策树很好的分析能力，在决策过程应用较多，因为可以选择特征
