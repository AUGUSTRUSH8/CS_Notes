# 决策树

## 1什么是决策树

- 决策树: 从根节点开始一步步走到叶子节点
    - 所有的数据最终都会落到叶子节点，既可以做分类也可以做回归
    - 决策树思想的来源非常朴素，程序设计中的条件分支结构就是if-else结构
- 数的组成:
    - 根节点:第一个选择点
    - 非叶子节点与分支:中间过程
    - 叶子节点:最终的决策结果      
    
- 目标:通过一种衡量标准，来计算通过不同特征进行分支选择后的分类情况，找出来最好的那个当成根节点，以此类推
    - 高效的进行决策
    - 快速的确定特征的先后顺序
    

## 2决策树分类原理衡量标准

### 2.1信息熵
- 熵是表示随机变量不确定性的度量
    - H的专业术语称为信息熵，单位为比特bit
    - 不确定越大，得到的熵值也就越大
    - p(x)的范围在0-1之间，当p(x)=1时，logp(x)=0，H(x)=0，当p(x)=0时，H(x)=0，随机变量完全没有不确定性，在p(x)越接近1或0时，H(x)越接近0
    - 当p=0.5时，H(x)=1，此时随机变量的不确定性最大
    - 公式:
     
 ![公式](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/%E4%BF%A1%E6%81%AF%E7%86%B5.png)    

### 2.2信息熵性质
- 单调性，即发生概率越高的事件，其所携带的信息熵越低。极端案例就是“太 阳从东方升起”，因为为确定事件，所以不携带任何信息量。从信息论的角度，认为这句话没有消除任何不确定性。
- 非负性，即信息熵不能为负。即你得知了某个信息后，却增加了不确定性是不合逻辑的。
- 累加性，即多随机事件同时发生的总不确定性的量度是可以表示为各事件不确定性的量度的和。

### 2.3信息增益
- 决策树的划分依据之一
- 表示特征A使得数据集D的不确定性减少的程度
- 特征A对训练数据集D的信息增益g(D,A),定义为集合D的信息熵H(D)与特征A给定条件下D的信息条件熵H(D|A)之差。
    - 信息增益表示得知特征A的信息从而信息的不确定性减少的程度，使得总的信息熵减少的程度
    - g(D|A) = H(D) - H(D|A)
    - 公式:  
    
![公式](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/%E6%9D%A1%E4%BB%B6%E7%86%B5.png)  


## 3案例  
- 在历史数据中，14天里面有9天打球，5天不打球，此时的熵值:-(9/14)log2(9/14)-(5/14)long2(5/14)=0.940  
- 此数据中一共有4个特征，我们先分析outlook,outlook有sunny，overcast，rainy三种:
- sunny时，sunny有5天，其中出去两天，没出去三天: -(2/5)log2(2/5)-(3/5)log2(3/5)=0.971
- overcast 0
- rainy 0.971
- 此时的熵值: 0.971*(5/14)+0*(4/14)+0.971*(5/14)=0.693
- 信息增益: 0.940-0.693=0.247
- 另外三个特征的增加分别为0.029，0.152，0.048


## 4决策树分类原理其他方法
- ID3 
    - 信息增益，最大准则
- C4.5
    - 信息增益率 解决ID3问题，考虑自身熵信息 增益/自身熵值 最大准则
- CART
    - 分类树: GINI基尼系数 最小准则 在sklearn中可选择划分的默认原则
    - 优势: 划分更加细致  
    - 公式:  
    
![GINI基尼系数](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/GINI%E7%B3%BB%E6%95%B0.png)

## 5连续值划分
- 之前获取的数值都是离散值，或者按属性分；对于连续值最好的处理方法，是进行'二分'，找到离散点进行切分

## 6决策树剪枝策略
- 为什么要剪枝: 决策树过拟合风险很大，泛化能力太差，理论上可以完全分得开数据
- 剪枝策略: 预剪枝，后剪枝
- 预剪枝: 边建立决策树边进行剪枝的操作，限制参数 更实用
    - 限制深度，叶子节点个数，叶子节点样本数，信息增益量等
- 后剪枝: 当建立完决策树后来进行剪枝操作


## 7决策树API
- **from sklearn import tree**
- **sklearn.tree.DecisionTreeClassifier(criterion='gini',max_depth=None,random_state=None)**
    - 决策树分类器
    - criterion:决策树的划分依据, 默认是'gini'系数，也可以选择信息增益的熵'entropy'
    - max_depth:数的深度大小 (如果分的过细，很有可能泛化能力比较差，在训练集上表现的很好，但是在测试集上表现就没那么好，此时可以设置下数的深度大小，提高准确率)
    - random_state:随机数种子，是在任意带有随机性的类或函数里作为参数来控制随机模式。

## 8树模型参数：
- 1.**criterion: gini or entropy**决策树的**划分依据**, 默认是'gini'系数，也可以选择信息增益的熵'entropy'

- 2.splitter: best or random 前者是在所有特征中找最好的切分点 后者是在部分特征中（数据量大的时候）默认best

- 3.max_features: 默认是None（即所有），log2，sqrt，N 特征小于50的时候一般使用所有的

- 4.**max_depth**: **深度**，数据少或者特征少的时候可以不管这个值，如果模型样本量多，特征也多的情况下，可以尝试限制下 max_depth=2 只选择最好的两个特征来建立模型

- 5.**min_samples_split**: **叶子节点样本数**，如果某节点的**样本数**少于min_samples_split，则不会继续再尝试选择最优特征来进行划分；如果样本量不大，不需要管这个值，如果样本量数量级非常大，则推荐增大这个值

- 6.min_samples_leaf: 这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被**剪枝**，如果样本量不大，不需要管这个值，大些如10W可是尝试下5

- 7.min_weight_fraction_leaf 这个值限制了叶子节点所有样本**权重**和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了

- 8.max_leaf_nodes 通过**限制最大叶子节点数**，可以**防止过拟合**，默认是"None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制具体的值可以通过交叉验证得到

- 9.class_weight 指定样本**各类别的的权重**，主要是为了防止训练集某些类别的样本过多导致训练的决策树过于偏向这些类别。这里可以自己指定各个样本的权重如果使用“balanced”，则算法会自己计算权重，样本量少的类别所对应的样本权重会高

- 10.min_impurity_split 这个值限制了决策树的增长，如果某节点的不纯度(基尼系数，信息增益，均方差，绝对差)小于这个阈值则该节点不再生成子节点。即为叶子节点 

- 11.n_estimators:要建立**树的个数**

## 9鸢尾花案例

```python
# 导入数据集
from sklearn.datasets import load_iris
# 划分数据集
from sklearn.model_selection import train_test_split
# 决策树分类器
from sklearn.tree import DecisionTreeClassifier

def tree_iris():
    '''
    用决策树对鸢尾花进行分类
    :return:
    '''
    # 获取数据
    iris = load_iris()
    # 划分数据
    x_train,x_test,y_train,y_test = train_test_split(iris.data,iris.target,random_state=22)
    # 使用决策树预估器进行分类
    estimator = DecisionTreeClassifier(criterion="entropy")
    estimator.fit(x_train,y_train)
    # 模型评估
    estimator.predict(x_test)
    print("y_predict:\n", y_predict)
    print("直接比对真实值和预测值:\n", y_test == y_predict)

    # 方法2：计算准确率
    score = estimator.score(x_test, y_test)
    print("准确率为：\n", score)
    return None
    
output:
y_predict:
 [0 2 1 2 1 1 1 1 1 0 2 1 2 2 0 2 1 1 1 1 0 2 0 1 2 0 1 2 2 1 0 0 1 1 1 0 0
 0]
    tree_iris()
直接比对真实值和预测值:
 [ True  True  True  True  True  True  True False  True  True  True  True
  True  True  True  True  True  True False  True  True  True  True  True
  True  True False  True  True False  True  True  True  True  True  True
  True  True]
准确率为：
 0.894736842105
```
- 决策树模型计算出来的准确率为89.47%, 而之前用knn算法算出来的结果准确率为97%。实际上不同的算法有不同的应用环境，决策树更适合用于数据量比较大的数据集

## 10决策树可视化API
- sklearn.tree.export_graphviz() 该函数能导出DOT格式
    - tree.export_graphviz(estimator,out_file='tree.dot',feature_names=[","])
```python
export_graphviz(estimator, out_file="iris_tree.dot", feature_names=iris.feature_names)
```
```
output:

digraph Tree {
node [shape=box] ;
0 [label="petal width (cm) <= 0.75\nentropy = 1.584\nsamples = 112\nvalue = [39, 37, 36]"] ;
1 [label="entropy = 0.0\nsamples = 39\nvalue = [39, 0, 0]"] ;
0 -> 1 [labeldistance=2.5, labelangle=45, headlabel="True"] ;
2 [label="petal width (cm) <= 1.75\nentropy = 1.0\nsamples = 73\nvalue = [0, 37, 36]"] ;
0 -> 2 [labeldistance=2.5, labelangle=-45, headlabel="False"] ;
3 [label="petal length (cm) <= 5.05\nentropy = 0.391\nsamples = 39\nvalue = [0, 36, 3]"] ;
2 -> 3 ;
4 [label="sepal length (cm) <= 4.95\nentropy = 0.183\nsamples = 36\nvalue = [0, 35, 1]"] ;
3 -> 4 ;
5 [label="petal width (cm) <= 1.35\nentropy = 1.0\nsamples = 2\nvalue = [0, 1, 1]"] ;
4 -> 5 ;
6 [label="entropy = 0.0\nsamples = 1\nvalue = [0, 1, 0]"] ;
5 -> 6 ;
7 [label="entropy = 0.0\nsamples = 1\nvalue = [0, 0, 1]"] ;
5 -> 7 ;
8 [label="entropy = 0.0\nsamples = 34\nvalue = [0, 34, 0]"] ;
4 -> 8 ;
9 [label="sepal length (cm) <= 6.05\nentropy = 0.918\nsamples = 3\nvalue = [0, 1, 2]"] ;
3 -> 9 ;
10 [label="entropy = 0.0\nsamples = 1\nvalue = [0, 1, 0]"] ;
9 -> 10 ;
11 [label="entropy = 0.0\nsamples = 2\nvalue = [0, 0, 2]"] ;
9 -> 11 ;
12 [label="petal length (cm) <= 4.85\nentropy = 0.191\nsamples = 34\nvalue = [0, 1, 33]"] ;
2 -> 12 ;
13 [label="entropy = 0.0\nsamples = 1\nvalue = [0, 1, 0]"] ;
12 -> 13 ;
14 [label="entropy = 0.0\nsamples = 33\nvalue = [0, 0, 33]"] ;
12 -> 14 ;
}
```
- 网站显示结构
    - webgraphviz.com/

## 11模型选择和调优
### 11.1超参数搜索-网格搜索(Grid Search)
超参数 通常情况下，参数是需要手动指定的(如k-近邻算法中的k值)，这种叫超参数  

但是手动过程繁杂，所以需要对模型预设几种超参数组合。每组超参数都采用交叉验证来进行评估。最后选出最优参数组合建立模型。
```
k值   k=3   k=5   k=7
模型 模型1 模型2 模型3
```
### 11.2模型选择与调优API
- **sklearn.model_selection.GridSearchCV(estimator,param_grid=None,cv=None)**
- 对估计器的指定参数进行详尽搜索
    - estimator:估计器对象
    - param_grid:估计器参数(dict)("n_neighbors":[1,3,5]) 准备好的k的取值，以字典的 形式传进来
    - cv:指定几折交叉验证 经过几次交叉验证 一般是10折
        - cv=3 训练集分成三分，其中一份测试，一共训练三次
- fit():输入训练数据
    - estimator.fit(x_train,y_train)
- predict():预测测试集结果
    - y_predict = estimator.predict(x_test)
    - print("y_predict:\n", y_predict)
- score():准确率
    - score = estimator.score(x_test,y_test)
    - print("准确率：\n",score)
- 结果分析:
    - 最佳参数: best_params_
    - 最佳结果: best_score_
    - 最佳估计器: best_estimator_
    - 交叉验证结果: cv_results_
    - p=1是曼哈顿距离，p=2是欧式距离
    

## 12案例网格搜索(Grid Search)
```python
# 模型调优与选择
# 超参数搜索-网格搜索(Grid Search) GridSearchCV
# param_grid 为参数；参数候选项写成字典的格式
# cv 进行几次交叉验证

from sklearn.grid_search import GridSearchCV
tree_param_grid = { 'min_samples_split': list((3,6,9)),'n_estimators':list((10,50,100))}
grid = GridSearchCV(RandomForestRegressor(),param_grid=tree_param_grid, cv=5)
grid.fit(x_train, y_train)
grid.grid_scores_, grid.best_params_, grid.best_score_
```
```
output:
([mean: 0.78405, std: 0.00505, params: {'min_samples_split': 3, 'n_estimators': 10},
  mean: 0.80529, std: 0.00448, params: {'min_samples_split': 3, 'n_estimators': 50},
  mean: 0.80673, std: 0.00433, params: {'min_samples_split': 3, 'n_estimators': 100},
  mean: 0.79016, std: 0.00124, params: {'min_samples_split': 6, 'n_estimators': 10},
  mean: 0.80496, std: 0.00491, params: {'min_samples_split': 6, 'n_estimators': 50},
  mean: 0.80671, std: 0.00408, params: {'min_samples_split': 6, 'n_estimators': 100},
  mean: 0.78747, std: 0.00341, params: {'min_samples_split': 9, 'n_estimators': 10},
  mean: 0.80481, std: 0.00322, params: {'min_samples_split': 9, 'n_estimators': 50},
  mean: 0.80603, std: 0.00437, params: {'min_samples_split': 9, 'n_estimators': 100}],
 {'min_samples_split': 3, 'n_estimators': 100},
 0.8067250881273065)
```

- 带入min_samples_split': 3, 'n_estimators': 100测试
```python
rfr = RandomForestRegressor( min_samples_split=3,n_estimators = 100,random_state = 42)
rfr.fit(data_train, target_train)
rfr.score(data_test, target_test)
```
```
output:
0.80908290496531576 
```

## 13流程分析
- 获取数据
- 数据处理
    - 特征值 x
    - 目标值 y
- 特征工程：标准化
- 算法预估流程
- 模型选择与调优
- 模型评估
