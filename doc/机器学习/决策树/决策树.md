# 决策树

## 1什么是决策树
- 决策树思想的来源非常朴素，程序设计中的条件分支结构就是if-else结构，最早的决策树就是利用这类结构分割数据的一种分类学习  

- 决策树: 从根节点开始一步步走到叶子节点
    - 所有的数据最终都会落到叶子节点，既可以做分类也可以做回归
- 数的组成:
    - 根节点:第一个选择点
    - 非叶子节点与分支:中间过程
    - 叶子节点:最终的决策结果      
    
- 目标:通过一种衡量标准，来计算通过不同特征进行分支选择后的分类情况，找出来最好的那个当成根节点，以此类推
    - 高效的进行决策
    - 快速的确定特征的先后顺序
    
## 2决策树分类原理衡量标准

### 2.1信息熵
- 熵是表示随机变量不确定性的度量
    - H的专业术语称为信息熵，单位为比特bit
    - 不确定越大，得到的熵值也就越大
    - p(x)的范围在0-1之间，当p(x)=1时，logp(x)=0，H(x)=0，当p(x)=0时，H(x)=0，随机变量完全没有不确定性，在p(x)越接近1或0时，H(x)越接近0
    - 当p=0.5时，H(x)=1，此时随机变量的不确定性最大
    - 公式:
     
 ![公式](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/%E4%BF%A1%E6%81%AF%E7%86%B5.png)    

### 2.2信息熵性质
- 单调性，即发生概率越高的事件，其所携带的信息熵越低。极端案例就是“太 阳从东方升起”，因为为确定事件，所以不携带任何信息量。从信息论的角度，认为这句话没有消除任何不确定性。
- 非负性，即信息熵不能为负。即你得知了某个信息后，却增加了不确定性是不合逻辑的。
- 累加性，即多随机事件同时发生的总不确定性的量度是可以表示为各事件不确定性的量度的和。

### 2.3信息增益
- 决策树的划分依据之一
- 表示特征A使得数据集D的不确定性减少的程度
- 特征A对训练数据集D的信息增益g(D,A),定义为集合D的信息熵H(D)与特征A给定条件下D的信息条件熵H(D|A)之差。
    - 信息增益表示得知特征A的信息从而信息的不确定性减少的程度，使得总的信息熵减少的程度
    - g(D|A) = H(D) - H(D|A)
    - 公式: 
![公式](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/%E6%9D%A1%E4%BB%B6%E7%86%B5.png)  


## 3案例  
- 在历史数据中，14天里面有9天打球，5天不打球，此时的熵值: -(9/14)log2(9/14)-(5/14)long2(5/14)=0.940  
- 此数据中一共有4个特征，我们先分析outlook,outlook有sunny，overcast，rainy三种:
- sunny时，sunny有5天，其中出去两天，没出去三天: -(2/5)log2(2/5)-(3/5)log2(3/5)=0.971
- overcast 0
- rainy 0.971
- 此时的熵值: 0.971*(5/14)+0*(4/14)+0.971*(5/14)=0.693
- 信息增益: 0.940-0.693=0.247
- 另外三个特征的增加分别为0.029，0.152，0.048

## 3决策树分类原理其他方法
- ID3 
    - 信息增益，最大准则
- C4.5
    - 信息增益率 解决ID3问题，考虑自身�熵信息 增益/自身熵值 最大准则
- CART
    - 分类树: GINI基尼系数 最小准则 在sklearn中可选择划分的默认原则
    - 优势: 划分更加细致
    - ![GINI基尼系数](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/GINI%E7%B3%BB%E6%95%B0.png)

## 4连续值划分
- 之前获取的数值都是离散值，或者按属性分；对于连续值最好的处理方法，是进行'二分'，找到离散点进行切分

## 5决策树剪枝策略
- 为什么要剪枝: 决策树过拟合风险很大，理论上可以完全分得开数据
- 剪枝策略: 预剪枝，后剪枝
- 预剪枝: 边建立决策树边进行剪枝的操作
- 后剪枝: 当建立完决策树后来进行剪枝操作


## 决策树API
- sklearn.tree.DecisionTreeClassifier(criterion='gini',max_depth=None,random_state=None)
    - 决策树分类器
    - criterion:决策树的划分依据, 默认是'gini'系数，也可以选择信息增益的熵'entropy'
    - max_depth:数的深度大小 (如果分的过细，很有可能泛化能力比较差，在训练集上表现的很好，但是在测试集上表现就没那么好，此时可以设置下数的深度大小，提高准确率)
    - random_state:随机数种子

```
# 导入数据集
from sklearn.datasets import load_iris
# 划分数据集
from sklearn.model_selection import train_test_split
# 决策树分类器
from sklearn.tree import DecisionTreeClassifier

def tree_iris():
    '''
    用决策树对鸢尾花进行分类
    :return:
    '''
    # 获取数据
    iris = load_iris()
    # 划分数据
    x_train,x_test,y_train,y_test = train_test_split(iris.data,iris.target,random_state=22)
    # 使用决策树预估器进行分类
    estimator = DecisionTreeClassifier(criterion="entropy")
    estimator.fit(x_train,y_train)
    # 模型评估
    estimator.predict(x_test)
    print("y_predict:\n", y_predict)
    print("直接比对真实值和预测值:\n", y_test == y_predict)

    # 方法2：计算准确率
    score = estimator.score(x_test, y_test)
    print("准确率为：\n", score)
    return None
    
output:
y_predict:
 [0 2 1 2 1 1 1 1 1 0 2 1 2 2 0 2 1 1 1 1 0 2 0 1 2 0 1 2 2 1 0 0 1 1 1 0 0
 0]
    tree_iris()
直接比对真实值和预测值:
 [ True  True  True  True  True  True  True False  True  True  True  True
  True  True  True  True  True  True False  True  True  True  True  True
  True  True False  True  True False  True  True  True  True  True  True
  True  True]
准确率为：
 0.894736842105
```
- 决策树模型计算出来的准确率为89.47%, 而之前用knn算法算出来的结果准确率为97%。实际上不同的算法有不同的应用环境，决策树更适合用于数据量比较大的数据集

## 决策树可视化API
- sklearn.tree.export_graphviz() 该函数能导出DOT格式
    - tree.export_graphviz(estimator,out_file='tree.dot',feature_names=[","])
```
export_graphviz(estimator, out_file="iris_tree.dot", feature_names=iris.feature_names)
```

```
digraph Tree {
node [shape=box] ;
0 [label="petal width (cm) <= 0.75\nentropy = 1.584\nsamples = 112\nvalue = [39, 37, 36]"] ;
1 [label="entropy = 0.0\nsamples = 39\nvalue = [39, 0, 0]"] ;
0 -> 1 [labeldistance=2.5, labelangle=45, headlabel="True"] ;
2 [label="petal width (cm) <= 1.75\nentropy = 1.0\nsamples = 73\nvalue = [0, 37, 36]"] ;
0 -> 2 [labeldistance=2.5, labelangle=-45, headlabel="False"] ;
3 [label="petal length (cm) <= 5.05\nentropy = 0.391\nsamples = 39\nvalue = [0, 36, 3]"] ;
2 -> 3 ;
4 [label="sepal length (cm) <= 4.95\nentropy = 0.183\nsamples = 36\nvalue = [0, 35, 1]"] ;
3 -> 4 ;
5 [label="petal width (cm) <= 1.35\nentropy = 1.0\nsamples = 2\nvalue = [0, 1, 1]"] ;
4 -> 5 ;
6 [label="entropy = 0.0\nsamples = 1\nvalue = [0, 1, 0]"] ;
5 -> 6 ;
7 [label="entropy = 0.0\nsamples = 1\nvalue = [0, 0, 1]"] ;
5 -> 7 ;
8 [label="entropy = 0.0\nsamples = 34\nvalue = [0, 34, 0]"] ;
4 -> 8 ;
9 [label="sepal length (cm) <= 6.05\nentropy = 0.918\nsamples = 3\nvalue = [0, 1, 2]"] ;
3 -> 9 ;
10 [label="entropy = 0.0\nsamples = 1\nvalue = [0, 1, 0]"] ;
9 -> 10 ;
11 [label="entropy = 0.0\nsamples = 2\nvalue = [0, 0, 2]"] ;
9 -> 11 ;
12 [label="petal length (cm) <= 4.85\nentropy = 0.191\nsamples = 34\nvalue = [0, 1, 33]"] ;
2 -> 12 ;
13 [label="entropy = 0.0\nsamples = 1\nvalue = [0, 1, 0]"] ;
12 -> 13 ;
14 [label="entropy = 0.0\nsamples = 33\nvalue = [0, 0, 33]"] ;
12 -> 14 ;
}
```
- 网站显示结构
    - webgraphviz.com/

## 决策树总结
- 优点: 简单的理解和解释，树木可视化
- 缺点:
    - 决策树学习者可能创建不能够很好的用来推广数据的过于复杂的数，这样会被称为过拟合
- 改进:
    - 减技cart算法(决策树API当中已经实现，随机森林参数调优有相关介绍)
    - 随机森林
    - 企业重要决策，由于决策树很好的分析能力，在决策过程应用较多，因为可以选择特征
