# 什么是机器学习
## 定义
- 机器学习是从数据中自动分析获取模型，并利用模型对未知数据进行预测

## 解释
- 我们人从大量的日常经验中归纳规律，在面临新的问题的时候，可以利用以往总结的规律去分析现实状况，采取最佳策略

## 数据集的构成
- 我们把机器学习要学习的数据称为数据集
- 对于每一行数据我们称之为样本
- 数据集：数据具有特征，目标值为预测结果
- 有些数据集可以没有目标值

### 可用数据集
- kaggle
- UCI数据集
- scikit-learn
### sklearn数据集
- sklearn.datasets
    - load_* 获取小规模数据集
    - fetch_* 获取大规模数据集
        - sklearn.datasets.fetch_name(data_home=None,subset='train')
            - subset：train/test/all

- sklearn数据集返回值介绍
    - load和fetch返回的数据类型都是datasets.base.Bunch 继承自字典
        - data：特征值，特征数据数组，是[n_samples*n_features]的二维
            numpy.ndarray 数组
        - target：目标值，标签数组，是n_samples的一维numpy.ndarray数组
        - DESCR: 数据集描述
        - feature_names: 特征值名字，新闻数据，手写数字，回归数据集没有
        - target_names: 目标值标签名
    - dict['key'] = values 用字典键值对获得数据
      bunch.key = values
```
from sklearn.datasets import load_iris
def datasets_demo():
    '''
    sklearn数据集使用
    :return:
    '''
    # 获取数据集
    iris = load_iris()
    print("鸢尾花数据集：\n", iris)
    print("查看数据集的描述：\n",iris["DESCR"])
    print("查看特征值的名字:\n" , iris.feature_names)
    print("查看特征值：\n", iris.data, iris.data.shape)
    # 数据集划分  特征值，目标值
    x_train,x_test,y_train,y_test = train_test_split(iris.data, iris.target,test_size=0.2)
    print(x_train,x_train.shape)
    return None
```
     
### 数据集的划分
- 机器学习一般的数据集会划分成两个部分：
    - 训练数据：用于训练，构建模型 70%~80%
    - 测试数据: 在模型检验的时使用，用于评估模型是否有效 20%~30%
- 数据集划分api
    - sklearn.model_selection.trian_test_split(arrays,*options)
    - x 数据集的特征值
        - 训练集特征值 x_train
        - 测试集特征值 x_test
    - y 数据集的标签值
        - 训练集目标值 y_train
        - 测试集目标值 y_test
    - test_size测试集的大小，一般为float
    - random_state随机数种子，不同的种子会造成不同的随机采样结果。相同的种子采样结果相同
    - return 训练集特征值，测试集特征值，训练集目标值，测试集目标值
### 鸢尾花数据集
- 特征值： 4个特征，花瓣的长度和宽度，花萼的长度，宽度
- 目标值： setosa vericolor virginica
- 数据集划分之后，特征值的数量不变还是四个特征，但是样本的数量变了


## sklearn转换器和估计器
### 转换器
- 转换器是特征工程的父类，所有在进行特征工程的时候都是继承自transformer这个类
- 之前做特征工程的步骤
    - 1.实例化 实例化一个转换器类 transformer
    - 2.调用fit_transform（对于文档建立分类词频矩阵，不能同时调用）
    - 我们把特征工程的家口称之为转换器，其中转换器调用有着几个种形式
        - fit_transform()
        - fit() 
        - transform()
    - 标准化公式:(x-mean)/std
        - fit_transform()
        - fit() 计算每一列的平均值和标准差
        - transform() 带入公式(x-mean)/std 进行最终的转换
        
### 预估器
- sklearn机器学习算法的实现都被封装到估计器这个父类当中
- 在sklearn中，估计器estimator是一个重要的角色，是一类实现了算法的API；所有的估计器都是estimator的子类
- 用于分类的估计器：
    - sklearn.neighbors k-近邻
    - sklearn.naive_bayes 贝叶斯
    - sklearn.linear_model.LogisticRegression 逻辑回归
    - sklearn.tree 决策树与随机森林
- 用于回归的估计器
    - sklearn.linear_model.linearRegression 线性回归
    - sklearn.linear_model.Ridge 岭回归
- 用于无监督学习的估计器
    - sklearn.cluster.KMeans 聚类

## 特征工程介绍
- 什么是特征工程
    - 是是用专业背景知识和技巧处理数据，能得特征能在机器学习算法上发挥更好的作用的过程
    - 目前用的工具 sklearn
- 为什么需要特征工程 Feature Engineering
    - 数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已
- 影响效果的原因
    - 算法
    - 特征工程
- 特征工程的位置与数据处理的比较
    - pandas：一个数据读取非常方便以及基本的处理格式的工具 用于数据清洗，数据处理（缺失值处理）
    - sklearn： 对于特征的处理特供了强大的接口 用于特征工程
- 特征工程包含内容
    - 特征抽取
    - 特征预处理
    - 特征降维

## 机器学习算法分类
- 监督学习 supervised learning(预测)
    - 定义：输入数据是由输入特征值和目标值所组成。函数的输出可以是一个连续的值（称为回归），或是输出的是有限的离散值（称为分类），
    - 分类问题 回归 输入数据有特征有标签，即有标准答案
        - 目标值：结果为类别
        - k-近邻算法，朴素贝叶斯分类，决策树，随机森林，逻辑回归
    - 回归问题
        - 目标值:连续型的数据
        - 线性回归，岭回归
- 无监督学习 unsupervised learning
    - 定义：输入数据是由输入特征值组成
    - 目标值：没有目标值 输入数据有特征无标签，即无标准答案    
        - 聚类 k-means

## 机器学习开发流程
- 获取数据 原始数据
- 数据处理
- 特征工程
- 使用机器学习算法训练 得到模型
- 模型评估

## 学习框架
- 重点  
    - 算法是核心，数据与计算是基础
- 大部分复杂模型的算法设计部都是算法工程师在做，而我们
    - 分析很多数据
    - 分析具体业务
    - 应用常见的算法
    - 特征工程，调参数，优化
        


# 特征工程

## 特征工程介绍
- 什么是特征工程
    - 是是用专业背景知识和技巧处理数据，能得特征能在机器学习算法上发挥更好的作用的过程
    - 目前用的工具 sklearn
- 为什么需要特征工程 Feature Engineering
    - 数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已
- 影响效果的原因
    - 算法
    - 特征工程
- 特征工程的位置与数据处理的比较
    - pandas：一个数据读取非常方便以及基本的处理格式的工具 用于数据清洗，数据处理（缺失值处理）
    - sklearn： 对于特征的处理特供了强大的接口 用于特征工程
- 特征工程包含内容
    - 特征抽取
    - 特征预处理
    - 特征降维


## 特征抽取
- 特征提取
- 机器学习算法 
    - 统计方法, 就是数学公式，但是不能处理字符串，就需要将文本类型的字符串转成成数值类型，即需要特征抽取
        - 文本类型转化成数值
        - 类别，类型转成数值，转换成one-hot编码，哑变量
- 为什么需要特征提取：
    - 将任意数据（如文本或图像）转换成可用于机器学习的数字特征（特征值化）
- 特征提取API
    - sklearn.feature_extraction 是一个类
        - feature 特征
        - extraction 提取
    - 不同类型的数据有不同的转换方法
        - 字典特征提取（特征离散化）
        - 文本特征提取
        - 图像特征提取（深度学习）
     

         
### 字典特征提取  

对字典数据进行特征值化

- sklearn.feature_extraction.DictVectorizer(spars=True...) 默认sparse为True
    - vector 向量 矢量 可以用一维数组来存储向量
        - 矩阵matrix 计算机中用二维数组存储 矩阵可以看成由向量构成
    - Dict Vectorizer 字典vectorizer转换成向量的形式，告诉计算机把字典转出成数值了，可以把每一个样本理解为一个向量，n个样本就是n个向量，可以看成是一个二维数组，也可以理解为矩阵
    - 调用了DictVectorizer，相当于实例化了一个转换器对象，父类是一个transfer，转换器类，其中一个方法就是把字典换成成数值  
    - 调用实例化好的对象DictVectorizer，里面有个方法叫fit_transform
    - DictVectorizer.fit_transform(x) x:字典或者包含字典的迭代器   返回值：返回sparse矩阵 稀疏
        - 稀疏矩阵将非0值按位置表现出来，节省内存，可以提高加载运行效率
    - DictVectorizer.fit_transform(x) x:array数组或者sparse矩阵    返回值：转换之前数据格式
        - DictVectorizer.get_feature_names() 返回类别名称
      
字典特征提取
- 对于特征当中存在类别信息的我们都会做one-hot编码处理，哑变量

应用数据：  
字典：  
```
data = [{'city': '北京', 'temperature': 100}, 
        {'city': '上海', 'temperature': 60}, 
        {'city': '深圳', 'temperature': 30}]
output:      
           [[0,1,0,100]
            [1,0,0,60]
            [0,0,1,30]]
```

每一个样本理解为一个向量，一共三行。三个样本，两个特征，是一个三行两列的二维数组/矩阵
原来每个样本有两个特征，再进行转化之后，字典特征抽取之后，样本量不变，特征数量变成了4个。当特征中有类别的时候，表示方法时字符串，想表示为数值并且数值没有比大小，我们就使用ont-hot变量，哑变量。
```
from sklearn.feature_extraction import DictVectorizer
def dict_demo():
    '''
    字典特征抽取
    :return:
    '''
    data = [{'city': '北京', 'temperature': 100}, {'city': '上海', 'temperature': 60}, {'city': '深圳', 'temperature': 30}]
    #1 实例化一个转换器类
    # sparse 稀疏矩阵
    transform = DictVectorizer(sparse=False)
    # 2 调用fit_transform()方法，里面传字典或者包含字典的迭代器
    data_new = transform.fit_transform(data)
    print("data_new:\n", data_new)
    print("特征名字：",transform.get_feature_names())
    return None
```
#### 字典提取应用场景
- 当我们面对数据集中有类别特征比较多
    - 将数据集的特征转换成字典类型
    - DictVectorizer转换
- 本身拿到的数据类型是字典类型
    
### 文本特征提取
- 特征：特征词
#### 第一个方法
- CountVectorizer
- sklearn.feature_extraction.text.CountVectorizer(stop_words=[])
- 返回词频矩阵 统计每个样本特征词出现的次数
- stop_words 停用词，我们觉得某些词对最终分类没有用处 is/to，以列表的形式传递
- CountVectorizer.fit_transform(x) x:文本或者包含文本字符串的可迭代对象  返回：返回sparse矩阵
    - CountVectorizer在设计的时候没有sparse这个参数，sparse矩阵转化为二维数组 可以用data.toarray()这个方法
    - 中文文档把短语当成一个特征值，处理中文要注意分词
- CountVectorizer.inverse_transform(x) x:array数组或者sparse矩阵 返回值：转换之前数据格
- CountVectorizer.get_feature_names() 返回值：单词列表
- jieba.cut
    - 进行分词处理中文
    - 实例化CountVectorizer
    - 将分词结果变成字符串当做fit_transform的输入值

```
from sklearn.feature_extraction.text import CountVectorizer
def count_demo():
    '''
    文本特征抽取
    :return:
    '''
    data = ["life is short,i like like python", "life is too long,i dislike python"]
    #实例化一个转换器
    transfer = CountVectorizer(stop_words = ['is','to'])
    #调用fit_transform
    new_data = transfer.fit_transform(data)
    print(new_data,new_data.toarray())
    print("特征名字：",transfer.get_feature_names())
    return None
```
```
from sklearn.feature_extraction.text import CountVectorizer
def Chinese_demo():
    '''
    中文文档的特征抽取
    :return:
    '''
    data = ["一闪 一闪 亮晶晶，漫天 都 是 小星星"]
    transfer = CountVectorizer()
    data_new = transfer.fit_transform(data)
    print(data_new, data_new.toarray())
    # 这个把短语当成一个特征值，处理中文要注意分词
    print(transfer.get_feature_names())
    return  None

def count_Chinese_demo():
    '''
    中文文本特征提取，自动分词
    :return:
    '''
    data = ["一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。",
            "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。",
            "如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"]
    ## 首先要分词处理
    data_new=[]
    for i in data:
        ### 注意是i 不是data
        data_new.append(cut_word_demo(i))
    print(data_new)
    tranfer = CountVectorizer(stop_words=["一种",'因为'])
    data_final = tranfer.fit_transform(data_new)
    print("data_final", data_final.toarray())
    print(data_final)
    print("特征名字：", tranfer.get_feature_names())

    return None

def cut_word_demo(text):
    '''
    用于中文分词
    :return:
    '''
    # 先强转为list的格式，在转换成字符串
    text1 = ' '.join(list(jieba.cut(text)))
    #print(text1)
    #print("...")
    # 返回的是一个字符串
    return ' '.join(list(jieba.cut(text)))
```
```

def tfidf_demo():
    '''
    用TF-IDF的方法进行文本特征抽取
    :return:
    '''
    data = ["一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。",
            "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。",
            "如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"]
    ## 首先要分词处理
    data_new = []
    for i in data:
        ### 注意是i 不是data
        data_new.append(cut_word_demo(i))
    #print(data_new)
    tranfer = TfidfVectorizer(stop_words=["一种", '因为'])
    data_final = tranfer.fit_transform(data_new)
    print(data_final)
    print("data_final", data_final.toarray())
    print("特征名字：", tranfer.get_feature_names())

    return None
```

#### 第二个方法    
- TfidfVectorizer   
- 关键词：在某个类别的文章中，出现的次数很多，但是在别的类别的文章中出现的很少
- TF-IDF的主要思想是：如果某个词或者短语在一篇文章中出现的概率并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类
- TF-IDF作用：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度
- 公式：
    - 词频：term frequency，tf 值的是某一个给定的词语在该文件中出现的频率
    - 逆向文档频率：inverse document frequency idf 是一个词语普遍重要性的度量。某一特定词语的idf，可以由总文件数目除以包含该该词语之文件的数目，再将得到的商取10为底的对数得到
    - tfidfij = tfij * idfi
    - 例子： 两个词 '经济''非常' 1000篇文章作为语料库 100篇文章都有非常 10篇文章有经济，现在有两篇文章AB，AB各100个词; A中10次经济，B中10次非常。计算TF-IDF的值。
        - A:tf=10/100     B:tf=10/100
        - A:idf log(1000/10)=2   B:idf log(1000/100)=1
        - A: TF-IDF 0.2 B: TF-IDF 0.1
- API：sklearn.feature_extraction.text.TfidfVectorizer(stop_words=None...)
    - 返回词的权重矩阵
        - TfidfVectorizer.fit_transform(x)
            - x:文本或者包含文本字符串的可迭代对象
            - 返回值: 返回sparse矩阵
        - TfidfVectorizer.inverse_transform(x)
            - x: array数组或者sparse矩阵
            - 返回值: 转换之前数据格式
        - TfidfVectorizer.get_feature_names()
            - 返回值：单词列表
```
from sklearn.feature_extraction.text import TfidfVectorizer
def tfidf_demo():
    '''
    用TF-IDF的方法进行文本特征抽取
    :return:
    '''
    data = ["一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。",
            "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。",
            "如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"]
    ## 首先要分词处理
    data_new = []
    for i in data:
        ### 注意是i 不是data
        data_new.append(cut_word_demo(i))
    #print(data_new)
    tranfer = TfidfVectorizer(stop_words=["一种", '因为'])
    data_final = tranfer.fit_transform(data_new)
    print(data_final)
    print("data_final", data_final.toarray())
    print("特征名字：", tranfer.get_feature_names())

    return None
```
            
## 特征预处理
### 什么是特征预处理
- 通过一些转换函数将特征数据转换成更加适合算法模型的特征数据过程
- provides several common utility functions and transformer classes 
- - 为什么我们要进行归一化/标准化
    - 特征的单位或者大小相差较大，或者某特征的方差相比其他的特征要大出几个数量级，容易影响(支配)目标结果，使得一些算法无法学习到其他的特征
- 数值型数据的无量纲化 
    - 因为量纲不统一，导致了某一特征值影响远超其他的特征值,为了让特征同等重要，就需要做数据的无量纲化。我们需要用到一些方法进行无量纲化，使不同规格的数据转换到统一规格
    - 归一化
    - 标准化
- 特征预处理API
    - sklearn.preprocessing
- 对于归一化来说，如果出现异常值，影响了最大值和最小值，那么结果会发生改变
- 对于标准化来说，如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而对方差标准差的影响较小
```
三个特征：每年获得的飞机常客里程数/里程数，每周消耗的冰淇淋公升数/公升数，玩游戏所有消耗时间的百分比/消耗时间比
评价三个类别： 不喜欢didnt 魅力一般small 极具魅力large
也许说飞机里程数对于结算结果影响较大，但是统计的人觉得这三个特征同等重要
```
### 归一化
- 通过对原始数据进行变换把数据映射到(默认为[0,1])之间
- 公式:
    - x' = (x-min)/(max-min) x''=x'*(mx-mi)+mi
    - 作用于每一列，max为一列的最大值，min为每一列的最小值，那么x''为最终结果，mx，mi分别为指定区间，默认mx为1，mi为0
- sklearn.preprocessing.MinMaxScaler(feature_range=(0,1)...)
    - MinMaxScaler.fit_transform(x)
        - x: numpy array格式的数据[n_samples,n_features]
    - 返回值: 转换后的形状相同的array
- 归一化缺陷：如果数据中异常点较多，比如最大值/最小值，归一化就是用的最大最小，所以这种方法鲁棒性较差，只适合传统精确小数据场景
```
from sklearn.preprocessing import MinMaxScaler
def minmax_demo():
    '''
    MinMaxScaler特征预处理
    :return:
    '''
    #获取数据
    data = pd.read_csv("dating.txt")
    data = data.iloc[:,:3]
    #print(data)
    #实例化 范围可以自己设定
    transfer = MinMaxScaler(feature_range=[2,3])
    #调用方法 调用转化器进行转化
    data_new = transfer.fit_transform(data)
    print(data_new)
    return None
```

### 标准化
- 通过对原始数据进行变换把数据变换到均值为0，标准差为1范围内
- x'=(x-mean)/σ 
    - σ：集中程度，离散程度 就算有一些异常数据，σ也不会有大的变化
- sklearn.preprocessing.StandardScaler()
    - 处理之后，对每列来说，所有数据都聚集在均值为0附近，标准差为1
    - StandardScaler.fit_transform(x)
        - x: numpy array格式的数据[n_samples,n_features]
    - 返回值: 转换后的形状相同的array
- 在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据的场景 
```
from sklearn.preprocessing import StandardScaler
def standard_demo():
    '''
    MinMaxScaler特征预处理
    :return:
    '''
    #获取数据
    data = pd.read_csv("dating.txt")
    data = data.iloc[:,:3]
    #print(data)
    #实例化 范围可以自己设定
    transfer = StandardScaler()
    #调用方法 调用转化器进行转化
    data_new = transfer.fit_transform(data)
    print(data_new)
    return None

```

## 特征降维

### 什么是降维
- ndarray 
        - 维数: 嵌套的层数
        - 0维 具体的数，标量
        - 1维 向量
        - 2维 矩阵
        - 3维 嵌套三次 多个2维数组嵌套而成
        - n维
- 降维是指在某些限定条件下，降低随机变量(特征)个数，得到一组"不相关"主变量的过程
    - 降低的是列的个数，特征的个数
    - 降低随机变量的个数
    - 效果要求特征与特征之间不相关
- 相关特性： correlated feature
    - 相对湿度与降雨量之间的相关（相关性大）
    - 相关特征太多会导致信息冗余
- 因为在进行训练的时候，我们都是使用特征进行学习，如果特征本身存在问题或者特征之间相关性较强，对于算法学习预测会影响很大
- 降维的两种方式
    - 特征选择
    - 主成分分析

### 特征选择
- 数据中包含冗余或者相关变量(或称特征，属性，指标等)，旨在从原有特征中找出主要特征
- 方法：
    - Filter(过滤式):主要探究特征本身特点，特征与特征和目标值之间关联
        - 方差选择法：低方差特征过滤 
            - 方差少，数据比较集中
        - 相关系数
            - 可以衡量两个特征之间是否有很强的相关性
    - Embedded(嵌入式):算法自动选择特征（特征与目标值之间的关联）
        - 决策树：信息熵，信息增益
        - 正则化：L1,L2
        - 深度学习：卷积等
- API:sklearn.feature_selection

#### 过滤式
- 低方差特征过滤
    - 删除低方差的一些特征，再结合方差的大小来考虑这个方式的角度
        - 特征方差小：某个特征大多样本的值比较相近 适合删除
        - 特征方差大：某个特征很多样本的值都有差别 适合保留
- API
    - sklearn.feature_selection.VarianceThreshold(threshold=0.0)
        - 删除所有低方差特征
        - Variance.fit_transform(x)
            - x:numpy array 格式的数据[n_sample,n_features]
            - 返回值:训练集差异低于threshold的特征将会被删除，默认值是保留所有非零方差特征，即删除所有样本重具有相同值的特征
        - 初始化VarianceThreshold 指定方差，调用fit_transform
```
from sklearn.feature_selection import VarianceThreshold
def variance_demo():
    '''
    过滤低方差特征
    :return:
    '''
    data = pd.read_csv("factor_returns.csv")
    data = data.iloc[:,1:-2]
    transfer = VarianceThreshold(threshold=10)
    data_new = transfer.fit_transform(data)
    print(data_new,data_new.shape)
    # 计算某两个变量之间的相关系数
    r1 = pearsonr(data["pe_ratio"],data["pb_ratio"])
    print("相关系数",r1)

    r2 = pearsonr(data["revenue"],data["total_expense"])
    print("相关系数",r2)
    return  None
```

#### 相关系数
- 皮尔逊相关系数(Pearson Correiation Coefficient)
    - 反映变量之间相关关系密切程度的统计指标
- 特点：
- 相关系数的值介于-1与+1之间，即-1<=r<=+1
    - 当r>0时，表示两变量正相关，r<0,表示两个变量为负相关
    - 当|r|=1时，表示两变量为完全相关，当r=0，表示两变量之间无相关关系
    - 当0<|r|<1时，表示两变量存在一定程度的相关，当|r|越接近1，两变量间线性关系越密切；当|r|越接近0，表示两变量的线性关系越弱
    - 一般可以按三级划分：当|r|<0.4为低度相关；0.4<=|r|<0.7为显著性相关；0.7<=|r|<1为高度线性相关
- API
    - from scipy.stats import pearsonr
        - x:(N,) array_like
        - y:(N,) array_like Return:(Pearson's correlation coefficient,p-value)
- 特征与特征之间相关性很高：
    - 选择其中一个作为代表
    - 按一定权重加权求和
    - 主成分分析 自动处理将相关性比较强的特征

### 主成分分析PCA
- 可以理解一种特征提取的方式，降维并尽可能的保持原有信息，减少信息的损失
- 定义：高维数据转化成低维数据的过程，在此过程中可以回舍弃原有数据，创造新的数据
- 作用：是数据维度压缩，尽可能降低原数据的维数(复杂度)，损失少量信息
- 应用：回归分析或者聚类分析当中
- 二维降到一维
    - 找到一个合适的直线，通过一个矩阵运算得出主成分分析的结果
- sklearn.decomposition.PCA(n_components=None)
    - 将数据分解为较低维数空间
    - n_components:
        - 小数：表示保留百分之多少的信息
        - 整数：减少到多少特征
    - PCA.fit_transform(x) 
        - x: numpy array格式的数据
        - [n_samples, n_features]
    - 返回值：转换后指定维度的array
```
from sklearn.decomposition import PCA
def PCA_demo():
    '''
    PCA降维
    :return:
    '''
    data = [[2, 8, 4, 5], [6, 3, 0, 8], [5, 4, 9, 1]]
    #实例化一个转化器类
    transfer = PCA(n_components=0.95)
    #调用方法
    data_new = transfer.fit_transform(data)
    print(data_new)
    return None
```
## sklearn转换器和估计器
### 转换器
- 转换器是特征工程的父类，所有在进行特征工程的时候都是继承自transformer这个类
- 之前做特征工程的步骤
    - 1.实例化 实例化一个转换器类 transformer
    - 2.调用fit_transform（对于文档建立分类词频矩阵，不能同时调用）
    - 我们把特征工程的家口称之为转换器，其中转换器调用有着几个种形式
        - fit_transform()
        - fit() 
        - transform()
    - 标准化公式:(x-mean)/std
        - fit_transform()
        - fit() 计算每一列的平均值和标准差
        - transform() 带入公式(x-mean)/std 进行最终的转换

        
### 预估器
- sklearn机器学习算法的实现都被封装到估计器这个父类当中
- 在sklearn中，估计器estimator是一个重要的角色，是一类实现了算法的API；所有的估计器都是estimator的子类
- 用于分类的估计器：
    - sklearn.neighbors k-近邻
    - sklearn.naive_bayes 贝叶斯
    - sklearn.linear_model.LogisticRegression 逻辑回归
    - sklearn.tree 决策树与随机森林
- 用于回归的估计器
    - sklearn.linear_model.linearRegression 线性回归
    - sklearn.linear_model.Ridge 岭回归
- 用于无监督学习的估计器
    - sklearn.cluster.KMeans 聚类
- 估计器工作流程estimator
    - 1.实例化，实例化一个预估器 estimator
    - 2.调用estimator.fit(x_train,y_trian) 
        - 调用完毕，模型生成
    - 3.模型评估
        - 直接比对真实值和预测值
             y_predict = estimator.predict(x_test)
             y_test == y_predict 生成布尔值，比对结果是否一致
        - 计算准确率
             accuracy = estimator.score(x_test,y_test)

# 分类算法
- 分类问题 回归 输入数据有特征有标签，即有标准答案
    - 目标值：结果为类别
    - k-近邻算法，朴素贝叶斯分类，决策树，随机森林，逻辑回归
- 回归问题
    - 目标值:连续型的数据

## k-近邻算法 KNN算法
### 什么是KNN算法
- K Nearest Neighbor
- 如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别 但是k=1时，容易受到异常点的影响，选择一个合适的k值可以避免受到异常值的影响
- 如何确定谁是邻居
    - 欧式距离 两个样本的距离
    - 曼哈顿距离 绝对值距离 |a1-b1|+|a2-b2|+|a3-b3|
    - 明可夫斯基距离
- 电影类型分析
    - 当k=1的时候，分类为爱情片
    - 当k=2的时候，取两个最近的值
    ...
    - 当k=6的时候，一共就6个样本，一半爱情片，一半动作片 分类无法确定
    - 当k=7的时候，加入一共电影为动作片，这样三个是爱情片，四个是动作片，这个电影归类到动作片，但是实际上这个片子偏向爱情片
- 当k值取的过小的时候，容易受到异常值的影响 
- 当k值取的过大的时候，受到样本不均衡的影响，类型很容易分错
- 无量纲化的处理
    - 标准化
API

- sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, algorithm='auto')
    - n_neighbors: int,可选(默认=5)，k值，k_neighbors 查询默认使用的邻居数
    - algorithm: {'auto','ball_tree','kd_tree','brute'},可选用于计算最近邻居的算法
        - ball_tree 使用BallTree
        - kd_tree 使用KDTree
        - auto 将尝试根据传递给fit方法的值来决定最合适的算法 不同现实方式影响效率
        
### 案例1 鸢尾花种类预测
- 1.数据集介绍
```
Iris数据集是常用的分类实验数据集，用称鸢尾花数据集，是一类多重变量分析的数据集。数据集的具体介绍：
实例数量： 150 三个类各有50个
属性数量： 4（数值型，数值型，帮助预测的属性和类）
属性信息：
    sepal length 萼片长度 cm
    sepal width 萼片宽度 cm
    petal length 花瓣长度 cm
    petal width 花瓣宽度 cm
类：
    Iris-SetosaN 山鸢尾
    Iris-Versicolour 变色鸢尾
    Iris-Virginica 维吉妮亚鸢尾
```
- 2.数据集划分
- 3.特征工程
    - 标准化
- 4.机器学习训练
    - KNN预估器流程
- 5.模型评估

```python
# 导入数据集
from sklearn.datasets import load_iris
# 划分数据集
from sklearn.model_selection import train_test_split
# 数据标准化
from sklearn.preprocessing import StandardScaler
# 调用knn算法
from sklearn.neighbors import KNeighborsClassifier

def knn_iris():
    '''
    用knn算法对鸢尾花进行分类
    :return: 
    '''
    # 1获取数据
    iris = load_iris()
    # 2划分数据集 random_state取6
    x_train, x_test, y_train, y_test = train_test_split(iris.data,iris.target,random_state=22)
    # 3特征工程：标准化
    transfer = StandardScaler()
    # 训练集标准化
    x_train = transfer.fit_transform(x_train)
    # 测试集也需要标准化，用训练集的平均值和标准差 模型用同样分布状况的数据
    x_test = transfer.transform(x_test)
    # 4KNN算法预估器,k取3
    # 放入训练集的特征值和目标值放进来
    estimator = KNeighborsClassifier(n_neighbors=3)
    # 相当于有了模型
    estimator.fit(x_train,y_train)
    # 5模型评估
    # 方法1，直接比对真实值和预测值
    # 根据x_train和y_trian模型，预测x_test会得到y_test,再和实际的y_test相比较
    # 得到预测的结果
    y_predict = estimator.predict(x_test)
    print("y_predict:\n", y_predict)
    print("对比真实值和预测值：", y_predict == y_test )
    # 方法2，计算准确率
    score = estimator.score(x_test,y_test)
    print("准确率：\n",score)
    return None

if __name__ == "__main__":
    knn_iris()
    
output:  
y_predict:
 [0 2 1 2 1 1 1 2 1 0 2 1 2 2 0 2 1 1 1 1 0 2 0 1 2 0 2 2 2 2 0 0 1 1 1 0 0
 0]
对比真实值和预测值： [ True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True False  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True]
准确率：
 0.973684210526
```
### KNN算法优缺点
- 优点：简单，易于理解，易于实现，无需训练
- 缺点：
    - 懒惰算法，对测试样本分类时的计算量大，内存开销大
    - 必须指定k值，k值选择不当则分类精度不能保证
- 使用场景：小数据场景，几千~几万样本，具体场景具体业务去测试，大数据不适合用KNN算法

## 模型选择和调优
### 什么是交叉验证(cross validation)  
交叉验证：将拿到的训练数据，分为训练和验证集。将数据分成四份，其中一份作为验证集。然后经过4次(组)的测试，每次都更换不同的验证集，即得到4组模型的结果，取平均数作为最终结果，又称为4折交叉验证。
#### 分析  
我们之前知道数据分成训练集和测试集，但是为类让训练得到模型的结果更为准确，做以下处理  
- 训练集：训练集+验证集
- 测试集：测试集
- 得到不同的准确率，求平均值之后，平均值作为模型最终准确率
```
一组：验证集 训练集 训练集 训练集  80%
二组：训练集 验证集 训练集 训练集  78%
三组：训练集 训练集 验证集 训练集  75%
四组：训练集 训练集 训练集 验证集  82%
```
#### 为什么需要交叉验证
- 交叉验证目的：为了让被评估的模型更加准确可信
- 如何选择或者调优参数

### 超参数搜索-网格搜索(Grid Search)
超参数 通常情况下，狠毒偶参数是需要手动指定的(如k-近邻算法中的k值)，这种叫超参数  

但是手动过程繁杂，所以需要对模型预设几种超参数组合。每组超参数都采用交叉验证来进行评估。最后选出最优参数组合建立模型。
```
k值   k=3   k=5   k=7
模型 模型1 模型2 模型3
```
### 模型选择与调优API
sklearn.model_selection.GridSearchCV(estimator,param_grid=None,cv=None)
    - 对估计器的指定参数进行详尽搜索
    - estimator:估计器对象
    - param_grid:估计器参数(dict)("n_neighbors":[1,3,5]) 准备好的k的取值，以字典的 形式传进来
    - cv:指定几折交叉验证 经过几次交叉验证 一般是10折
    - fit():输入训练数据
    - score():准确率
    - 结果分析:
        - 最佳参数: best_params_
        - 最佳结果: best_score_
        - 最佳估计器: best_estimator_
        - 交叉验证结果: cv_results_
        - p=1是曼哈顿距离，p=2是欧式距离
### 鸢尾花案例增加K值调优
```python
# 导入数据集
from sklearn.datasets import load_iris
# 划分数据集
from sklearn.model_selection import train_test_split
# 数据标准化
from sklearn.preprocessing import StandardScaler
# 调用knn算法
from sklearn.neighbors import KNeighborsClassifier
# 调用gscv调优
from sklearn.model_selection import GridSearchCV

def knn_iris_gscv():
    '''
    用knn算法对鸢尾花进行分类, 添加网格搜索和交叉验证
    :return:
    '''
    # 获取数据
    iris = load_iris()
    # 划分数据
    x_train, x_test, y_train, y_test = train_test_split(iris.data,iris.target,random_state=6)
    # 特征工程：标准化
    transfer = StandardScaler()
    x_train = transfer.fit_transform(x_train)
    x_test = transfer.transform(x_test)
    # KNN算法预估器，因为要试，所以这里不需要加上k值
    estimator = KNeighborsClassifier()
    # 加入网格搜索和交叉验证
    ## 参数准备 k值 字典的格式
    param_dict = {"n_neighbors":[1,3,5,7,9,11]}
    estimator = GridSearchCV(estimator,param_grid=param_dict,cv=10)
    estimator.fit(x_train,y_train)
    # 模型评估
    y_predict = estimator.predict(x_test)
    print("y_predict:\n", y_predict)
    print("对比真实值和预测值：", y_predict == y_test )
    score = estimator.score(x_test,y_test)
    print("准确率：\n",score)
    # 最佳参数: best_params_
    print("最佳参数：",estimator.best_params_)
    # 最佳结果: best_score_
    # 此最佳结果是训练集中验证集当中的结果
    print("最佳结果",estimator.best_score_)
    # 最佳估计器: best_estimator_
    print("最佳估计器",estimator.best_estimator_)
    # 交叉验证结果: cv_results_
    print("交叉验证结果",estimator.cv_results_)
    return None

if __name__ == "__main__":
    knn_iris_gscv()

output:
y_predict:
 [0 2 0 0 2 1 2 0 2 1 2 1 2 2 1 1 2 1 1 0 0 2 0 0 1 1 1 2 0 1 0 1 0 0 1 2 1
 2]
对比真实值和预测值： [ True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True False  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True False  True
  True  True]
准确率：
 0.947368421053
最佳参数： {'n_neighbors': 5}
最佳结果 0.973214285714
```
### 预测facebook签到位置  

数据集介绍  
本次大赛的目的是预测一个人想签入到哪个地方。FaceBook创建了一个人造的世界，10公里*10公里的一个区域。对于一个给定的坐标，你的任务是返回最有可能的地方的排名列表  

数据集数据  
```
train_csv, test_csv
row_id: id of the check_in event 登记事情的id
x y: coordiantes 坐标系
accuracy: location accuracy 定位准确度
time: timestamp 时间戳
place_id: id of the business. this is the target you are predicting   预测签到的位置 
```
#### 流程分析
- 获取数据
- 数据处理
    - 为了减少时间，缩小数据范围
        - 2<x<2.5   1.0<y<1.5
    - time处理成有意义的数据年月日时分秒 
    - 过滤签到次数少的地点
    - 特征值 x
    - 目标值 y
- 特征工程：标准化
- KNN算法预估流程
- 模型选择与调优
- 模型评估

## 朴素贝叶斯算法
### 什么是朴素贝叶斯分类方法
- 贝叶斯分类是一系列分类算法的总称,这类算法均以贝叶斯定理为基础,故统称为贝叶斯分类。朴素贝叶斯算法（Naive Bayesian) 是其中应用最为广泛的分类算法之一
- 朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法
- 朴素: 特征和特征之间是相互独立的 贝叶斯: 贝叶斯公式
- 朴素贝叶斯分类器基于一个简单的假定：给定目标值时属性之间相互条件独立,特征和特征之间是相互独立的。

### 概率基础
- 概念定义为一件事情发生的可能性
- P(X):取值在[0,1]

### 联合概率，条件概率与互相独立
- 联合概率:包含多个条件，且所有条件同时成立的概率
    - 记作:P(A,B)
- 条件概率:就是事件A在另外一个事件B已经发生的条件下发生的概率
    - 记作:P(A|B)
- 相互独立: 如果P(A,B)=P(A)P(B),则称事件A与事件B相互独立

### 贝叶斯公式
P(C|W)=P(W|C)P(C)/P(W)

### 应用场景
- 文本分类
    - 单词作为特征，词和词之间是相互独立的
- 公式可以理解为:  
    - P(C|F1,F2...)=P(F1,F2...|C)P(C)/P(F1,F2...)
    - P(C):每个文档类别的概率(某文档类别数/总文档数量)
    - P(W|C):给定类别下特征(被预测文档中出现的词)的概率
        - 计算方法: P(F1|C)=Ni/N(训练文档中出现的次数)
        - N为所属类别C下的文档所有词出现的次数和
    - P(F1,F2...)预测文档中每个词的频率
- 如果计算两个类别概率比较:
    - 所以我们只要比较前面的大小就可以，得出谁的概率大

```
        文档ID   文档中的词                  属于c=China类
训练集    1      Chinese Beijing Chinese        yes
          2      Chinese Chinese Shanghai       yes
          3      Chinese Macao                  yes
          4      Tokyo Japan Chinese            No
测试集    5      Chinese Chinese Chinese Tokyo Janpan ？
P(C)=3/4  
P(非C)=1/4
P(C|Chinese，Chinese，Chinese，Tokyo，Japan) = P(Chinese，Chinese，Chinese，Tokyo，Japan|C)P(C)/P(Chinese，Chinese，Chinese，Tokyo，Japan)
## 因为贝叶斯算法下，特征和特征之间是相互独立的
## 出现0，样本量太少，需要引入拉普拉斯平滑系数
P(Chinese|C) = 5/8  P(Tokyo|C) = 0/8 P(Japan|C) =0/8
P(Chinese，Chinese，Chinese，Tokyo，Japan|C) = P(Chinese|C)^3*P(Tokyo|C)*P(Japan|C) = 

P(非C|Chinese，Chinese，Chinese，Tokyo，Japan) =  P(Chinese，Chinese，Chinese，Tokyo，Japan|非mC)P(非C)/P(Chinese，Chinese，Chinese，Tokyo，Japan)

分母一样，只要比较分子
```
### 拉普拉斯平滑系数
- 目的: 防止计算出的分类概率为0
- P(F1|C)=(Ni+α)/(N+αm)
    - α为指定系数一般为1，m为训练集文档中统计出的特征词个数
```
P(Chinese|C) = (5+1)/(8+1*6)=3/7  
P(Tokyo|C) = (0+1)/(8+1*6)=1/14 
P(Japan|C) =(0+1)/(8+1*6)=1/14
```
### API
- sklearn.navie_bayes.MultinomialNB(alpha=1.0)
    - 朴素贝叶斯分类
    - alpha: l拉普拉斯平滑系数

### 案例二十类新闻分类
- 获取数据  
    - 获取的是sklearn的数据，所以我们不需要处理
    - 新闻组数据集是大约20000个新闻组文档的集合，平均分布在20个不同的新闻组中。数据被组织成20个不同的新闻组，每个新闻组对应不同的主题。一些新闻组彼此之间关系密切，而另外一些新闻组则非常不相关
- 进行数据集的分割
- 特征工程：
    - TFIDF进行的特征抽取
        - 将文章字符串进行单词抽取
- 朴素贝叶斯预估器预测
- 模型评估

```
from sklearn.datasets import fetch_20newsgroups
# 划分数据集
from sklearn.model_selection import train_test_split
# 文本数据抽取
from sklearn.feature_extraction.text import TfidfVectorizer
# 朴素贝叶斯算法
from sklearn.naive_bayes import MultinomialNB

def nb_news():
    '''
    用朴素贝叶斯算法对新闻进行分类
    :return:
    '''
    # 获取数据
    news = fetch_20newsgroups(subset= "all")
    # 划分数据集
    x_train,x_test,y_train,y_test = train_test_split(news.data,news.target)
    # 特征工程：文本特征抽取 TFIDF方式
    transfer = TfidfVectorizer()
    x_train = transfer.fit_transform(x_train)
    x_test = transfer.transform(x_test)
    # 朴素贝叶斯预估器预测,默认的alpha为1.0
    estimator = MultinomialNB()
    estimator.fit(x_train,y_train)
    # 模型评估
    y_predict = estimator.predict(x_test)
    print("y_predict:\n", y_predict)
    print("对比真实值和预测值：", y_predict == y_test)
    score = estimator.score(x_test, y_test)
    print("准确率：\n", score)
    return None
    
output:
y_predict:
 [ 7 18 14 ..., 11 18 13]
直接比对真实值和预测值:
 [False  True  True ...,  True  True  True]
准确率为：
 0.850594227504

```
### 朴素贝叶斯算法总结
- 优点：
    - 朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率
    - 对缺失数据不太敏感，算法比较简单，常用于文本分类
    - 分类准确度高，速度快
- 缺点：
    - 由于使用类样本属性独立性的假设，所有如果特征属性有关联时其效果不好


## 决策树
### 认识决策树
- 决策树思想的来源非常朴素，程序设计中的条件分支结构就是if-else结构，最早的决策树就是利用这类结构分割数据的一种分类学习
- 如何高效的进行决策
- 确定快速的确定特征的先后顺序

### 决策树分类原理详解
- 信息论
    - 信息: 消除随机不定性的东西
    - 信息是物质、能量、信息及其属性的标示。
    - 信息是确定性的增加。
    - 信息是事物现象及其属性标识的集合。
- 信息熵
    - 来衡量我们消除的不确定的程度
    - H的专业术语称为信息熵，单位为比特bit
        - 公式:  ![公式][1]
[1]:https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/%E4%BF%A1%E6%81%AF%E7%86%B5.png
- 信息熵性质
    - 单调性，即发生概率越高的事件，其所携带的信息熵越低。极端案例就是“太阳从东方升起”，因为为确定事件，所以不携带任何信息量。从信息论的角度，认为这句话没有消除任何不确定性。
    - 非负性，即信息熵不能为负。这个很好理解，因为负的信息，即你得知了某个信息后，却增加了不确定性是不合逻辑的。
    - 累加性，即多随机事件同时发生存在的总不确定性的量度是可以表示为各事件不确定性的量度的和。

- 决策树的划分依据之一 信息增益
    - 特征A对训练数据集D的信息增益g(D,A), 定义为集合D的信息熵H(D)与特征A给定条件下D的信息条件熵H(D|A)之差。
    - 信息增益表示得知特征X的信息从而信息的不确定性减少的程度，使得总的信息熵减少的程度
    - g(D|A) = H(D) - H(D|A)
    - 公式:  ![公式][2]
[2]:
https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/%E6%9D%A1%E4%BB%B6%E7%86%B5.png

### 银行贷款案例
- ![银行贷款案例][3]
[3]:
https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/%E9%93%B6%E8%A1%8C%E8%B4%B7%E6%AC%BE%E6%95%B0%E6%8D%AE.png
- g(D|A) = H(D) - H(D|A)
         = H(D) - (5/15H(青年)+5/15H(中年)+5/15H(老年))
- H(D) = -(6/15*log(6/15)+9/15*log(9/15))
- H(青年) = -(3/5log(3/5)+2/5log(2/5))
- H(中年) = -(3/5log(3/5)+2/5log(2/5))
- H(老年) = -(4/5log(3/5)+1/5log(2/5))
- 以A1,A2,A3,A4代表年龄，工作，房子和贷款情况。最终结果g(D|A1)=0.313 g(D|A2)=0.324 g(D|A3)=0.420 g(D|A4)=0.363 从而我们选择A3房子作为划分的第一个特征。

当然决策树的原理不止信息增益这一种，还有其他方法
- ID3
    - 信息增益，最大准则
- C4.5
    - 信息增益比 最大准则
- CART
    - 分类树: 基尼系数 最小准则 在sklearn中可选择划分的默认原则
    - 优势: 划分更加细致
    

### 决策树API
- sklearn.tree.DecisionTreeClassifier(criterion='gini',max_depth=None,random_state=None)
    - 决策树分类器
    - criterion:决策树的划分依据, 默认是'gini'系数，也可以选择信息增益的熵'entropy'
    - max_depth:数的深度大小 (如果分的过细，很有可能泛化能力比较差，在训练集上表现的很好，但是在测试集上表现就没那么好，此时可以设置下数的深度大小，提高准确率)
    - random_state:随机数种子

```
# 导入数据集
from sklearn.datasets import load_iris
# 划分数据集
from sklearn.model_selection import train_test_split
# 决策树分类器
from sklearn.tree import DecisionTreeClassifier

def tree_iris():
    '''
    用决策树对鸢尾花进行分类
    :return:
    '''
    # 获取数据
    iris = load_iris()
    # 划分数据
    x_train,x_test,y_train,y_test = train_test_split(iris.data,iris.target,random_state=22)
    # 使用决策树预估器进行分类
    estimator = DecisionTreeClassifier(criterion="entropy")
    estimator.fit(x_train,y_train)
    # 模型评估
    estimator.predict(x_test)
    print("y_predict:\n", y_predict)
    print("直接比对真实值和预测值:\n", y_test == y_predict)

    # 方法2：计算准确率
    score = estimator.score(x_test, y_test)
    print("准确率为：\n", score)
    return None
    
output:
y_predict:
 [0 2 1 2 1 1 1 1 1 0 2 1 2 2 0 2 1 1 1 1 0 2 0 1 2 0 1 2 2 1 0 0 1 1 1 0 0
 0]
    tree_iris()
直接比对真实值和预测值:
 [ True  True  True  True  True  True  True False  True  True  True  True
  True  True  True  True  True  True False  True  True  True  True  True
  True  True False  True  True False  True  True  True  True  True  True
  True  True]
准确率为：
 0.894736842105
```
- 决策树模型计算出来的准确率为89.47%, 而之前用knn算法算出来的结果准确率为97%。实际上不同的算法有不同的应用环境，决策树更适合用于数据量比较大的数据集

### 决策树可视化API
- sklearn.tree.export_graphviz() 该函数能导出DOT格式
    - tree.export_graphviz(estimator,out_file='tree.dot',feature_names=[","])
```
export_graphviz(estimator, out_file="iris_tree.dot", feature_names=iris.feature_names)
```

```
digraph Tree {
node [shape=box] ;
0 [label="petal width (cm) <= 0.75\nentropy = 1.584\nsamples = 112\nvalue = [39, 37, 36]"] ;
1 [label="entropy = 0.0\nsamples = 39\nvalue = [39, 0, 0]"] ;
0 -> 1 [labeldistance=2.5, labelangle=45, headlabel="True"] ;
2 [label="petal width (cm) <= 1.75\nentropy = 1.0\nsamples = 73\nvalue = [0, 37, 36]"] ;
0 -> 2 [labeldistance=2.5, labelangle=-45, headlabel="False"] ;
3 [label="petal length (cm) <= 5.05\nentropy = 0.391\nsamples = 39\nvalue = [0, 36, 3]"] ;
2 -> 3 ;
4 [label="sepal length (cm) <= 4.95\nentropy = 0.183\nsamples = 36\nvalue = [0, 35, 1]"] ;
3 -> 4 ;
5 [label="petal width (cm) <= 1.35\nentropy = 1.0\nsamples = 2\nvalue = [0, 1, 1]"] ;
4 -> 5 ;
6 [label="entropy = 0.0\nsamples = 1\nvalue = [0, 1, 0]"] ;
5 -> 6 ;
7 [label="entropy = 0.0\nsamples = 1\nvalue = [0, 0, 1]"] ;
5 -> 7 ;
8 [label="entropy = 0.0\nsamples = 34\nvalue = [0, 34, 0]"] ;
4 -> 8 ;
9 [label="sepal length (cm) <= 6.05\nentropy = 0.918\nsamples = 3\nvalue = [0, 1, 2]"] ;
3 -> 9 ;
10 [label="entropy = 0.0\nsamples = 1\nvalue = [0, 1, 0]"] ;
9 -> 10 ;
11 [label="entropy = 0.0\nsamples = 2\nvalue = [0, 0, 2]"] ;
9 -> 11 ;
12 [label="petal length (cm) <= 4.85\nentropy = 0.191\nsamples = 34\nvalue = [0, 1, 33]"] ;
2 -> 12 ;
13 [label="entropy = 0.0\nsamples = 1\nvalue = [0, 1, 0]"] ;
12 -> 13 ;
14 [label="entropy = 0.0\nsamples = 33\nvalue = [0, 0, 33]"] ;
12 -> 14 ;
}
```
- 网站显示结构
    - webgraphviz.com/

### 决策树总结
- 优点: 简单的理解和解释，树木可视化
- 缺点:
    - 决策树学习者可能创建不能够很好的用来推广数据的过于复杂的数，这样会被称为过拟合
- 改进:
    - 减技cart算法(决策树API当中已经实现，随机森林参数调优有相关介绍)
    - 随机森林
    - 企业重要决策，由于决策树很好的分析能力，在决策过程应用较多，因为可以选择特征
 

## 随机森林
### 什么是集成学习方法
- 集成学习通过建立几个模型组合来解决单一预测问题。他的工作原理就是生成多个分类器/模型。各自独立地学习和作出预测。这些预测最后结合成组合预测，因此优于任何一个单分类的作出预测

### 什么是随机森林
- 集成学习方法中的一个，在机器学习中，随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别数输出的类别的众数而定
- 例如，如果你训练5个树，其中4个树的结果是True，一个树的结果是False，那么最终结果就是True

### 随机森林原理过程
- 训练集随机，特征值随机
- 用N来表示训练用树(样本)的个数，M表示特征数目
    - 一次随机选出一个样本，重复N次，有可能出现重复的样本
    - 随机去选m个特征，m<<M，建立决策树，可以起到降维的结果
- 采取bootstrap抽样

### 为什么采用bootstrap抽样
随机有放回抽样 这样每一个训练集都是独有的
- 为什么要随机抽样训练集
    - 如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的
- 为什么要有放回的抽样
    - 如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是有偏的，都是绝对片面的，每棵树训练出来都是有很大的差异，而随机森林最后分类取决于多棵树(弱分类器)的投票表决

### API
- sklearn.ensemble.RandomForestClassifier(n_estimators=10,criterion='gini',max_depth=None,bootstrap=True,random_state=None,min_sample_split=2)
    - n_estimators: integer,optional(default=10) 森林里面数目的数量
    - criterion:string，可选(default='gini') 分割特征的测量方法
    - max_depth: integer或None，可选(默认=无) 树的最大深度 5，8，15，25，30
    - bootstrap: boolean,optional(default=True) 是否在构建树是放回抽样
    - max_features='auto',每个决策树最大特征数量 m
        - if auto, then max_features=sqrt(n_features)
        - if sqrt, then max_features(n_features) same as auto
        - if log2 then max_features=log2(n_features)
        - if None, then max_features=n_features m=M 达不到降维的效果，每棵树的时间很长
    - min_samples_split:节点划分最少样本数
    - min_samples_leaf:叶子节点的最小样本数
- 超参数:n_estimators,max_depth,min_samples_split,min_samples_leaf

### 随机森林总结
- 在当前所有算法中，具有极好的准确率
- 能够有效的运行在大数据集上，处理具有高维特征的输入样本，而且不需要降维
- 能够评估各个特征在分类问题上的重要性


# 回归算法

# 聚类算法
