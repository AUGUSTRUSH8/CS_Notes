# 什么是机器学习
## 机器学习
### 定义
- 机器学习是从数据中自动分析获取模型，并利用模型对未知数据进行预测

### 解释
- 我们人从大量的日常经验中归纳规律，在面临新的问题的时候，可以利用以往总结的规律去分析现实状况，采取最佳策略

## 数据集的构成
- 我们把机器学习要学习的数据称为数据集
- 对于每一行数据我们称之为样本
- 数据集：数据具有特征，目标值为预测结果
- 有些数据集可以没有目标值

### 可用数据集
- kaggle
- UCI数据集
- scikit-learn
### sklearn数据集
- sklearn.datasets
    - load_* 获取小规模数据集
    - fetch_* 获取大规模数据集
        - sklearn.datasets.fetch_name(data_home=None,subset='train')
            - subset：train/test/all

- sklearn数据集返回值介绍
    - load和fetch返回的数据类型都是datasets.base.Bunch 继承自字典
        - data：特征值，特征数据数组，是[n_samples*n_features]的二维
            numpy.ndarray 数组
        - target：目标值，标签数组，是n_samples的一维numpy.ndarray数组
        - DESCR: 数据集描述
        - feature_names: 特征值名字，新闻数据，手写数字，回归数据集没有
        - target_names: 目标值标签名
    - dict['key'] = values 用字典键值对获得数据
      bunch.key = values

     
### 数据集的划分
- 机器学习一般的数据集会划分成两个部分：
    - 训练数据：用于训练，构建模型 70%~80%
    - 测试数据: 在模型检验的时使用，用于评估模型是否有效 20%~30%
- 数据集划分api
    - sklearn.model_selection.trian_test_split(arrays,*options)
    - x 数据集的特征值
        - 训练集特征值 x_train
        - 测试集特征值 x_test
    - y 数据集的标签值
        - 训练集目标值 y_train
        - 测试集目标值 y_test
    - test_size测试集的大小，一般为float
    - random_state随机数种子，不同的种子会造成不同的随机采样结果。相同的种子采样结果相同
    - return 训练集特征值，测试集特征值，训练集目标值，测试集目标值
### 鸢尾花数据集
- 特征值： 4个特征，花瓣的长度和宽度，花萼的长度，宽度
- 目标值： setosa vericolor virginica
- 数据集划分之后，特征值的数量不变还是四个特征，但是样本的数量变了


## sklearn转换器和估计器
### 转换器
- 转换器是特征工程的父类，所有在进行特征工程的时候都是继承自transformer这个类
- 之前做特征工程的步骤
    - 1.实例化 实例化一个转换器类 transformer
    - 2.调用fit_transform（对于文档建立分类词频矩阵，不能同时调用）
    - 我们把特征工程的家口称之为转换器，其中转换器调用有着几个种形式
        - fit_transform()
        - fit() 
        - transform()
    - 标准化公式:(x-mean)/std
        - fit_transform()
        - fit() 计算每一列的平均值和标准差
        - transform() 带入公式(x-mean)/std 进行最终的转换
        
### 预估器
- sklearn机器学习算法的实现都被封装到估计器这个父类当中
- 在sklearn中，估计器estimator是一个重要的角色，是一类实现了算法的API；所有的估计器都是estimator的子类
- 用于分类的估计器：
    - sklearn.neighbors k-近邻
    - sklearn.naive_bayes 贝叶斯
    - sklearn.linear_model.LogisticRegression 逻辑回归
    - sklearn.tree 决策树与随机森林
- 用于回归的估计器
    - sklearn.linear_model.linearRegression 线性回归
    - sklearn.linear_model.Ridge 岭回归
- 用于无监督学习的估计器
    - sklearn.cluster.KMeans 聚类

## 特征工程
- 什么是特征工程
    - 是是用专业背景知识和技巧处理数据，能得特征能在机器学习算法上发挥更好的作用的过程
    - 目前用的工具 sklearn
- 为什么需要特征工程 Feature Engineering
    - 数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已
- 影响效果的原因
    - 算法
    - 特征工程
- 特征工程的位置与数据处理的比较
    - pandas：一个数据读取非常方便以及基本的处理格式的工具 用于数据清洗，数据处理（缺失值处理）
    - sklearn： 对于特征的处理特供了强大的接口 用于特征工程
- 特征工程包含内容
    - 特征抽取
    - 特征预处理
    - 特征降维

## 机器学习算法分类
- 监督学习 supervised learning(预测)
    - 定义：输入数据是由输入特征值和目标值所组成。函数的输出可以是一个连续的值（称为回归），或是输出的是有限的离散值（称为分类），
    - 分类问题 回归 输入数据有特征有标签，即有标准答案
        - 目标值：结果为类别
        - k-近邻算法，朴素贝叶斯分类，决策树，随机森林，逻辑回归
    - 回归问题
        - 目标值:连续型的数据
        - 线性回归，岭回归
- 无监督学习 unsupervised learning
    - 定义：输入数据是由输入特征值组成
    - 目标值：没有目标值 输入数据有特征无标签，即无标准答案    
        - 聚类 k-means

## 机器学习开发流程
- 获取数据 原始数据
- 数据处理
- 特征工程
- 使用机器学习算法训练 得到模型
- 模型评估

## 学习框架
- 重点  
    - 算法是核心，数据与计算是基础
- 大部分复杂模型的算法设计部都是算法工程师在做，而我们
    - 分析很多数据
    - 分析具体业务
    - 应用常见的算法
    - 特征工程，调参数，优化
        


# 特征工程
## 特征工程介绍
- 什么是特征工程
    - 是是用专业背景知识和技巧处理数据，能得特征能在机器学习算法上发挥更好的作用的过程
    - 目前用的工具 sklearn
- 为什么需要特征工程 Feature Engineering
    - 数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已
- 影响效果的原因
    - 算法
    - 特征工程
- 特征工程的位置与数据处理的比较
    - pandas：一个数据读取非常方便以及基本的处理格式的工具 用于数据清洗，数据处理（缺失值处理）
    - sklearn： 对于特征的处理特供了强大的接口 用于特征工程
- 特征工程包含内容
    - 特征抽取
    - 特征预处理
    - 特征降维


## 特征抽取
- 特征提取
- 机器学习算法 
    - 统计方法, 就是数学公式，但是不能处理字符串，就需要将文本类型的字符串转成成数值类型，即需要特征抽取
        - 文本类型转化成数值
        - 类别，类型转成数值，转换成one-hot编码，哑变量
- 为什么需要特征提取：
    - 将任意数据（如文本或图像）转换成可用于机器学习的数字特征（特征值化）
- 特征提取API
    - sklearn.feature_extraction 是一个类
        - feature 特征
        - extraction 提取
    - 不同类型的数据有不同的转换方法
        - 字典特征提取（特征离散化）
        - 文本特征提取
        - 图像特征提取（深度学习）
     

         
### 字典特征提取  

对字典数据进行特征值化
#### API
- sklearn.feature_extraction.DictVectorizer(spars=True...) 默认sparse为True
    - vector 向量 矢量 可以用一维数组来存储向量
        - 矩阵matrix 计算机中用二维数组存储 矩阵可以看成由向量构成
    - Dict Vectorizer 字典vectorizer转换成向量的形式，告诉计算机把字典转出成数值了，可以把每一个样本理解为一个向量，n个样本就是n个向量，可以看成是一个二维数组，也可以理解为矩阵
    - 调用了DictVectorizer，相当于实例化了一个转换器对象，父类是一个transfer，转换器类，其中一个方法就是把字典换成成数值  
    - 调用实例化好的对象DictVectorizer，里面有个方法叫fit_transform
    - DictVectorizer.fit_transform(x) x:字典或者包含字典的迭代器   返回值：返回sparse矩阵 稀疏
        - 稀疏矩阵将非0值按位置表现出来，节省内存，可以提高加载运行效率
    - DictVectorizer.fit_transform(x) x:array数组或者sparse矩阵    返回值：转换之前数据格式
        - DictVectorizer.get_feature_names() 返回类别名称
      
字典特征提取
- 对于特征当中存在类别信息的我们都会做one-hot编码处理，哑变量

应用数据：  
字典：  
```
data = [{'city': '北京', 'temperature': 100}, 
        {'city': '上海', 'temperature': 60}, 
        {'city': '深圳', 'temperature': 30}]
output:      
           [[0,1,0,100]
            [1,0,0,60]
            [0,0,1,30]]
```

每一个样本理解为一个向量，一共三行。三个样本，两个特征，是一个三行两列的二维数组/矩阵
原来每个样本有两个特征，再进行转化之后，字典特征抽取之后，样本量不变，特征数量变成了4个。当特征中有类别的时候，表示方法时字符串，想表示为数值并且数值没有比大小，我们就使用ont-hot变量，哑变量
#### 字典特征提取案例
```python
from sklearn.feature_extraction import DictVectorizer
def dict_demo():
    '''
    字典特征抽取
    :return:
    '''
    data = [{'city': '北京', 'temperature': 100}, {'city': '上海', 'temperature': 60}, {'city': '深圳', 'temperature': 30}]
    #1 实例化一个转换器类
    # sparse 稀疏矩阵
    transform = DictVectorizer(sparse=False)
    # 2 调用fit_transform()方法，里面传字典或者包含字典的迭代器
    data_new = transform.fit_transform(data)
    print("data_new:\n", data_new)
    print("特征名字：",transform.get_feature_names())
    return None
```
#### 字典提取应用场景
- 当我们面对数据集中有类别特征比较多
    - 将数据集的特征转换成字典类型
    - DictVectorizer转换
- 本身拿到的数据类型是字典类型
    
### 文本特征提取
- 特征：特征词
#### 返回词频矩阵API
- CountVectorizer
- sklearn.feature_extraction.text.CountVectorizer(stop_words=[])
- 返回词频矩阵 统计每个样本特征词出现的次数
- stop_words 停用词，我们觉得某些词对最终分类没有用处 is/to，以列表的形式传递
- CountVectorizer.fit_transform(x) x:文本或者包含文本字符串的可迭代对象  返回：返回sparse矩阵
    - CountVectorizer在设计的时候没有sparse这个参数，sparse矩阵转化为二维数组 可以用data.toarray()这个方法
    - 中文文档把短语当成一个特征值，处理中文要注意分词
- CountVectorizer.inverse_transform(x) x:array数组或者sparse矩阵 返回值：转换之前数据格
- CountVectorizer.get_feature_names() 返回值：单词列表
- jieba.cut
    - 进行分词处理中文
    - 实例化CountVectorizer
    - 将分词结果变成字符串当做fit_transform的输入值
#### 案例
```python
from sklearn.feature_extraction.text import CountVectorizer
def count_demo():
    '''
    文本特征抽取
    :return:
    '''
    data = ["life is short,i like like python", "life is too long,i dislike python"]
    #实例化一个转换器
    transfer = CountVectorizer(stop_words = ['is','to'])
    #调用fit_transform
    new_data = transfer.fit_transform(data)
    print(new_data,new_data.toarray())
    print("特征名字：",transfer.get_feature_names())
    return None
```
```python
from sklearn.feature_extraction.text import CountVectorizer
def Chinese_demo():
    '''
    中文文档的特征抽取
    :return:
    '''
    data = ["一闪 一闪 亮晶晶，漫天 都 是 小星星"]
    transfer = CountVectorizer()
    data_new = transfer.fit_transform(data)
    print(data_new, data_new.toarray())
    # 这个把短语当成一个特征值，处理中文要注意分词
    print(transfer.get_feature_names())
    return  None

def count_Chinese_demo():
    '''
    中文文本特征提取，自动分词
    :return:
    '''
    data = ["一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。",
            "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。",
            "如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"]
    ## 首先要分词处理
    data_new=[]
    for i in data:
        ### 注意是i 不是data
        data_new.append(cut_word_demo(i))
    print(data_new)
    tranfer = CountVectorizer(stop_words=["一种",'因为'])
    data_final = tranfer.fit_transform(data_new)
    print("data_final", data_final.toarray())
    print(data_final)
    print("特征名字：", tranfer.get_feature_names())

    return None

def cut_word_demo(text):
    '''
    用于中文分词
    :return:
    '''
    # 先强转为list的格式，在转换成字符串
    text1 = ' '.join(list(jieba.cut(text)))
    #print(text1)
    #print("...")
    # 返回的是一个字符串
    return ' '.join(list(jieba.cut(text)))
```
```python

def tfidf_demo():
    '''
    用TF-IDF的方法进行文本特征抽取
    :return:
    '''
    data = ["一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。",
            "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。",
            "如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"]
    ## 首先要分词处理
    data_new = []
    for i in data:
        ### 注意是i 不是data
        data_new.append(cut_word_demo(i))
    #print(data_new)
    tranfer = TfidfVectorizer(stop_words=["一种", '因为'])
    data_final = tranfer.fit_transform(data_new)
    print(data_final)
    print("data_final", data_final.toarray())
    print("特征名字：", tranfer.get_feature_names())

    return None
```

#### 关键词API   
- TfidfVectorizer   
- 关键词：在某个类别的文章中，出现的次数很多，但是在别的类别的文章中出现的很少
- TF-IDF的主要思想是：如果某个词或者短语在一篇文章中出现的概率并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类
- TF-IDF作用：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度
- 公式：
    - 词频：term frequency，tf 值的是某一个给定的词语在该文件中出现的频率
    - 逆向文档频率：inverse document frequency idf 是一个词语普遍重要性的度量。某一特定词语的idf，可以由总文件数目除以包含该该词语之文件的数目，再将得到的商取10为底的对数得到
    - tfidfij = tfij * idfi
    - 例子： 两个词 '经济''非常' 1000篇文章作为语料库 100篇文章都有非常 10篇文章有经济，现在有两篇文章AB，AB各100个词; A中10次经济，B中10次非常。计算TF-IDF的值。
        - A:tf=10/100     B:tf=10/100
        - A:idf log(1000/10)=2   B:idf log(1000/100)=1
        - A: TF-IDF 0.2 B: TF-IDF 0.1
- API：sklearn.feature_extraction.text.TfidfVectorizer(stop_words=None...)
    - 返回词的权重矩阵
        - TfidfVectorizer.fit_transform(x)
            - x:文本或者包含文本字符串的可迭代对象
            - 返回值: 返回sparse矩阵
        - TfidfVectorizer.inverse_transform(x)
            - x: array数组或者sparse矩阵
            - 返回值: 转换之前数据格式
        - TfidfVectorizer.get_feature_names()
            - 返回值：单词列表
#### 案例
```python
from sklearn.feature_extraction.text import TfidfVectorizer
def tfidf_demo():
    '''
    用TF-IDF的方法进行文本特征抽取
    :return:
    '''
    data = ["一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。",
            "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。",
            "如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"]
    ## 首先要分词处理
    data_new = []
    for i in data:
        ### 注意是i 不是data
        data_new.append(cut_word_demo(i))
    #print(data_new)
    tranfer = TfidfVectorizer(stop_words=["一种", '因为'])
    data_final = tranfer.fit_transform(data_new)
    print(data_final)
    print("data_final", data_final.toarray())
    print("特征名字：", tranfer.get_feature_names())

    return None
```
            
## 特征预处理
### 什么是特征预处理
- 通过一些转换函数将特征数据转换成更加适合算法模型的特征数据过程
- provides several common utility functions and transformer classes 
- - 为什么我们要进行归一化/标准化
    - 特征的单位或者大小相差较大，或者某特征的方差相比其他的特征要大出几个数量级，容易影响(支配)目标结果，使得一些算法无法学习到其他的特征
- 数值型数据的无量纲化 
    - 因为量纲不统一，导致了某一特征值影响远超其他的特征值,为了让特征同等重要，就需要做数据的无量纲化。我们需要用到一些方法进行无量纲化，使不同规格的数据转换到统一规格
    - 归一化
    - 标准化
- 特征预处理API
    - sklearn.preprocessing
- 对于归一化来说，如果出现异常值，影响了最大值和最小值，那么结果会发生改变
- 对于标准化来说，如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而对方差标准差的影响较小
```
三个特征：每年获得的飞机常客里程数/里程数，每周消耗的冰淇淋公升数/公升数，玩游戏所有消耗时间的百分比/消耗时间比
评价三个类别： 不喜欢didnt 魅力一般small 极具魅力large
也许说飞机里程数对于结算结果影响较大，但是统计的人觉得这三个特征同等重要
```
### 归一化
- 通过对原始数据进行变换把数据映射到(默认为[0,1])之间
- 公式:
    - x' = (x-min)/(max-min) x''=x'*(mx-mi)+mi
    - 作用于每一列，max为一列的最大值，min为每一列的最小值，那么x''为最终结果，mx，mi分别为指定区间，默认mx为1，mi为0
    
### 归一化API
- sklearn.preprocessing.MinMaxScaler(feature_range=(0,1)...)
    - MinMaxScaler.fit_transform(x)
        - x: numpy array格式的数据[n_samples,n_features]
    - 返回值: 转换后的形状相同的array
- 归一化缺陷：如果数据中异常点较多，比如最大值/最小值，归一化就是用的最大最小，所以这种方法鲁棒性较差，只适合传统精确小数据场景
### 归一化案例
```python
from sklearn.preprocessing import MinMaxScaler
def minmax_demo():
    '''
    MinMaxScaler特征预处理
    :return:
    '''
    #获取数据
    data = pd.read_csv("dating.txt")
    data = data.iloc[:,:3]
    #print(data)
    #实例化 范围可以自己设定
    transfer = MinMaxScaler(feature_range=[2,3])
    #调用方法 调用转化器进行转化
    data_new = transfer.fit_transform(data)
    print(data_new)
    return None
```

### 标准化
- 通过对原始数据进行变换把数据变换到均值为0，标准差为1范围内
- x'=(x-mean)/σ 
    - σ：集中程度，离散程度 就算有一些异常数据，σ也不会有大的变化
### 标准化API
- sklearn.preprocessing.StandardScaler()
    - 处理之后，对每列来说，所有数据都聚集在均值为0附近，标准差为1
    - StandardScaler.fit_transform(x)
        - x: numpy array格式的数据[n_samples,n_features]
    - 返回值: 转换后的形状相同的array
- 在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据的场景 
### 标准化案例
```python
from sklearn.preprocessing import StandardScaler
def standard_demo():
    '''
    MinMaxScaler特征预处理
    :return:
    '''
    #获取数据
    data = pd.read_csv("dating.txt")
    data = data.iloc[:,:3]
    #print(data)
    #实例化 范围可以自己设定
    transfer = StandardScaler()
    #调用方法 调用转化器进行转化
    data_new = transfer.fit_transform(data)
    print(data_new)
    return None

```

## 特征降维
### 什么是降维
- ndarray 
        - 维数: 嵌套的层数
        - 0维 具体的数，标量
        - 1维 向量
        - 2维 矩阵
        - 3维 嵌套三次 多个2维数组嵌套而成
        - n维
- 降维是指在某些限定条件下，降低随机变量(特征)个数，得到一组"不相关"主变量的过程
    - 降低的是列的个数，特征的个数
    - 降低随机变量的个数
    - 效果要求特征与特征之间不相关
- 相关特性： correlated feature
    - 相对湿度与降雨量之间的相关（相关性大）
    - 相关特征太多会导致信息冗余
- 因为在进行训练的时候，我们都是使用特征进行学习，如果特征本身存在问题或者特征之间相关性较强，对于算法学习预测会影响很大
- 降维的两种方式
    - 特征选择
    - 主成分分析

### 特征选择
- 数据中包含冗余或者相关变量(或称特征，属性，指标等)，旨在从原有特征中找出主要特征
- 方法：
    - Filter(过滤式):主要探究特征本身特点，特征与特征和目标值之间关联
        - 方差选择法：低方差特征过滤 
            - 方差少，数据比较集中
        - 相关系数
            - 可以衡量两个特征之间是否有很强的相关性
    - Embedded(嵌入式):算法自动选择特征（特征与目标值之间的关联）
        - 决策树：信息熵，信息增益
        - 正则化：L1,L2
        - 深度学习：卷积等
- API:sklearn.feature_selection

#### 过滤式
- 低方差特征过滤
    - 删除低方差的一些特征，再结合方差的大小来考虑这个方式的角度
        - 特征方差小：某个特征大多样本的值比较相近 适合删除
        - 特征方差大：某个特征很多样本的值都有差别 适合保留
#### 过滤式API
    - sklearn.feature_selection.VarianceThreshold(threshold=0.0)
        - 删除所有低方差特征
        - Variance.fit_transform(x)
            - x:numpy array 格式的数据[n_sample,n_features]
            - 返回值:训练集差异低于threshold的特征将会被删除，默认值是保留所有非零方差特征，即删除所有样本重具有相同值的特征
        - 初始化VarianceThreshold 指定方差，调用fit_transform
#### 过滤式案例
```python
from sklearn.feature_selection import VarianceThreshold
def variance_demo():
    '''
    过滤低方差特征
    :return:
    '''
    data = pd.read_csv("factor_returns.csv")
    data = data.iloc[:,1:-2]
    transfer = VarianceThreshold(threshold=10)
    data_new = transfer.fit_transform(data)
    print(data_new,data_new.shape)
    # 计算某两个变量之间的相关系数
    r1 = pearsonr(data["pe_ratio"],data["pb_ratio"])
    print("相关系数",r1)

    r2 = pearsonr(data["revenue"],data["total_expense"])
    print("相关系数",r2)
    return  None
```

#### 相关系数
- 皮尔逊相关系数(Pearson Correiation Coefficient)
    - 反映变量之间相关关系密切程度的统计指标
- 特点：
- 相关系数的值介于-1与+1之间，即-1<=r<=+1
    - 当r>0时，表示两变量正相关，r<0,表示两个变量为负相关
    - 当|r|=1时，表示两变量为完全相关，当r=0，表示两变量之间无相关关系
    - 当0<|r|<1时，表示两变量存在一定程度的相关，当|r|越接近1，两变量间线性关系越密切；当|r|越接近0，表示两变量的线性关系越弱
    - 一般可以按三级划分：当|r|<0.4为低度相关；0.4<=|r|<0.7为显著性相关；0.7<=|r|<1为高度线性相关
#### API
    - from scipy.stats import pearsonr
        - x:(N,) array_like
        - y:(N,) array_like Return:(Pearson's correlation coefficient,p-value)
- 特征与特征之间相关性很高：
    - 选择其中一个作为代表
    - 按一定权重加权求和
    - 主成分分析 自动处理将相关性比较强的特征

### 主成分分析PCA
- 可以理解一种特征提取的方式，降维并尽可能的保持原有信息，减少信息的损失
- 定义：高维数据转化成低维数据的过程，在此过程中可以回舍弃原有数据，创造新的数据
- 作用：是数据维度压缩，尽可能降低原数据的维数(复杂度)，损失少量信息
- 应用：回归分析或者聚类分析当中
- 二维降到一维
    - 找到一个合适的直线，通过一个矩阵运算得出主成分分析的结果
#### API
sklearn.decomposition.PCA(n_components=None)
    - 将数据分解为较低维数空间
    - n_components:
        - 小数：表示保留百分之多少的信息
        - 整数：减少到多少特征
    - PCA.fit_transform(x) 
        - x: numpy array格式的数据
        - [n_samples, n_features]
    - 返回值：转换后指定维度的array
#### 案例
```python
from sklearn.decomposition import PCA
def PCA_demo():
    '''
    PCA降维
    :return:
    '''
    data = [[2, 8, 4, 5], [6, 3, 0, 8], [5, 4, 9, 1]]
    #实例化一个转化器类
    transfer = PCA(n_components=0.95)
    #调用方法
    data_new = transfer.fit_transform(data)
    print(data_new)
    return None
```

## sklearn转换器和估计器
### 转换器
- 转换器是特征工程的父类，所有在进行特征工程的时候都是继承自transformer这个类
- 之前做特征工程的步骤
    - 1.实例化 实例化一个转换器类 transformer
    - 2.调用fit_transform（对于文档建立分类词频矩阵，不能同时调用）
    - 我们把特征工程的家口称之为转换器，其中转换器调用有着几个种形式
        - fit_transform()
        - fit() 
        - transform()
    - 标准化公式:(x-mean)/std
        - fit_transform()
        - fit() 计算每一列的平均值和标准差
        - transform() 带入公式(x-mean)/std 进行最终的转换

        
### 预估器
- sklearn机器学习算法的实现都被封装到估计器这个父类当中
- 在sklearn中，估计器estimator是一个重要的角色，是一类实现了算法的API；所有的估计器都是estimator的子类
- 用于分类的估计器：
    - sklearn.neighbors k-近邻
    - sklearn.naive_bayes 贝叶斯
    - sklearn.linear_model.LogisticRegression 逻辑回归
    - sklearn.tree 决策树与随机森林
- 用于回归的估计器
    - sklearn.linear_model.linearRegression 线性回归
    - sklearn.linear_model.Ridge 岭回归
- 用于无监督学习的估计器
    - sklearn.cluster.KMeans 聚类
- 估计器工作流程estimator
    - 1.实例化，实例化一个预估器 estimator
    - 2.调用estimator.fit(x_train,y_trian) 
        - 调用完毕，模型生成
    - 3.模型评估
        - 直接比对真实值和预测值
             y_predict = estimator.predict(x_test)
             y_test == y_predict 生成布尔值，比对结果是否一致
        - 计算准确率
             accuracy = estimator.score(x_test,y_test)

# 分类算法
- 分类问题 回归 输入数据有特征有标签，即有标准答案
    - 目标值：结果为类别
    - k-近邻算法，朴素贝叶斯分类，决策树，随机森林，逻辑回归
- 回归问题
    - 目标值:连续型的数据

## k-近邻算法 KNN算法
### 什么是KNN算法
- K Nearest Neighbor
- 如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别 但是k=1时，容易受到异常点的影响，选择一个合适的k值可以避免受到异常值的影响
- 如何确定谁是邻居
    - 欧式距离 两个样本的距离
    - 曼哈顿距离 绝对值距离 |a1-b1|+|a2-b2|+|a3-b3|
    - 明可夫斯基距离
- 电影类型分析
    - 当k=1的时候，分类为爱情片
    - 当k=2的时候，取两个最近的值
    ...
    - 当k=6的时候，一共就6个样本，一半爱情片，一半动作片 分类无法确定
    - 当k=7的时候，加入一共电影为动作片，这样三个是爱情片，四个是动作片，这个电影归类到动作片，但是实际上这个片子偏向爱情片
- 当k值取的过小的时候，容易受到异常值的影响 
- 当k值取的过大的时候，受到样本不均衡的影响，类型很容易分错
- 无量纲化的处理
    - 标准化
### API
- sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, algorithm='auto')
    - n_neighbors: int,可选(默认=5)，k值，k_neighbors 查询默认使用的邻居数
    - algorithm: {'auto','ball_tree','kd_tree','brute'},可选用于计算最近邻居的算法
        - ball_tree 使用BallTree
        - kd_tree 使用KDTree
        - auto 将尝试根据传递给fit方法的值来决定最合适的算法 不同现实方式影响效率
        
### 案例1 鸢尾花种类预测
- 1.数据集介绍
```python
Iris数据集是常用的分类实验数据集，用称鸢尾花数据集，是一类多重变量分析的数据集。数据集的具体介绍：
实例数量： 150 三个类各有50个
属性数量： 4（数值型，数值型，帮助预测的属性和类）
属性信息：
    sepal length 萼片长度 cm
    sepal width 萼片宽度 cm
    petal length 花瓣长度 cm
    petal width 花瓣宽度 cm
类：
    Iris-SetosaN 山鸢尾
    Iris-Versicolour 变色鸢尾
    Iris-Virginica 维吉妮亚鸢尾
```
- 2.数据集划分
- 3.特征工程
    - 标准化
- 4.机器学习训练
    - KNN预估器流程
- 5.模型评估

```python
# 导入数据集
from sklearn.datasets import load_iris
# 划分数据集
from sklearn.model_selection import train_test_split
# 数据标准化
from sklearn.preprocessing import StandardScaler
# 调用knn算法
from sklearn.neighbors import KNeighborsClassifier

def knn_iris():
    '''
    用knn算法对鸢尾花进行分类
    :return: 
    '''
    # 1获取数据
    iris = load_iris()
    # 2划分数据集 random_state取6
    x_train, x_test, y_train, y_test = train_test_split(iris.data,iris.target,random_state=22)
    # 3特征工程：标准化
    transfer = StandardScaler()
    # 训练集标准化
    x_train = transfer.fit_transform(x_train)
    # 测试集也需要标准化，用训练集的平均值和标准差 模型用同样分布状况的数据
    x_test = transfer.transform(x_test)
    # 4KNN算法预估器,k取3
    # 放入训练集的特征值和目标值放进来
    estimator = KNeighborsClassifier(n_neighbors=3)
    # 相当于有了模型
    estimator.fit(x_train,y_train)
    # 5模型评估
    # 方法1，直接比对真实值和预测值
    # 根据x_train和y_trian模型，预测x_test会得到y_test,再和实际的y_test相比较
    # 得到预测的结果
    y_predict = estimator.predict(x_test)
    print("y_predict:\n", y_predict)
    print("对比真实值和预测值：", y_predict == y_test )
    # 方法2，计算准确率
    score = estimator.score(x_test,y_test)
    print("准确率：\n",score)
    return None

if __name__ == "__main__":
    knn_iris()
    
output:  
y_predict:
 [0 2 1 2 1 1 1 2 1 0 2 1 2 2 0 2 1 1 1 1 0 2 0 1 2 0 2 2 2 2 0 0 1 1 1 0 0
 0]
对比真实值和预测值： [ True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True False  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True]
准确率：
 0.973684210526
```
### KNN算法优缺点
- 优点：简单，易于理解，易于实现，无需训练
- 缺点：
    - 懒惰算法，对测试样本分类时的计算量大，内存开销大
    - 必须指定k值，k值选择不当则分类精度不能保证
- 使用场景：小数据场景，几千~几万样本，具体场景具体业务去测试，大数据不适合用KNN算法

## 模型选择和调优
### 什么是交叉验证(cross validation)  
交叉验证：将拿到的训练数据，分为训练和验证集。将数据分成四份，其中一份作为验证集。然后经过4次(组)的测试，每次都更换不同的验证集，即得到4组模型的结果，取平均数作为最终结果，又称为4折交叉验证。
#### 分析  
我们之前知道数据分成训练集和测试集，但是为类让训练得到模型的结果更为准确，做以下处理  
- 训练集：训练集+验证集
- 测试集：测试集
- 得到不同的准确率，求平均值之后，平均值作为模型最终准确率
```
一组：验证集 训练集 训练集 训练集  80%
二组：训练集 验证集 训练集 训练集  78%
三组：训练集 训练集 验证集 训练集  75%
四组：训练集 训练集 训练集 验证集  82%
```
#### 为什么需要交叉验证
- 交叉验证目的：为了让被评估的模型更加准确可信
- 如何选择或者调优参数

### 超参数搜索-网格搜索(Grid Search)
超参数 通常情况下，狠毒偶参数是需要手动指定的(如k-近邻算法中的k值)，这种叫超参数  

但是手动过程繁杂，所以需要对模型预设几种超参数组合。每组超参数都采用交叉验证来进行评估。最后选出最优参数组合建立模型。
```
k值   k=3   k=5   k=7
模型 模型1 模型2 模型3
```
### 模型选择与调优API
sklearn.model_selection.GridSearchCV(estimator,param_grid=None,cv=None)
    - 对估计器的指定参数进行详尽搜索
    - estimator:估计器对象
    - param_grid:估计器参数(dict)("n_neighbors":[1,3,5]) 准备好的k的取值，以字典的 形式传进来
    - cv:指定几折交叉验证 经过几次交叉验证 一般是10折
    - fit():输入训练数据
    - score():准确率
    - 结果分析:
        - 最佳参数: best_params_
        - 最佳结果: best_score_
        - 最佳估计器: best_estimator_
        - 交叉验证结果: cv_results_
        - p=1是曼哈顿距离，p=2是欧式距离
### 鸢尾花案例增加K值调优
```python
# 导入数据集
from sklearn.datasets import load_iris
# 划分数据集
from sklearn.model_selection import train_test_split
# 数据标准化
from sklearn.preprocessing import StandardScaler
# 调用knn算法
from sklearn.neighbors import KNeighborsClassifier
# 调用gscv调优
from sklearn.model_selection import GridSearchCV

def knn_iris_gscv():
    '''
    用knn算法对鸢尾花进行分类, 添加网格搜索和交叉验证
    :return:
    '''
    # 获取数据
    iris = load_iris()
    # 划分数据
    x_train, x_test, y_train, y_test = train_test_split(iris.data,iris.target,random_state=6)
    # 特征工程：标准化
    transfer = StandardScaler()
    x_train = transfer.fit_transform(x_train)
    x_test = transfer.transform(x_test)
    # KNN算法预估器，因为要试，所以这里不需要加上k值
    estimator = KNeighborsClassifier()
    # 加入网格搜索和交叉验证
    ## 参数准备 k值 字典的格式
    param_dict = {"n_neighbors":[1,3,5,7,9,11]}
    estimator = GridSearchCV(estimator,param_grid=param_dict,cv=10)
    estimator.fit(x_train,y_train)
    # 模型评估
    y_predict = estimator.predict(x_test)
    print("y_predict:\n", y_predict)
    print("对比真实值和预测值：", y_predict == y_test )
    score = estimator.score(x_test,y_test)
    print("准确率：\n",score)
    # 最佳参数: best_params_
    print("最佳参数：",estimator.best_params_)
    # 最佳结果: best_score_
    # 此最佳结果是训练集中验证集当中的结果
    print("最佳结果",estimator.best_score_)
    # 最佳估计器: best_estimator_
    print("最佳估计器",estimator.best_estimator_)
    # 交叉验证结果: cv_results_
    print("交叉验证结果",estimator.cv_results_)
    return None

if __name__ == "__main__":
    knn_iris_gscv()

output:
y_predict:
 [0 2 0 0 2 1 2 0 2 1 2 1 2 2 1 1 2 1 1 0 0 2 0 0 1 1 1 2 0 1 0 1 0 0 1 2 1
 2]
对比真实值和预测值： [ True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True False  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True False  True
  True  True]
准确率：
 0.947368421053
最佳参数： {'n_neighbors': 5}
最佳结果 0.973214285714
```
### 预测facebook签到位置  

数据集介绍  
本次大赛的目的是预测一个人想签入到哪个地方。FaceBook创建了一个人造的世界，10公里*10公里的一个区域。对于一个给定的坐标，你的任务是返回最有可能的地方的排名列表  

数据集数据  
```
train_csv, test_csv
row_id: id of the check_in event 登记事情的id
x y: coordiantes 坐标系
accuracy: location accuracy 定位准确度
time: timestamp 时间戳
place_id: id of the business. this is the target you are predicting   预测签到的位置 
```
#### 流程分析
- 获取数据
- 数据处理
    - 为了减少时间，缩小数据范围
        - 2<x<2.5   1.0<y<1.5
    - time处理成有意义的数据年月日时分秒 
    - 过滤签到次数少的地点
    - 特征值 x
    - 目标值 y
- 特征工程：标准化
- KNN算法预估流程
- 模型选择与调优
- 模型评估

## 朴素贝叶斯算法
### 什么是朴素贝叶斯分类方法
- 贝叶斯分类是一系列分类算法的总称,这类算法均以贝叶斯定理为基础,故统称为贝叶斯分类。朴素贝叶斯算法（Naive Bayesian) 是其中应用最为广泛的分类算法之一
- 朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法
- 朴素: 特征和特征之间是相互独立的 贝叶斯: 贝叶斯公式
- 朴素贝叶斯分类器基于一个简单的假定：给定目标值时属性之间相互条件独立,特征和特征之间是相互独立的。

### 概率基础
- 概念定义为一件事情发生的可能性
- P(X):取值在[0,1]

### 联合概率，条件概率与互相独立
- 联合概率:包含多个条件，且所有条件同时成立的概率
    - 记作:P(A,B)
- 条件概率:就是事件A在另外一个事件B已经发生的条件下发生的概率
    - 记作:P(A|B)
- 相互独立: 如果P(A,B)=P(A)P(B),则称事件A与事件B相互独立

### 贝叶斯公式
P(C|W)=P(W|C)P(C)/P(W)

### 应用场景
- 文本分类
    - 单词作为特征，词和词之间是相互独立的
- 公式可以理解为:  
    - P(C|F1,F2...)=P(F1,F2...|C)P(C)/P(F1,F2...)
    - P(C):每个文档类别的概率(某文档类别数/总文档数量)
    - P(W|C):给定类别下特征(被预测文档中出现的词)的概率
        - 计算方法: P(F1|C)=Ni/N(训练文档中出现的次数)
        - N为所属类别C下的文档所有词出现的次数和
    - P(F1,F2...)预测文档中每个词的频率
- 如果计算两个类别概率比较:
    - 所以我们只要比较前面的大小就可以，得出谁的概率大
### 案例
```
        文档ID   文档中的词                  属于c=China类
训练集    1      Chinese Beijing Chinese        yes
          2      Chinese Chinese Shanghai       yes
          3      Chinese Macao                  yes
          4      Tokyo Japan Chinese            No
测试集    5      Chinese Chinese Chinese Tokyo Janpan ？
P(C)=3/4  
P(非C)=1/4
P(C|Chinese，Chinese，Chinese，Tokyo，Japan) = P(Chinese，Chinese，Chinese，Tokyo，Japan|C)P(C)/P(Chinese，Chinese，Chinese，Tokyo，Japan)
## 因为贝叶斯算法下，特征和特征之间是相互独立的
## 出现0，样本量太少，需要引入拉普拉斯平滑系数
P(Chinese|C) = 5/8  P(Tokyo|C) = 0/8 P(Japan|C) =0/8
P(Chinese，Chinese，Chinese，Tokyo，Japan|C) = P(Chinese|C)^3*P(Tokyo|C)*P(Japan|C) = 

P(非C|Chinese，Chinese，Chinese，Tokyo，Japan) =  P(Chinese，Chinese，Chinese，Tokyo，Japan|非mC)P(非C)/P(Chinese，Chinese，Chinese，Tokyo，Japan)

分母一样，只要比较分子
```
### 拉普拉斯平滑系数
- 目的: 防止计算出的分类概率为0
- P(F1|C)=(Ni+α)/(N+αm)
    - α为指定系数一般为1，m为训练集文档中统计出的特征词个数
```
P(Chinese|C) = (5+1)/(8+1*6)=3/7  
P(Tokyo|C) = (0+1)/(8+1*6)=1/14 
P(Japan|C) =(0+1)/(8+1*6)=1/14
```
### API
- sklearn.navie_bayes.MultinomialNB(alpha=1.0)
    - 朴素贝叶斯分类
    - alpha: l拉普拉斯平滑系数

### 案例二十类新闻分类
- 获取数据  
    - 获取的是sklearn的数据，所以我们不需要处理
    - 新闻组数据集是大约20000个新闻组文档的集合，平均分布在20个不同的新闻组中。数据被组织成20个不同的新闻组，每个新闻组对应不同的主题。一些新闻组彼此之间关系密切，而另外一些新闻组则非常不相关
- 进行数据集的分割
- 特征工程：
    - TFIDF进行的特征抽取
        - 将文章字符串进行单词抽取
- 朴素贝叶斯预估器预测
- 模型评估

```python
from sklearn.datasets import fetch_20newsgroups
# 划分数据集
from sklearn.model_selection import train_test_split
# 文本数据抽取
from sklearn.feature_extraction.text import TfidfVectorizer
# 朴素贝叶斯算法
from sklearn.naive_bayes import MultinomialNB

def nb_news():
    '''
    用朴素贝叶斯算法对新闻进行分类
    :return:
    '''
    # 获取数据
    news = fetch_20newsgroups(subset= "all")
    # 划分数据集
    x_train,x_test,y_train,y_test = train_test_split(news.data,news.target)
    # 特征工程：文本特征抽取 TFIDF方式
    transfer = TfidfVectorizer()
    x_train = transfer.fit_transform(x_train)
    x_test = transfer.transform(x_test)
    # 朴素贝叶斯预估器预测,默认的alpha为1.0
    estimator = MultinomialNB()
    estimator.fit(x_train,y_train)
    # 模型评估
    y_predict = estimator.predict(x_test)
    print("y_predict:\n", y_predict)
    print("对比真实值和预测值：", y_predict == y_test)
    score = estimator.score(x_test, y_test)
    print("准确率：\n", score)
    return None
    
output:
y_predict:
 [ 7 18 14 ..., 11 18 13]
直接比对真实值和预测值:
 [False  True  True ...,  True  True  True]
准确率为：
 0.850594227504

```
### 朴素贝叶斯算法总结
- 优点：
    - 朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率
    - 对缺失数据不太敏感，算法比较简单，常用于文本分类
    - 分类准确度高，速度快
- 缺点：
    - 由于使用类样本属性独立性的假设，所有如果特征属性有关联时其效果不好


## 决策树
### 认识决策树
- 决策树思想的来源非常朴素，程序设计中的条件分支结构就是if-else结构，最早的决策树就是利用这类结构分割数据的一种分类学习
- 如何高效的进行决策
- 确定快速的确定特征的先后顺序

### 决策树分类原理详解
- 信息论
    - 信息: 消除随机不定性的东西
    - 信息是物质、能量、信息及其属性的标示。
    - 信息是确定性的增加。
    - 信息是事物现象及其属性标识的集合。
- 信息熵
    - 来衡量我们消除的不确定的程度
    - H的专业术语称为信息熵，单位为比特bit
        - 公式:  
![公式](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/%E4%BF%A1%E6%81%AF%E7%86%B5.png)  

- 信息熵性质
    - 单调性，即发生概率越高的事件，其所携带的信息熵越低。极端案例就是“太阳从东方升起”，因为为确定事件，所以不携带任何信息量。从信息论的角度，认为这句话没有消除任何不确定性。
    - 非负性，即信息熵不能为负。这个很好理解，因为负的信息，即你得知了某个信息后，却增加了不确定性是不合逻辑的。
    - 累加性，即多随机事件同时发生存在的总不确定性的量度是可以表示为各事件不确定性的量度的和。

- 决策树的划分依据之一 信息增益
    - 特征A对训练数据集D的信息增益g(D,A), 定义为集合D的信息熵H(D)与特征A给定条件下D的信息条件熵H(D|A)之差。
    - 信息增益表示得知特征X的信息从而信息的不确定性减少的程度，使得总的信息熵减少的程度
    - g(D|A) = H(D) - H(D|A)
    - 公式:  
![公式](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/%E6%9D%A1%E4%BB%B6%E7%86%B5.png)     

### 银行贷款案例
![银行贷款案例](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%86%B3%E7%AD%96%E6%A0%91/%E9%93%B6%E8%A1%8C%E8%B4%B7%E6%AC%BE%E6%95%B0%E6%8D%AE.png)  
- g(D|A) = H(D) - H(D|A)
         = H(D) - (5/15H(青年)+5/15H(中年)+5/15H(老年))
- H(D) = -(6/15*log(6/15)+9/15*log(9/15))
- H(青年) = -(3/5log(3/5)+2/5log(2/5))
- H(中年) = -(3/5log(3/5)+2/5log(2/5))
- H(老年) = -(4/5log(3/5)+1/5log(2/5))
- 以A1,A2,A3,A4代表年龄，工作，房子和贷款情况。最终结果g(D|A1)=0.313 g(D|A2)=0.324 g(D|A3)=0.420 g(D|A4)=0.363 从而我们选择A3房子作为划分的第一个特征。

当然决策树的原理不止信息增益这一种，还有其他方法
- ID3
    - 信息增益，最大准则
- C4.5
    - 信息增益比 最大准则
- CART
    - 分类树: 基尼系数 最小准则 在sklearn中可选择划分的默认原则
    - 优势: 划分更加细致
    

### 决策树API
- sklearn.tree.DecisionTreeClassifier(criterion='gini',max_depth=None,random_state=None)
    - 决策树分类器
    - criterion:决策树的划分依据, 默认是'gini'系数，也可以选择信息增益的熵'entropy'
    - max_depth:数的深度大小 (如果分的过细，很有可能泛化能力比较差，在训练集上表现的很好，但是在测试集上表现就没那么好，此时可以设置下数的深度大小，提高准确率)
    - random_state:随机数种子
### 案例
```python
# 导入数据集
from sklearn.datasets import load_iris
# 划分数据集
from sklearn.model_selection import train_test_split
# 决策树分类器
from sklearn.tree import DecisionTreeClassifier

def tree_iris():
    '''
    用决策树对鸢尾花进行分类
    :return:
    '''
    # 获取数据
    iris = load_iris()
    # 划分数据
    x_train,x_test,y_train,y_test = train_test_split(iris.data,iris.target,random_state=22)
    # 使用决策树预估器进行分类
    estimator = DecisionTreeClassifier(criterion="entropy")
    estimator.fit(x_train,y_train)
    # 模型评估
    estimator.predict(x_test)
    print("y_predict:\n", y_predict)
    print("直接比对真实值和预测值:\n", y_test == y_predict)

    # 方法2：计算准确率
    score = estimator.score(x_test, y_test)
    print("准确率为：\n", score)
    return None
    
output:
y_predict:
 [0 2 1 2 1 1 1 1 1 0 2 1 2 2 0 2 1 1 1 1 0 2 0 1 2 0 1 2 2 1 0 0 1 1 1 0 0
 0]
    tree_iris()
直接比对真实值和预测值:
 [ True  True  True  True  True  True  True False  True  True  True  True
  True  True  True  True  True  True False  True  True  True  True  True
  True  True False  True  True False  True  True  True  True  True  True
  True  True]
准确率为：
 0.894736842105
```
- 决策树模型计算出来的准确率为89.47%, 而之前用knn算法算出来的结果准确率为97%。实际上不同的算法有不同的应用环境，决策树更适合用于数据量比较大的数据集

### 决策树可视化API
- sklearn.tree.export_graphviz() 该函数能导出DOT格式
    - tree.export_graphviz(estimator,out_file='tree.dot',feature_names=[","])
```
export_graphviz(estimator, out_file="iris_tree.dot", feature_names=iris.feature_names)
```

```
digraph Tree {
node [shape=box] ;
0 [label="petal width (cm) <= 0.75\nentropy = 1.584\nsamples = 112\nvalue = [39, 37, 36]"] ;
1 [label="entropy = 0.0\nsamples = 39\nvalue = [39, 0, 0]"] ;
0 -> 1 [labeldistance=2.5, labelangle=45, headlabel="True"] ;
2 [label="petal width (cm) <= 1.75\nentropy = 1.0\nsamples = 73\nvalue = [0, 37, 36]"] ;
0 -> 2 [labeldistance=2.5, labelangle=-45, headlabel="False"] ;
3 [label="petal length (cm) <= 5.05\nentropy = 0.391\nsamples = 39\nvalue = [0, 36, 3]"] ;
2 -> 3 ;
4 [label="sepal length (cm) <= 4.95\nentropy = 0.183\nsamples = 36\nvalue = [0, 35, 1]"] ;
3 -> 4 ;
5 [label="petal width (cm) <= 1.35\nentropy = 1.0\nsamples = 2\nvalue = [0, 1, 1]"] ;
4 -> 5 ;
6 [label="entropy = 0.0\nsamples = 1\nvalue = [0, 1, 0]"] ;
5 -> 6 ;
7 [label="entropy = 0.0\nsamples = 1\nvalue = [0, 0, 1]"] ;
5 -> 7 ;
8 [label="entropy = 0.0\nsamples = 34\nvalue = [0, 34, 0]"] ;
4 -> 8 ;
9 [label="sepal length (cm) <= 6.05\nentropy = 0.918\nsamples = 3\nvalue = [0, 1, 2]"] ;
3 -> 9 ;
10 [label="entropy = 0.0\nsamples = 1\nvalue = [0, 1, 0]"] ;
9 -> 10 ;
11 [label="entropy = 0.0\nsamples = 2\nvalue = [0, 0, 2]"] ;
9 -> 11 ;
12 [label="petal length (cm) <= 4.85\nentropy = 0.191\nsamples = 34\nvalue = [0, 1, 33]"] ;
2 -> 12 ;
13 [label="entropy = 0.0\nsamples = 1\nvalue = [0, 1, 0]"] ;
12 -> 13 ;
14 [label="entropy = 0.0\nsamples = 33\nvalue = [0, 0, 33]"] ;
12 -> 14 ;
}
```
- 网站显示结构
    - webgraphviz.com/

### 决策树总结
- 优点: 简单的理解和解释，树木可视化
- 缺点:
    - 决策树学习者可能创建不能够很好的用来推广数据的过于复杂的数，这样会被称为过拟合
- 改进:
    - 减技cart算法(决策树API当中已经实现，随机森林参数调优有相关介绍)
    - 随机森林
    - 企业重要决策，由于决策树很好的分析能力，在决策过程应用较多，因为可以选择特征
 

## 随机森林
### 什么是集成学习方法
- 集成学习通过建立几个模型组合来解决单一预测问题。他的工作原理就是生成多个分类器/模型。各自独立地学习和作出预测。这些预测最后结合成组合预测，因此优于任何一个单分类的作出预测

### 什么是随机森林
- 集成学习方法中的一个，在机器学习中，随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别数输出的类别的众数而定
- 例如，如果你训练5个树，其中4个树的结果是True，一个树的结果是False，那么最终结果就是True

### 随机森林原理过程
- 训练集随机，特征值随机
- 用N来表示训练用树(样本)的个数，M表示特征数目
    - 一次随机选出一个样本，重复N次，有可能出现重复的样本
    - 随机去选m个特征，m<<M，建立决策树，可以起到降维的结果
- 采取bootstrap抽样

### 为什么采用bootstrap抽样
随机有放回抽样 这样每一个训练集都是独有的
- 为什么要随机抽样训练集
    - 如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的
- 为什么要有放回的抽样
    - 如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是有偏的，都是绝对片面的，每棵树训练出来都是有很大的差异，而随机森林最后分类取决于多棵树(弱分类器)的投票表决

### API
-   sklearn.ensemble.RandomForestClassifier(n_estimators=10,criterion='gini',max_depth=None,bootstrap=True,random_state=None,min_sample_split=2)
    - n_estimators: integer,optional(default=10) 森林里面数目的数量
    - criterion:string，可选(default='gini') 分割特征的测量方法
    - max_depth: integer或None，可选(默认=无) 树的最大深度 5，8，15，25，30
    - bootstrap: boolean,optional(default=True) 是否在构建树是放回抽样
    - max_features='auto',每个决策树最大特征数量 m
        - if auto, then max_features=sqrt(n_features)
        - if sqrt, then max_features(n_features) same as auto
        - if log2 then max_features=log2(n_features)
        - if None, then max_features=n_features m=M 达不到降维的效果，每棵树的时间很长
    - min_samples_split:节点划分最少样本数
    - min_samples_leaf:叶子节点的最小样本数
- 超参数:n_estimators,max_depth,min_samples_split,min_samples_leaf

### 随机森林总结
- 在当前所有算法中，具有极好的准确率
- 能够有效的运行在大数据集上，处理具有高维特征的输入样本，而且不需要降维
- 能够评估各个特征在分类问题上的重要性

## 逻辑回归与二分类
- 逻辑回归(Logistic Regression)是机器学习中的一种分类模型，逻辑回归是一种分类算法。由于算法的简单和高效，在实际中应用广泛

### 逻辑回归应用场景
- 广告点击率
- 是否为垃圾邮件
- 是否患病
- 金融诈骗
- 虚假账号
- 以上的例子，发现其中的特点，那就是都是属于两个类别之间的判断，逻辑回归就是解决二分类问题的利器

### 逻辑回归的原理
- 输入  
    - h(w)=w1x1+w2x2+w3x3...+b
    - 逻辑回归的输入就是一个线性回归的结果
    - 线性回归的输出，就是逻辑回归的输入
- 激活函数
    - sigmoid函数
    - 将回归结果输入到sigmoid函数当中
    - 输出结果:[0,1]区间中的一个概率值，默认为0.5的阈值 (阈值可以设定，大于阈值，属于这个分类，小于阈值则不属于这个分类)
- 逻辑回归最终的分类是通过属于某个类别的概率值来判断是否属于某个类别，并且这个类别默认标记为1(正例)，另外的一个类别会标记为0(反例)，方便损失计算

![逻辑回归激活函数](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.png)
![函数图像](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/%E5%87%BD%E6%95%B0%E5%9B%BE%E5%83%8F.png)

### 损失函数
- 对数似然损失
    - 分开类别
    - y=1 真实值属于这个类别；当真实值为1，输出结果h(x)越接近1，预测的越准确，则损失函数越低，接近为0
    - y=0 真实值不属于这个类别；当真实值为0，输出结果h(x)越接近0，同理预测结果越准确，则损失函数越低，接近为0

![分开类别](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/%E5%AF%B9%E6%95%B0%E4%BC%BC%E7%84%B6%E6%8D%9F%E5%A4%B1.png)
![当y=1](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/%E5%AF%B9%E6%95%B0%E4%BC%BC%E7%84%B6%E6%8D%9F%E5%A4%B1y%3D1.png)
![当y=0](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/%E5%AF%B9%E6%95%B0%E4%BC%BC%E7%84%B6%E6%8D%9F%E5%A4%B1y%3D0.png)

- 综合完整损失函数
![综合完整损失函数](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/%E7%BB%BC%E5%90%88%E5%AE%8C%E6%95%B4%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.png)

### 优化损失
- 同样使用梯度下降优化算法，去减少损失函数的值，这样去更新逻辑回归前面对应算法的权重参数，提高原本属于1类别的概率，降低原本是0类别的概率

### 逻辑回归API
- sklearn.linear_model.LogisticRegression(solver='liblinear',penalty='L2',C=1.0)
    - solver: 优化求解方式(默认开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数)
        - sag:根据数据集自动选择，随机平均梯度下降
    - pennality: 正则优化的种类 防止过拟合
    - C: 正则化力度
- 默认将类别数量少的当做正例
- LogisticRegression方法相当于SGDClassifier(loss='log',penalty='')(Classifier分类器),SGDClassifier实现一个普通的随机梯度下降学习，也支持平均随机梯度下降法(ASGD),可以通过设置average=True。而使用LogisticRegression实现了SAG，solver='sag'

### 案例癌症分类预测
- 癌症分类预测-良/恶性乳腺癌肿瘤预测

## 分类的评估方法
- 真的患癌症，能够被检查出来的概率
### 精确率与召回率
- 混淆矩阵
    - 在分类任务下，预测结果(Predicted Codition)与正确标记(True Condition)之间存在四种不同的组合，构成混淆矩阵(适用于多分类)
    - TP True possitive
    - FN False Negative
![混淆矩阵](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5.png)

- 精确率Precision和召唤率Recall
    - 精确率: 预测结果为正例样本中真实为正例的比例
        - TP/(TP+FP)
    - 召回率: 真实为正例的样本中预测结果为正例的比例(查的全，对正样本的区分能力)
        - TP/(TP+FN)
        - 真正患癌症的，能够被检测出来的概率；查的全不全
        - 应用场景: 癌症，工厂质量检测
- F1-score 反映模型的稳健性
![F1-score](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/F1-score.png)

### 分类评估API
- sklearn.metrics,classification_report(y_ture,y_pred,label=[],target_names=None)
    - y_true:真实目标值
    - y_pred:估计器预测目标值
    - labels:指定类别对应的数字
    - target_names:目标类别名称
    - return:每个目标精确率和召回率
```
print('精确率和召回率为:', classification.report(y_test,lr.predict(x_test)))
```
### ROC曲线与AUC指标

总共有100个样本，其中99个样本为癌症，一个样本非癌症  
不管怎么样，我们全部预测正例(默认癌症为正例)- 不负责任  
准确率: 99/100=99%  
召回率: 99/99=100%  
精确率: 99/100=99%  
F1-score: 99.497%  2*99%/(100%+99%)=99.497%  

造成这样的结果是因为样本不均衡，正样本太多，反例太少，我们需要找到能够衡量样本不均衡情况下分类器的效果引入ROC曲线与AUC指标  
AUC:0.5  
     TPR: 100%  
     FPR: 1/1=100%  
     
#### 知道TPR与FPR
- TPR = TP/(TP+FN) 召回率 
    - 所有真实类别为1的样本中，预测类别为1的比例
- FPR = FP/(FP+TN)
    - 所有真实类别为0的样本中，预测类别为1的比例
- TPR = FPR 不负责任的模型

#### ROC曲线
- ROC曲线的横轴就是FPRate,纵轴就是TPRate，当二者相等时，表示的意义则是:对于不论真实类别是1还是0的样本，分类器预测为1的概率是相等的，此时AUC为0.5
![ROC曲线](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/ROC%E6%9B%B2%E7%BA%BF.png)
- TPR>FPR, 最小值AUC为0.5，最大值极限AUC为1，TPR为1，FPR为0，即所有真实类别为1的样本中，都预测正确为1
- AUC的概率意义就是随机取一对正负样本，正样本得分大于负样本的概率
- AUC为0.5时，和random guess那条红线重合，是非常不负责任模型
- AUC的最小值为0.5 最大为1 取值越高越好
- AUC为1，完美分类器，采用这个预测模型是，不管设定什么阈值都能得出完美预测，绝大多数预测的场合，不存在完美分类器
- 0.5<AUC<1，优于随机猜测，这个分类器(模型)妥善设定阈值的话，能有预测价值
- 最终AUC的范围在[0.5,1]之间，并且越接近1越好
- 如果出现AUC小于0.5的情况则反着看

### API
- AUC计算API
- from sklearn.metrics import roc_auc_score
    - sklearn.metrics.roc_auc_score(y_true,y_score)
        - 计算ROC曲线面积，即AUC值
        - y_true: 每个样本的真实类别，必须为0(反例)，1(正例)标记
        - y_score：预测得分，可以是正类的预计概率，置信值或者分类器方法的返回值

### 总结
- AUC只能用来评价二分类
- AUC非常适合评价样本不平衡中的分类性能

# 回归算法
- 目标值为连续型的数据，这类问题就叫做回归问题，解决这类问题的算法就叫做回归算法

## 线性回归
### 定义
- 线性回归 (Linear regression)是利用回归方程(函数)对一个或多个自变量(特征值)和因变量(目标值)之间的关系进行建模的一种分析方法
- 线性关系&线性模型:线性关系一定是线性模型。但是线性模型不一定是线性关系，可能是非线性关系
- 特点: 只有一个自变量的情况称为单变量回归，多于一个自变量情况的叫做多元回归
- 通用公式: h(w)=w1x1+w2x2+w3x3+...wnxn+b=(w^T)x+b (x1,x2,x3...特征值) (w1,w2,w3...权重值,回归系数) 
    - 多元回归，其中w,x可以理解为矩阵，结果为矩阵乘法: 
![线性回归](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%85%AC%E5%BC%8F.png) 
    - 单元回归 y=kx+b
- 我们看特征值与目标值之间建立一个关系，找到最合适的函数(w1,w2,w3...权重值,回归系数，模型参数)以及b，这个关系可以理解为线性模型，这个模型满足线性关系

### 线性回归的特征与目标的关系分析
- 线性回归当中线性模型有两种，一种是线性关系，另一种是非线性关系
- 线性关系:单特征与目标值的关系呈直线关系，或者两个特征与目标值呈现平面关系 自变量一次
- 非线性关系:不能再用一条直线表示 y=w1x1+w2x1^3+w4x2^3+...+b 参数一次

### 线性回归的损失和优化原理
- 目标：求模型参数，模型参数能够使得预测准确 

#### 损失函数/cost/成本函数/目标函数
![损失函数](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.png) 
    - hw(x_i)为第i个训练样本特征值组合预测函数
    - y_i为真实值
    - 又称为最小二乘法
    
#### 优化算法
- 如果求出模型当中的wi，使得损失最小
- 有两种优化算法
    - 正规方程
![正规方程](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B.png)
        - 矩阵求导 矩阵*逆矩阵=1
        - 理解: x为特征值矩阵，y为目标值矩阵，直接求到最好的结果
        - 缺点: 当特征过多或过复杂时，求解速度太慢并且得不到结果
    - 梯度下降 Gradient Descent
![梯度下降](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D.png)
        - α为学习速率，需要手动定值(超参数)，α旁边的整体表示方向，沿着这个函数下降的方向找，最后就能找到山谷的最低点，然后更新w值使用；面对训练数据模型十分庞大的任务，能够找到很好的结果

### API
- sklearn.linear_model.LinearRegression(fit_intercept=True)
    - 通过正规方程优化
    - fit_intercept: 是否计算偏置 
    - LinearRegression.coef_:回归系数
    - LinearRegression.intercept:偏重
- sklearn.linear_model.SGDRegressior(loss='squared_loss',fit_intercept=True,learning_rate='invscaling',eta0=0.01)
    - SGDRegressor类实现了随机梯度下降学习，他支持不同的loss函数和正则化惩罚项来拟合线性回归模型
    - loss:损失类型
        - loss='squared_loss':普通最小二乘法
    - fit_intercept:是否计算偏值
    - learning_rate:string,optional
        - 学习率填充
        - 'constant':eta=eta()  默认的是0.01 学习率保持在一个数据不变
        -  'optimal':eta=1.0/(alpha*(t+t0))[default]
        -  'invscaling':eta=eta0/pow(t,power_t) 默认下降的步长invscaling
            - power_t=0.25:存在于父类当中
        - 对于一个常数值的学习率来说，可以使用learning_rate='constant', 并使用eta0来指定学习率
    - SGDRegressor.coef_: 回归系数
    - SGDRegressor.intercept_:偏置


### 模型评估方法
- 均方误差
#### 回归当中的数据大小不一致，是否为导致结果影响较大，所以需要做标准化处理
    - 数据分割与标准化处理
    - 回归预测
    - 线性回归的算法效果评估
#### 回归性能评估
    - 均方误差(Mean Squared Error MSE)评价机制
![模型评估方法](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%9B%9E%E5%BD%92%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0.png)
#### API
- sklearn.metrics.mean_squared_erroy(y_true,y_pred)
    - 均方误差回归损失
    - y_true:真实值
    - y_pred:预测值
    - return:浮点数结果

### 案例波士顿房价预测
- 数据集介绍,获取数据集
```
实例数量:506
属性数量: 13数值型或类别型，帮助预测的属性；中心位(第14个属性) 经常是学习的目标
属性信息(按顺序)
- CRIM 城镇人均犯罪率 连续值
- ZN 占地面积超过2.5万平方英尺的住宅用地比例 连续值
- INDUS 城镇非零售业务地区的比例 连续值
- CHAS 查尔斯河虚拟变量(=1如果土地在河边；否则为0)，是否邻近查尔斯河 离散值，1=邻近；0=不邻近
- NOX 一氧化氮浓度(每1000万份) 连续值
- RM 每栋房屋的平均客房数 连续值
- AGE 在1940年之前建成的自用单位比例 连续值
- DIS 与五个波士顿就业中心的加权距离 连续值
- RAD 辐射状公路的可达性指数 连续值
- TAX 每10000美元的全额物业税率，全值财产税收 连续值
- PTRATIO 城镇师生比例 连续值
- B 1000(Bk-0.63)^2 其中Bk是城镇的黑人比例 连续值
- LSTAT 低收入人群占比 连续值
- MEDV 同类房屋价格的中位数 连续值

缺失属性值： 无
创建者：Harrison D andRubinfeld D,L
```
- 划分数据集
- 特征工程
    - 无量纲化-标准化
- 预估器流程
    - 可以调整学习率
    - fit()--->模型coef_ intercept_
```
from sklearn.datasets import  load_boston
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression,SGDRegressor
from sklearn.metrics import mean_squared_error
def linear_demo1():
    '''
    正规方程的优化方法对波士顿房价进行预测
    :return:
    '''
    # 获取数据
    boston = load_boston()
    # 划分数据集
    x_train,x_test,y_train,y_test = train_test_split(boston.data,boston.target,random_state=22)
    # 特征工程 标准化
    transfer = StandardScaler()
    x_train = transfer.fit_transform(x_train)
    x_test = transfer.fit_transform(x_test)
    # 预估器流程
    estimator = LinearRegression()
    estimator.fit(x_train,y_train)
    # 得出模型
    print("正规方程权重系数为：\n", estimator.coef_)
    print("正规方程偏置为:\n",estimator.intercept_)
    # 模型评估
    y_predict = estimator.predict(x_test)
    print("正规方程预测房价：\n", y_predict)
    error = mean_squared_error(y_test,y_predict)
    print("正规方程均方误差为:\n",error)
    return None


def linear_demo2():
    '''
    梯度下降的优化方法对波士顿房价进行预测
    :return:
    '''
    # 获取数据
    boston = load_boston()
    # 划分数据集
    x_train,x_test,y_train,y_test = train_test_split(boston.data,boston.target,random_state=22)
    # 特征工程 标准化
    transfer = StandardScaler()
    x_train = transfer.fit_transform(x_train)
    x_test = transfer.fit_transform(x_test)
    # 预估器流程, 学习率的算法constant，eta0=0.001 迭代次数10000
    estimator = SGDRegressor(learning_rate="constant",eta0=0.01,max_iter=10000)
    estimator.fit(x_train,y_train)
    # 得出模型
    print("梯度下降权重系数为：\n", estimator.coef_)
    print("梯度下降偏置为:\n",estimator.intercept_)
    # 模型评估
    y_predict = estimator.predict(x_test)
    print("梯度下降预测房价：\n", y_predict)
    error = mean_squared_error(y_test, y_predict)
    print("梯度下降均方误差为:\n", error)
    return None

if __name__ == "__main__":
    linear_demo1()
    linear_demo2()
    
output:
正规方程权重系数为：
 [-0.63330277  1.14524456 -0.05645213  0.74282329 -1.95823403  2.70614818
 -0.07544614 -3.29771933  2.49437742 -1.85578218 -1.7518438   0.8816005
 -3.92011059]
正规方程偏置为:
 22.6213720317
正规方程预测房价：
 [ 28.15624208  31.30869316  20.51485702  31.48205292  19.01722351
  18.25171434  20.57503703  18.45503556  18.46192151  32.94820922
  20.36213103  27.24752425  14.81963448  19.21146435  37.02505033
  18.32408346   7.70119888  17.56478207  30.19561854  23.61297215
  18.13379616  33.84017096  28.49921616  16.99629682  34.76148752
  26.227388    34.84170356  26.6267998   18.63962161  13.21549955
  30.36603792  14.70412444  37.18508975   8.91445391  15.06484067
  16.12468763   7.21797311  19.16335583  39.57444328  28.24501235
  24.62961494  16.72956407  37.82734499   5.70546434  21.20919004
  24.63811904  18.85963528  19.93919917  15.20065511  26.3036171
   7.4251188   27.14868579  29.19076714  16.28206033   7.94953105
  35.46279456  32.39096932  20.83555382  16.41378444  20.87373635
  22.92853043  23.61293997  19.32937197  38.34148716  23.87879591
  18.96954218  12.59209375   6.13512682  41.45864696  21.09486655
  16.23896752  21.48997696  40.7412586   20.4923302   36.81939833
  27.05431089  19.80309379  19.61594823  24.59557969  21.0926586
  30.92608611  19.33654808  22.30425056  31.09257529  26.36682634
  20.25040256  28.82330164  20.82975275  26.02088244  19.38265499
  24.96346722  22.30487912  18.92534649  18.86319188  14.02247729
  17.42627701  24.19757886  15.83147538  20.07623475  26.5274431
  20.1203599   17.01175154  23.87970119  22.84994222  21.01501787
  36.18004225  14.68047932  20.5703347   32.46950515  33.24267189
  19.81863526  26.56674006  20.90143053  16.41628584  20.76797132
  20.55412335  26.86514583  24.14833578  23.24521672  13.80658275
  15.37015731   2.78305654  28.90480222  19.78978857  21.50300566
  27.54836226  28.55537501]
正规方程均方误差为:
 20.0616866377
梯度下降权重系数为：
 [-1.1367301   1.29981158  0.01371703  0.70522691 -1.87924507  3.02783621
 -0.13158741 -3.35524493  2.51781459 -1.98037893 -2.10577075  0.74408847
 -4.14145192]
梯度下降偏置为:
 [ 22.98278092]
梯度下降预测房价：
 [ 28.79591589  32.46529121  20.6972206   32.27031095  19.02129416
  17.95845514  20.69062811  18.46059213  18.56479906  34.66701464
  20.58748893  27.41442739  14.47701213  19.29277326  39.1825514
  18.24197237   7.99499827  17.3565987   31.39513908  24.10611464
  17.94206754  35.88374581  29.30872993  16.11266615  36.13360925
  26.84557962  36.50572459  27.8286162   17.93750467  13.21106655
  32.18228073  14.72516264  39.61450296   6.1218561   14.69429992
  15.67196206   4.75423195  18.81753535  41.53287574  29.19937696
  25.09020445  16.64918092  39.21170216   4.03874703  20.99464407
  25.20875726  19.57341391  20.07851485  14.61924293  26.06095451
   6.83551472  27.94966016  29.86064671  15.09612234   7.58621584
  37.58702537  33.67752148  21.42633293  16.47071909  21.29911939
  23.28304646  23.8337421   19.28305196  40.44993115  24.65416451
  18.70292604  12.56611894   3.97841096  43.70120616  21.35786865
  16.15306576  21.87778958  43.4805356   20.76593801  39.11440314
  27.69318539  20.34802499  20.21777059  25.34028844  21.97115867
  32.01543659  19.56686171  22.4464336   33.0804518   27.12579609
  20.00946688  29.68493871  21.17930608  26.39383785  19.69575129
  26.03327167  22.35866282  18.904526    15.39559851  14.20725576
  17.27986265  24.66620958  15.75625322  19.83976949  27.05527824
  19.79054531  16.61507865  24.76015783  23.19264809  21.42980927
  37.45522173  14.32648618  21.014171    34.06632711  33.50908983
  20.01364528  27.32089996  22.03374396  16.37183233  20.89150303
  21.60194373  28.33491462  25.38779336  23.63872617  12.92319984
  14.45680619   0.80002118  29.81417126  19.77184892  21.75293765
  28.27667111  29.81715417]
梯度下降均方误差为:
 19.9206057374
```

### 模型评估正规方差和梯度下降
正规方程:
- 不需要选择学习率
- 不需要迭代求解
- 需要计算方程，时间复杂度高O(n3)

梯度下降  
- 需要选择学习率
- 需要迭代求解
- 特征数量较大可以使用

选择:  
- 小规模数据:
    - LinearRegression(不能解决拟合问题)
    - 岭回归
- 大规模数据: SGDRegressor

### 关于优化方法GD,SGD,SAG
- GD
    - 梯度下降(Gradient Descent), 原始的梯度下降法需要计算所有样本的值才能够得出梯度，计算量大，所以后面才会有一系列的改进
- SGD
    - 随机梯度下降(Stochatic gradient descent)是一个优化方法，他在一次迭代时只考虑一个训练样本
    - SGD的优点
        - 高效
        - 容易实现
    - SGD的缺点
        - SGD需要许多超参数，比如正则项参数，迭代数
        - SGD对于特征标准化是敏感的
- SAG
    - 随机平均梯度(Stochasitc Average Gradient), 由于收敛的速度太慢，有人提出SAG等基于梯度下降的算法

Scikit-learn:岭回归，逻辑回归等当中都会有SAG优化
                            
## 欠拟合和过拟合
- 正规方程在现实中用的很少，是因为他不能解决过拟合问题
- 可能会遇见的问题，训练数据训练的很好，误差也不大，但是在测试集上有问题

### 什么是过拟合和欠拟合
- 过拟合: 一个假设在训练数据上能够获得比其他假设更好的拟合，但是在册数数据集上却不能很好地拟合数据，此时人为这个数据出现了过拟合现象(模型过于复杂)
- 欠拟合: 一个假设在训练数据上不能获得更好的拟合，并且在测试数据集上也不能很好的拟合数据，此时认为这个假设出现了欠拟合的现象(模型过于简单)
![过拟合欠拟合](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E8%BF%87%E6%8B%9F%E5%90%88%E6%AC%A0%E6%8B%9F%E5%90%88.png)

### 原因及解决办法
- 欠拟合的原因和解决办法
    - 原因:学习到数据的特征过少
    - 解决办法:增加数据的特征数量
- 过拟合的办法和解决办法
    - 原始特征过多，存在一些嘈杂特征，模型过于复杂是因为模型尝试去兼顾各个测试数据点
    - 解决办法:
        - 使得模型简单些
        - 正则化
- 在这里针对回归，我们选择类正则化，但是对于其他机器学习算法比如分类算法来说也会出现这样的问题，除了一些算法本身作用之外(决策树，神经网络)，我们更多的也是自己去做特征选择，包括之前说的删除，合并一些特征
- 如何解决:
    - 尽量减少高次项特征的影响
    - 在学习的时候，数据提供的特征有些影响模型复杂度或者这个特征的数据点异常较多，所以算法在学习的时候尽可能减少这个特征的影响(甚至删除某个特征的影响)，这就是特征化
    - 调整时候，算法并不知道某个特征影响，而是去调整参数得出优化的结果

### 正则化类别
- L2用的比L1多些
- L2正则化:
    - 作用: 可以使得其中一些w的都很小，都接近0，削弱某个特征的影响
    - 优点：越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象
    - Ridge回归
    - 加入L2正则化后的损失函数: 
        - 损失函数+λ惩罚系数*惩罚项 
        - 目标: 让损失函数尽可能的小，同时也让惩罚系数尽可能的小，从而让模型提高了准确性还消除了高次项的影响)
    
![正则化类别](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%AD%A3%E5%88%99%E5%8C%96%E7%B1%BB%E5%88%AB.png)


- L1正则化
    - 作用:可以使得其中一些w的值直接成为0，删除这个特征的影响
    - LASSO回归 岭回归
    - 损失函数+λ惩罚项 惩罚项不是w^2而是w的绝对值，造成的结果是可以使得其中一些w的值直接成为0


## 岭回归
- 线性回归的改进
- 岭回归就是一个带L2正则化的线性回归
- 岭回归，其实也是一种线性回归，只不过在算法建立回归方程的时候，加上正则化的限制，从而达到解决过拟合的效果。所以正规方程化并不常用，岭回归是更常用的

### API
- sklearn.linear_model.Ridge(alpha=1.0,fit_intercept=True,solver='auto',normalize=False)
    - 具有L2正则化的线性回归
    - alpha: 惩罚项系数，正则化力度，也叫λ
        - λ取值:0~1 1~10
    - fit_intercept=True 选择是否添加偏置，添加使得模型更准确
    - solver:会根据数据自动化选择择优方式
        - SAG: 如果数据集，特征都比较大，选择该随机梯度下降优化
        - SGD：随机梯度下降 Stochastic gradient descent 一次迭代时只考虑一个训练样本，高效，容易实现，但是需要很多超参数，对于特征标准化是敏感的
        - Ridge方法相当于SGDRegressor(penalty='L2',loss='squared_loss'),只不过SGDRegressor实现了一个普通的随机梯度下降学习，是带L2的线性回归，推荐使用Rideg(实现类SAG)
            - 即线性回归添加了L2惩罚项，loss是最小二乘法
    - normalize:数据是否进行标准化
        - 如果是True，和在特征工程中先进行StandarScaler标准化数据是一样的，设置成True就不用再做标准化
        - 默认normalize=False: 可以在fit之前调用preprocessing.StandarScaler标准化数据
    - Ridge.coef_:回归权重
    - Ridge.intercept_:回归偏置
 
- sklearn.linear_model.RidgeCV(BaseRidegCV,RegressorMixin)
    - 具有L2正则化的线性回归，可以进行交叉验证
    - coef_:回归系数

### 观察正则化程度的变化  
- 观察正则化程度的变化，对结果的影响
    - 正则化力度越大，权重系数会越小
    - 正则化力度越小，权重系数会越大

![正则化程度的影响](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%AD%A3%E5%88%99%E5%8C%96%E7%A8%8B%E5%BA%A6%E7%9A%84%E5%8F%98%E5%8C%96.png)

横坐标:正则化力度α  纵坐标:权重系数weights 
- 正则化力度α越来越大，权重系数越来越接近于0
                        
### 波士顿房价预测案例
```
def linear_demo3():
    '''
    岭回归对波士顿房价进行预测
    :return:
    '''
    # 获取数据
    boston = load_boston()
    # 划分数据集
    x_train,x_test,y_train,y_test = train_test_split(boston.data,boston.target,random_state=22)
    # 特征工程 标准化
    transfer = StandardScaler()
    x_train = transfer.fit_transform(x_train)
    x_test = transfer.fit_transform(x_test)
    # 预估器流程, 学习率的算法constant，eta0=0.001 迭代次数10000
    estimator = Ridge(alpha=0.5,max_iter=10000)
    estimator.fit(x_train,y_train)
    # 得出模型
    print("岭回归权重系数为：\n", estimator.coef_)
    print("岭回归偏置为:\n",estimator.intercept_)
    # 模型评估
    y_predict = estimator.predict(x_test)
    print("岭回归预测房价：\n", y_predict)
    error = mean_squared_error(y_test, y_predict)
    print("岭回归均方误差为:\n", error)
    return None
    
    output:
    岭回归权重系数为：
 [-0.62710135  1.13221555 -0.07373898  0.74492864 -1.93983515  2.71141843
 -0.07982198 -3.27753496  2.44876703 -1.81107644 -1.74796456  0.88083243
 -3.91211699]
岭回归偏置为:
 22.6213720317
岭回归预测房价：
 [ 28.14980988  31.30005476  20.52972351  31.47072569  19.03738471
  18.2515444   20.58448636  18.46065814  18.47822278  32.9253545
  20.37688914  27.22040283  14.82433783  19.21909702  37.01094366
  18.31298687   7.73702695  17.57853316  30.19905513  23.61558173
  18.1326241   33.8248652   28.47676462  16.98519213  34.74283555
  26.21305152  34.80802748  26.62860932  18.63287977  13.28180355
  30.35485268  14.64609607  37.18577474   8.94068221  15.08489513
  16.10551255   7.22956076  19.15050136  39.5565419   28.26158947
  24.6308918   16.73535852  37.83410495   5.70268785  21.19002891
  24.62163908  18.88282355  19.94727347  15.19726616  26.29340687
   7.48806536  27.12758369  29.18672591  16.28031052   7.96550163
  35.44152785  32.33687405  20.89719296  16.42392256  20.87865837
  22.93165628  23.59993258  19.34765346  38.31082831  23.93493948
  18.96042932  12.60794029   6.13152953  41.45558456  21.09635345
  16.21825908  21.50546787  40.73024272  20.51662557  36.80209391
  27.04140791  19.86329311  19.62868177  24.59984417  21.18141704
  30.93357762  19.33695661  22.30651013  31.08556136  26.38132198
  20.24561514  28.80939986  20.8464224   26.0336187   19.31848012
  24.94485661  22.29733644  18.92673912  18.89328211  14.03554422
  17.42114021  24.18168589  15.83222588  20.05992382  26.52337056
  20.11082706  17.01706162  23.86414957  22.83910743  20.95125604
  36.14672616  14.70106802  20.62471484  32.45392546  33.20953724
  19.819153    26.51317386  20.93727618  16.44072777  20.76683347
  20.57365666  26.86331703  24.16771227  23.236999    13.79408517
  15.37608273   2.77999575  28.89574935  19.7879507   21.50537858
  27.54615553  28.52655513]
岭回归均方误差为:
 20.0626506883
```



# 聚类算法
## K-mean算法
