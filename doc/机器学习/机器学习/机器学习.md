# 什么是机器学习
## 定义
- 机器学习是从数据中自动分析获取模型，并利用模型对未知数据进行预测

## 解释
- 我们人从大量的日常经验中归纳规律，在面临新的问题的时候，可以利用以往总结的规律去分析现实状况，采取最佳策略

## 数据集的构成
- 我们把机器学习要学习的数据称为数据集
- 对于每一行数据我们称之为样本
- 数据集：数据具有特征，目标值为预测结果
- 有些数据集可以没有目标值

### 可用数据集
- kaggle
- UCI数据集
- scikit-learn
### sklearn数据集
- sklearn.datasets
    - load_* 获取小规模数据集
    - fetch_* 获取大规模数据集
        - sklearn.datasets.fetch_name(data_home=None,subset='train')
            - subset：train/test/all

- sklearn数据集返回值介绍
    - load和fetch返回的数据类型都是datasets.base.Bunch 继承自字典
        - data：特征值，特征数据数组，是[n_samples*n_features]的二维
            numpy.ndarray 数组
        - target：目标值，标签数组，是n_samples的一维numpy.ndarray数组
        - DESCR: 数据集描述
        - feature_names: 特征值名字，新闻数据，手写数字，回归数据集没有
        - target_names: 目标值标签名
    - dict['key'] = values 用字典键值对获得数据
      bunch.key = values

     
### 数据集的划分
- 机器学习一般的数据集会划分成两个部分：
    - 训练数据：用于训练，构建模型 70%~80%
    - 测试数据: 在模型检验的时使用，用于评估模型是否有效 20%~30%
- 数据集划分api
    - sklearn.model_selection.trian_test_split(arrays,*options)
    - x 数据集的特征值
        - 训练集特征值 x_train
        - 测试集特征值 x_test
    - y 数据集的标签值
        - 训练集目标值 y_train
        - 测试集目标值 y_test
    - test_size测试集的大小，一般为float
    - random_state随机数种子，不同的种子会造成不同的随机采样结果。相同的种子采样结果相同
    - return 训练集特征值，测试集特征值，训练集目标值，测试集目标值
### 鸢尾花数据集
- 特征值： 4个特征，花瓣的长度和宽度，花萼的长度，宽度
- 目标值： setosa vericolor virginica
- 数据集划分之后，特征值的数量不变还是四个特征，但是样本的数量变了


## sklearn转换器和估计器
### 转换器
- 转换器是特征工程的父类，所有在进行特征工程的时候都是继承自transformer这个类
- 之前做特征工程的步骤
    - 1.实例化 实例化一个转换器类 transformer
    - 2.调用fit_transform（对于文档建立分类词频矩阵，不能同时调用）
    - 我们把特征工程的家口称之为转换器，其中转换器调用有着几个种形式
        - fit_transform()
        - fit() 
        - transform()
    - 标准化公式:(x-mean)/std
        - fit_transform()
        - fit() 计算每一列的平均值和标准差
        - transform() 带入公式(x-mean)/std 进行最终的转换
        
### 预估器
- sklearn机器学习算法的实现都被封装到估计器这个父类当中
- 在sklearn中，估计器estimator是一个重要的角色，是一类实现了算法的API；所有的估计器都是estimator的子类
- 用于分类的估计器：
    - sklearn.neighbors k-近邻
    - sklearn.naive_bayes 贝叶斯
    - sklearn.linear_model.LogisticRegression 逻辑回归
    - sklearn.tree 决策树与随机森林
- 用于回归的估计器
    - sklearn.linear_model.linearRegression 线性回归
    - sklearn.linear_model.Ridge 岭回归
- 用于无监督学习的估计器
    - sklearn.cluster.KMeans 聚类

## 特征工程介绍
- 什么是特征工程
    - 是是用专业背景知识和技巧处理数据，能得特征能在机器学习算法上发挥更好的作用的过程
    - 目前用的工具 sklearn
- 为什么需要特征工程 Feature Engineering
    - 数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已
- 影响效果的原因
    - 算法
    - 特征工程
- 特征工程的位置与数据处理的比较
    - pandas：一个数据读取非常方便以及基本的处理格式的工具 用于数据清洗，数据处理（缺失值处理）
    - sklearn： 对于特征的处理特供了强大的接口 用于特征工程
- 特征工程包含内容
    - 特征抽取
    - 特征预处理
    - 特征降维

## 机器学习算法分类
- 监督学习 supervised learning(预测)
    - 定义：输入数据是由输入特征值和目标值所组成。函数的输出可以是一个连续的值（称为回归），或是输出的是有限的离散值（称为分类），
    - 分类问题 回归 输入数据有特征有标签，即有标准答案
        - 目标值：结果为类别
        - k-近邻算法，朴素贝叶斯分类，决策树，随机森林，逻辑回归
    - 回归问题
        - 目标值:连续型的数据
        - 线性回归，岭回归
- 无监督学习 unsupervised learning
    - 定义：输入数据是由输入特征值组成
    - 目标值：没有目标值 输入数据有特征无标签，即无标准答案    
        - 聚类 k-means

## 机器学习开发流程
- 获取数据 原始数据
- 数据处理
- 特征工程
- 使用机器学习算法训练 得到模型
- 模型评估

## 学习框架
- 重点  
    - 算法是核心，数据与计算是基础
- 大部分复杂模型的算法设计部都是算法工程师在做，而我们
    - 分析很多数据
    - 分析具体业务
    - 应用常见的算法
    - 特征工程，调参数，优化
        


# 特征工程

## 特征工程介绍
- 什么是特征工程
    - 是是用专业背景知识和技巧处理数据，能得特征能在机器学习算法上发挥更好的作用的过程
    - 目前用的工具 sklearn
- 为什么需要特征工程 Feature Engineering
    - 数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已
- 影响效果的原因
    - 算法
    - 特征工程
- 特征工程的位置与数据处理的比较
    - pandas：一个数据读取非常方便以及基本的处理格式的工具 用于数据清洗，数据处理（缺失值处理）
    - sklearn： 对于特征的处理特供了强大的接口 用于特征工程
- 特征工程包含内容
    - 特征抽取
    - 特征预处理
    - 特征降维


## 特征抽取
- 特征提取
- 机器学习算法 
    - 统计方法, 就是数学公式，但是不能处理字符串，就需要将文本类型的字符串转成成数值类型，即需要特征抽取
        - 文本类型转化成数值
        - 类别，类型转成数值，转换成one-hot编码，哑变量
- 为什么需要特征提取：
    - 将任意数据（如文本或图像）转换成可用于机器学习的数字特征（特征值化）
- 特征提取API
    - sklearn.feature_extraction 是一个类
        - feature 特征
        - extraction 提取
    - 不同类型的数据有不同的转换方法
        - 字典特征提取（特征离散化）
        - 文本特征提取
        - 图像特征提取（深度学习）
     

         
### 字典特征提取  

对字典数据进行特征值化

- sklearn.feature_extraction.DictVectorizer(spars=True...) 默认sparse为True
    - vector 向量 矢量 可以用一维数组来存储向量
        - 矩阵matrix 计算机中用二维数组存储 矩阵可以看成由向量构成
    - Dict Vectorizer 字典vectorizer转换成向量的形式，告诉计算机把字典转出成数值了，可以把每一个样本理解为一个向量，n个样本就是n个向量，可以看成是一个二维数组，也可以理解为矩阵
    - 调用了DictVectorizer，相当于实例化了一个转换器对象，父类是一个transfer，转换器类，其中一个方法就是把字典换成成数值  
    - 调用实例化好的对象DictVectorizer，里面有个方法叫fit_transform
    - DictVectorizer.fit_transform(x) x:字典或者包含字典的迭代器   返回值：返回sparse矩阵 稀疏
        - 稀疏矩阵将非0值按位置表现出来，节省内存，可以提高加载运行效率
    - DictVectorizer.fit_transform(x) x:array数组或者sparse矩阵    返回值：转换之前数据格式
        - DictVectorizer.get_feature_names() 返回类别名称
      
字典特征提取
- 对于特征当中存在类别信息的我们都会做one-hot编码处理，哑变量

应用数据：  
字典：  
```
data = [{'city': '北京', 'temperature': 100}, 
        {'city': '上海', 'temperature': 60}, 
        {'city': '深圳', 'temperature': 30}]
output:      
           [[0,1,0,100]
            [1,0,0,60]
            [0,0,1,30]]
```

每一个样本理解为一个向量，一共三行。三个样本，两个特征，是一个三行两列的二维数组/矩阵
原来每个样本有两个特征，再进行转化之后，字典特征抽取之后，样本量不变，特征数量变成了4个。当特征中有类别的时候，表示方法时字符串，想表示为数值并且数值没有比大小，我们就使用ont-hot变量，哑变量。

#### 字典提取应用场景
- 当我们面对数据集中有类别特征比较多
    - 将数据集的特征转换成字典类型
    - DictVectorizer转换
- 本身拿到的数据类型是字典类型
    
### 文本特征提取
- 特征：特征词
#### 第一个方法
- CountVectorizer
- sklearn.feature_extraction.text.CountVectorizer(stop_words=[])
- 返回词频矩阵 统计每个样本特征词出现的次数
- stop_words 停用词，我们觉得某些词对最终分类没有用处 is/to，以列表的形式传递
- CountVectorizer.fit_transform(x) x:文本或者包含文本字符串的可迭代对象  返回：返回sparse矩阵
    - CountVectorizer在设计的时候没有sparse这个参数，sparse矩阵转化为二维数组 可以用data.toarray()这个方法
    - 中文文档把短语当成一个特征值，处理中文要注意分词
- CountVectorizer.inverse_transform(x) x:array数组或者sparse矩阵 返回值：转换之前数据格
- CountVectorizer.get_feature_names() 返回值：单词列表
- jieba.cut
    - 进行分词处理中文
    - 实例化CountVectorizer
    - 将分词结果变成字符串当做fit_transform的输入值


#### 第二个方法    
- TfidfVectorizer   
- 关键词：在某个类别的文章中，出现的次数很多，但是在别的类别的文章中出现的很少
- TF-IDF的主要思想是：如果某个词或者短语在一篇文章中出现的概率并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类
- TF-IDF作用：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度
- 公式：
    - 词频：term frequency，tf 值的是某一个给定的词语在该文件中出现的频率
    - 逆向文档频率：inverse document frequency idf 是一个词语普遍重要性的度量。某一特定词语的idf，可以由总文件数目除以包含该该词语之文件的数目，再将得到的商取10为底的对数得到
    - tfidfij = tfij * idfi
    - 例子： 两个词 '经济''非常' 1000篇文章作为语料库 100篇文章都有非常 10篇文章有经济，现在有两篇文章AB，AB各100个词; A中10次经济，B中10次非常。计算TF-IDF的值。
        - A:tf=10/100     B:tf=10/100
        - A:idf log(1000/10)=2   B:idf log(1000/100)=1
        - A: TF-IDF 0.2 B: TF-IDF 0.1
- API：sklearn.feature_extraction.text.TfidfVectorizer(stop_words=None...)
    - 返回词的权重矩阵
        - TfidfVectorizer.fit_transform(x)
            - x:文本或者包含文本字符串的可迭代对象
            - 返回值: 返回sparse矩阵
        - TfidfVectorizer.inverse_transform(x)
            - x: array数组或者sparse矩阵
            - 返回值: 转换之前数据格式
        - TfidfVectorizer.get_feature_names()
            - 返回值：单词列表

            
## 特征预处理
### 什么是特征预处理
- 通过一些转换函数将特征数据转换成更加适合算法模型的特征数据过程
- provides several common utility functions and transformer classes 
- - 为什么我们要进行归一化/标准化
    - 特征的单位或者大小相差较大，或者某特征的方差相比其他的特征要大出几个数量级，容易影响(支配)目标结果，使得一些算法无法学习到其他的特征
- 数值型数据的无量纲化 
    - 因为量纲不统一，导致了某一特征值影响远超其他的特征值,为了让特征同等重要，就需要做数据的无量纲化。我们需要用到一些方法进行无量纲化，使不同规格的数据转换到统一规格
    - 归一化
    - 标准化
- 特征预处理API
    - sklearn.preprocessing
- 对于归一化来说，如果出现异常值，影响了最大值和最小值，那么结果会发生改变
- 对于标准化来说，如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而对方差标准差的影响较小
```
三个特征：每年获得的飞机常客里程数/里程数，每周消耗的冰淇淋公升数/公升数，玩游戏所有消耗时间的百分比/消耗时间比
评价三个类别： 不喜欢didnt 魅力一般small 极具魅力large
也许说飞机里程数对于结算结果影响较大，但是统计的人觉得这三个特征同等重要
```
### 归一化
- 通过对原始数据进行变换把数据映射到(默认为[0,1])之间
- 公式:
    - x' = (x-min)/(max-min) x''=x'*(mx-mi)+mi
    - 作用于每一列，max为一列的最大值，min为每一列的最小值，那么x''为最终结果，mx，mi分别为指定区间，默认mx为1，mi为0
- sklearn.preprocessing.MinMaxScaler(feature_range=(0,1)...)
    - MinMaxScaler.fit_transform(x)
        - x: numpy array格式的数据[n_samples,n_features]
    - 返回值: 转换后的形状相同的array
- 归一化缺陷：如果数据中异常点较多，比如最大值/最小值，归一化就是用的最大最小，所以这种方法鲁棒性较差，只适合传统精确小数据场景

### 标准化
- 通过对原始数据进行变换把数据变换到均值为0，标准差为1范围内
- x'=(x-mean)/σ 
    - σ：集中程度，离散程度 就算有一些异常数据，σ也不会有大的变化
- sklearn.preprocessing.StandardScaler()
    - 处理之后，对每列来说，所有数据都聚集在均值为0附近，标准差为1
    - StandardScaler.fit_transform(x)
        - x: numpy array格式的数据[n_samples,n_features]
    - 返回值: 转换后的形状相同的array
- 在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据的场景  

## 特征降维

### 什么是降维
- ndarray 
        - 维数: 嵌套的层数
        - 0维 具体的数，标量
        - 1维 向量
        - 2维 矩阵
        - 3维 嵌套三次 多个2维数组嵌套而成
        - n维
- 降维是指在某些限定条件下，降低随机变量(特征)个数，得到一组"不相关"主变量的过程
    - 降低的是列的个数，特征的个数
    - 降低随机变量的个数
    - 效果要求特征与特征之间不相关
- 相关特性： correlated feature
    - 相对湿度与降雨量之间的相关（相关性大）
    - 相关特征太多会导致信息冗余
- 因为在进行训练的时候，我们都是使用特征进行学习，如果特征本身存在问题或者特征之间相关性较强，对于算法学习预测会影响很大
- 降维的两种方式
    - 特征选择
    - 主成分分析

### 特征选择
- 数据中包含冗余或者相关变量(或称特征，属性，指标等)，旨在从原有特征中找出主要特征
- 方法：
    - Filter(过滤式):主要探究特征本身特点，特征与特征和目标值之间关联
        - 方差选择法：低方差特征过滤 
            - 方差少，数据比较集中
        - 相关系数
            - 可以衡量两个特征之间是否有很强的相关性
    - Embedded(嵌入式):算法自动选择特征（特征与目标值之间的关联）
        - 决策树：信息熵，信息增益
        - 正则化：L1,L2
        - 深度学习：卷积等
- API:sklearn.feature_selection

#### 过滤式
- 低方差特征过滤
    - 删除低方差的一些特征，再结合方差的大小来考虑这个方式的角度
        - 特征方差小：某个特征大多样本的值比较相近 适合删除
        - 特征方差大：某个特征很多样本的值都有差别 适合保留
- API
    - sklearn.feature_selection.VarianceThreshold(threshold=0.0)
        - 删除所有低方差特征
        - Variance.fit_transform(x)
            - x:numpy array 格式的数据[n_sample,n_features]
            - 返回值:训练集差异低于threshold的特征将会被删除，默认值是保留所有非零方差特征，即删除所有样本重具有相同值的特征
        - 初始化VarianceThreshold 指定方差，调用fit_transform
        

#### 相关系数
- 皮尔逊相关系数(Pearson Correiation Coefficient)
    - 反映变量之间相关关系密切程度的统计指标
- 特点：
- 相关系数的值介于-1与+1之间，即-1<=r<=+1
    - 当r>0时，表示两变量正相关，r<0,表示两个变量为负相关
    - 当|r|=1时，表示两变量为完全相关，当r=0，表示两变量之间无相关关系
    - 当0<|r|<1时，表示两变量存在一定程度的相关，当|r|越接近1，两变量间线性关系越密切；当|r|越接近0，表示两变量的线性关系越弱
    - 一般可以按三级划分：当|r|<0.4为低度相关；0.4<=|r|<0.7为显著性相关；0.7<=|r|<1为高度线性相关
- API
    - from scipy.stats import pearsonr
        - x:(N,) array_like
        - y:(N,) array_like Return:(Pearson's correlation coefficient,p-value)
- 特征与特征之间相关性很高：
    - 选择其中一个作为代表
    - 按一定权重加权求和
    - 主成分分析 自动处理将相关性比较强的特征

### 主成分分析PCA
- 可以理解一种特征提取的方式，降维并尽可能的保持原有信息，减少信息的损失
- 定义：高维数据转化成低维数据的过程，在此过程中可以回舍弃原有数据，创造新的数据
- 作用：是数据维度压缩，尽可能降低原数据的维数(复杂度)，损失少量信息
- 应用：回归分析或者聚类分析当中
- 二维降到一维
    - 找到一个合适的直线，通过一个矩阵运算得出主成分分析的结果
- sklearn.decomposition.PCA(n_components=None)
    - 将数据分解为较低维数空间
    - n_components:
        - 小数：表示保留百分之多少的信息
        - 整数：减少到多少特征
    - PCA.fit_transform(x) 
        - x: numpy array格式的数据
        - [n_samples, n_features]
    - 返回值：转换后指定维度的array

# 分类算法
- 分类问题 回归 输入数据有特征有标签，即有标准答案
    - 目标值：结果为类别
    - k-近邻算法，朴素贝叶斯分类，决策树，随机森林，逻辑回归
- 回归问题
    - 目标值:连续型的数据

## sklearn转换器和估计器
### 转换器
- 转换器是特征工程的父类，所有在进行特征工程的时候都是继承自transformer这个类
- 之前做特征工程的步骤
    - 1.实例化 实例化一个转换器类 transformer
    - 2.调用fit_transform（对于文档建立分类词频矩阵，不能同时调用）
    - 我们把特征工程的家口称之为转换器，其中转换器调用有着几个种形式
        - fit_transform()
        - fit() 
        - transform()
    - 标准化公式:(x-mean)/std
        - fit_transform()
        - fit() 计算每一列的平均值和标准差
        - transform() 带入公式(x-mean)/std 进行最终的转换

        
### 预估器
- sklearn机器学习算法的实现都被封装到估计器这个父类当中
- 在sklearn中，估计器estimator是一个重要的角色，是一类实现了算法的API；所有的估计器都是estimator的子类
- 用于分类的估计器：
    - sklearn.neighbors k-近邻
    - sklearn.naive_bayes 贝叶斯
    - sklearn.linear_model.LogisticRegression 逻辑回归
    - sklearn.tree 决策树与随机森林
- 用于回归的估计器
    - sklearn.linear_model.linearRegression 线性回归
    - sklearn.linear_model.Ridge 岭回归
- 用于无监督学习的估计器
    - sklearn.cluster.KMeans 聚类
- 估计器工作流程estimator
    - 1.实例化，实例化一个预估器 estimator
    - 2.调用estimator.fit(x_train,y_trian) 
        - 调用完毕，模型生成
    - 3.模型评估
        - 直接比对真实值和预测值
             y_predict = estimator.predict(x_test)
             y_test == y_predict 生成布尔值，比对结果是否一致
        - 计算准确率
             accuracy = estimator.score(x_test,y_test)

## k-近邻算法 KNN算法
### 什么是KNN算法
- K Nearest Neighbor
- 如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别 但是k=1时，容易受到异常点的影响，选择一个合适的k值可以避免受到异常值的影响
- 如何确定谁是邻居
    - 欧式距离 两个样本的距离
    - 曼哈顿距离 绝对值距离 |a1-b1|+|a2-b2|+|a3-b3|
    - 明可夫斯基距离
- 电影类型分析
    - 当k=1的时候，分类为爱情片
    - 当k=2的时候，取两个最近的值
    ...
    - 当k=6的时候，一共就6个样本，一半爱情片，一半动作片 分类无法确定
    - 当k=7的时候，加入一共电影为动作片，这样三个是爱情片，四个是动作片，这个电影归类到动作片，但是实际上这个片子偏向爱情片
- 当k值取的过小的时候，容易受到异常值的影响 
- 当k值取的过大的时候，受到样本不均衡的影响，类型很容易分错
- 无量纲化的处理
    - 标准化
API

- sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, algorithm='auto')
    - n_neighbors: int,可选(默认=5)，k值，k_neighbors 查询默认使用的邻居数
    - algorithm: {'auto','ball_tree','kd_tree','brute'},可选用于计算最近邻居的算法
        - ball_tree 使用BallTree
        - kd_tree 使用KDTree
        - auto 将尝试根据传递给fit方法的值来决定最合适的算法 不同现实方式影响效率
        
### 案例1 鸢尾花种类预测
- 1.数据集介绍
```
Iris数据集是常用的分类实验数据集，用称鸢尾花数据集，是一类多重变量分析的数据集。数据集的具体介绍：
实例数量： 150 三个类各有50个
属性数量： 4（数值型，数值型，帮助预测的属性和类）
属性信息：
    sepal length 萼片长度 cm
    sepal width 萼片宽度 cm
    petal length 花瓣长度 cm
    petal width 花瓣宽度 cm
类：
    Iris-SetosaN 山鸢尾
    Iris-Versicolour 变色鸢尾
    Iris-Virginica 维吉妮亚鸢尾
```
- 2.数据集划分
- 3.特征工程
    - 标准化
- 4.机器学习训练
    - KNN预估器流程
- 5.模型评估

```python
# 导入数据集
from sklearn.datasets import load_iris
# 划分数据集
from sklearn.model_selection import train_test_split
# 数据标准化
from sklearn.preprocessing import StandardScaler
# 调用knn算法
from sklearn.neighbors import KNeighborsClassifier

def knn_iris():
    '''
    用knn算法对鸢尾花进行分类
    :return: 
    '''
    # 1获取数据
    iris = load_iris()
    # 2划分数据集 random_state取6
    x_train, x_test, y_train, y_test = train_test_split(iris.data,iris.target,random_state=6)
    # 3特征工程：标准化
    transfer = StandardScaler()
    # 训练集标准化
    x_train = transfer.fit_transform(x_train)
    # 测试集也需要标准化，用训练集的平均值和标准差 模型用同样分布状况的数据
    x_test = transfer.transform(x_test)
    # 4KNN算法预估器,k取3
    # 放入训练集的特征值和目标值放进来
    estimator = KNeighborsClassifier(n_neighbors=3)
    # 相当于有了模型
    estimator.fit(x_train,y_train)
    # 5模型评估
    # 方法1，直接比对真实值和预测值
    # 根据x_train和y_trian模型，预测x_test会得到y_test,再和实际的y_test相比较
    # 得到预测的结果
    y_predict = estimator.predict(x_test)
    print("y_predict:\n", y_predict)
    print("对比真实值和预测值：", y_predict == y_test )
    # 方法2，计算准确率
    score = estimator.score(x_test,y_test)
    print("准确率：\n",score)
    return None

if __name__ == "__main__":
    knn_iris()
    
output:  
y_predict:
 [0 2 0 0 2 1 1 0 2 1 2 1 2 2 1 1 2 1 1 0 0 2 0 0 1 1 1 2 0 1 0 1 0 0 1 2 1
 2]
对比真实值和预测值： [ True  True  True  True  True  True False  True  True  True  True  True
  True  True  True False  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True False  True
  True  True]
准确率：
 0.921052631579
```
### KNN算法优缺点
- 优点：简单，易于理解，易于实现，无需训练
- 缺点：
    - 懒惰算法，对测试样本分类时的计算量大，内存开销大
    - 必须指定k值，k值选择不当则分类精度不能保证
- 使用场景：小数据场景，几千~几万样本，具体场景具体业务去测试，大数据不适合用KNN算法

## 模型选择和调优
### 什么是交叉验证(cross validation)  
交叉验证：将拿到的训练数据，分为训练和验证集。将数据分成四份，其中一份作为验证集。然后经过4次(组)的测试，每次都更换不同的验证集，即得到4组模型的结果，取平均数作为最终结果，又称为4折交叉验证。
#### 分析  
我们之前知道数据分成训练集和测试集，但是为类让训练得到模型的结果更为准确，做以下处理  
- 训练集：训练集+验证集
- 测试集：测试集
- 得到不同的准确率，求平均值之后，平均值作为模型最终准确率
```
一组：验证集 训练集 训练集 训练集  80%
二组：训练集 验证集 训练集 训练集  78%
三组：训练集 训练集 验证集 训练集  75%
四组：训练集 训练集 训练集 验证集  82%
```
#### 为什么需要交叉验证
- 交叉验证目的：为了让被评估的模型更加准确可信
- 如何选择或者调优参数

### 超参数搜索-网格搜索(Grid Search)
超参数 通常情况下，狠毒偶参数是需要手动指定的(如k-近邻算法中的k值)，这种叫超参数  

但是手动过程繁杂，所以需要对模型预设几种超参数组合。每组超参数都采用交叉验证来进行评估。最后选出最优参数组合建立模型。
```
k值   k=3   k=5   k=7
模型 模型1 模型2 模型3
```
### 模型选择与调优API
sklearn.model_selection.GridSearchCV(estimator,param_grid=None,cv=None)
    - 对估计器的指定参数进行详尽搜索
    - estimator:估计器对象
    - param_grid:估计器参数(dict)("n_neighbors":[1,3,5]) 准备好的k的取值，以字典的 形式传进来
    - cv:指定几折交叉验证 经过几次交叉验证 一般是10折
    - fit():输入训练数据
    - score():准确率
    - 结果分析:
        - 最佳参数: best_params_
        - 最佳结果: best_score_
        - 最佳估计器: best_estimator_
        - 交叉验证结果: cv_results_
        - p=1是曼哈顿距离，p=2是欧式距离
### 鸢尾花案例增加K值调优
```python
# 导入数据集
from sklearn.datasets import load_iris
# 划分数据集
from sklearn.model_selection import train_test_split
# 数据标准化
from sklearn.preprocessing import StandardScaler
# 调用knn算法
from sklearn.neighbors import KNeighborsClassifier
# 调用gscv调优
from sklearn.model_selection import GridSearchCV

def knn_iris_gscv():
    '''
    用knn算法对鸢尾花进行分类, 添加网格搜索和交叉验证
    :return:
    '''
    # 获取数据
    iris = load_iris()
    # 划分数据
    x_train, x_test, y_train, y_test = train_test_split(iris.data,iris.target,random_state=6)
    # 特征工程：标准化
    transfer = StandardScaler()
    x_train = transfer.fit_transform(x_train)
    x_test = transfer.transform(x_test)
    # KNN算法预估器，因为要试，所以这里不需要加上k值
    estimator = KNeighborsClassifier()
    # 加入网格搜索和交叉验证
    ## 参数准备 k值 字典的格式
    param_dict = {"n_neighbors":[1,3,5,7,9,11]}
    estimator = GridSearchCV(estimator,param_grid=param_dict,cv=10)
    estimator.fit(x_train,y_train)
    # 模型评估
    y_predict = estimator.predict(x_test)
    print("y_predict:\n", y_predict)
    print("对比真实值和预测值：", y_predict == y_test )
    score = estimator.score(x_test,y_test)
    print("准确率：\n",score)
    # 最佳参数: best_params_
    print("最佳参数：",estimator.best_params_)
    # 最佳结果: best_score_
    # 此最佳结果是训练集中验证集当中的结果
    print("最佳结果",estimator.best_score_)
    # 最佳估计器: best_estimator_
    print("最佳估计器",estimator.best_estimator_)
    # 交叉验证结果: cv_results_
    print("交叉验证结果",estimator.cv_results_)
    return None

if __name__ == "__main__":
    knn_iris_gscv()

output:
y_predict:
 [0 2 0 0 2 1 2 0 2 1 2 1 2 2 1 1 2 1 1 0 0 2 0 0 1 1 1 2 0 1 0 1 0 0 1 2 1
 2]
对比真实值和预测值： [ True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True False  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True False  True
  True  True]
准确率：
 0.947368421053
最佳参数： {'n_neighbors': 5}
最佳结果 0.973214285714
```
### 预测facebook签到位置  

数据集介绍  
本次大赛的目的是预测一个人想签入到哪个地方。FaceBook创建了一个人造的世界，10公里*10公里的一个区域。对于一个给定的坐标，你的任务是返回最有可能的地方的排名列表  

数据集数据  
```
train_csv, test_csv
row_id: id of the check_in event 登记事情的id
x y: coordiantes 坐标系
accuracy: location accuracy 定位准确度
time: timestamp 时间戳
place_id: id of the business. this is the target you are predicting   预测签到的位置 
```

## 朴素贝叶斯算法

## 决策树

## 随机森林


# 回归算法

# 聚类算法
