# 岭回归
- 线性回归的改进
- 岭回归就是一个带L2正则化的线性回归
- 岭回归，其实也是一种线性回归，只不过在算法建立回归方程的时候，加上正则化的限制，从而达到解决过拟合的效果。所以正规方程化并不常用，岭回归是更常用的

# 欠拟合和过拟合
- 正规方程在现实中用的很少，是因为他不能解决过拟合问题
- 可能会遇见的问题，训练数据训练的很好，误差也不大，但是在测试集上有问题

## 什么是过拟合和欠拟合
- 过拟合: 一个假设在训练数据上能够获得比其他假设更好的拟合，但是在册数数据集上却不能很好地拟合数据，此时人为这个数据出现了过拟合现象(模型过于复杂)
- 欠拟合: 一个假设在训练数据上不能获得更好的拟合，并且在测试数据集上也不能很好的拟合数据，此时认为这个假设出现了欠拟合的现象(模型过于简单)
![过拟合欠拟合](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E8%BF%87%E6%8B%9F%E5%90%88%E6%AC%A0%E6%8B%9F%E5%90%88.png)

## 原因及解决办法
- 欠拟合的原因和解决办法
    - 原因:学习到数据的特征过少
    - 解决办法:增加数据的特征数量
- 过拟合的办法和解决办法
    - 原始特征过多，存在一些嘈杂特征，模型过于复杂是因为模型尝试去兼顾各个测试数据点
    - 解决办法:
        - 使得模型简单些
        - 正则化
- 在这里针对回归，我们选择类正则化，但是对于其他机器学习算法比如分类算法来说也会出现这样的问题，除了一些算法本身作用之外(决策树，神经网络)，我们更多的也是自己去做特征选择，包括之前说的删除，合并一些特征
- 如何解决:
    - 尽量减少高次项特征的影响
    - 在学习的时候，数据提供的特征有些影响模型复杂度或者这个特征的数据点异常较多，所以算法在学习的时候尽可能减少这个特征的影响(甚至删除某个特征的影响)，这就是特征化
    - 调整时候，算法并不知道某个特征影响，而是去调整参数得出优化的结果

## 正则化类别
- L2用的比L1多些
- L2正则化:
    - 作用: 可以使得其中一些w的都很小，都接近0，削弱某个特征的影响
    - 优点：越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象
    - Ridge回归
    - 加入L2正则化后的损失函数: 
        - 损失函数+λ惩罚系数*惩罚项 
        - 目标: 让损失函数尽可能的小，同时也让惩罚系数尽可能的小，从而让模型提高了准确性还消除了高次项的影响)
    
![正则化类别](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%AD%A3%E5%88%99%E5%8C%96%E7%B1%BB%E5%88%AB.png)


- L1正则化
    - 作用:可以使得其中一些w的值直接成为0，删除这个特征的影响
    - LASSO回归 岭回归
    - 损失函数+λ惩罚项 惩罚项不是w^2而是w的绝对值，造成的结果是可以使得其中一些w的值直接成为0

## API
- sklearn.linear_model.Ridge(alpha=1.0,fit_intercept=True,solver='auto',normalize=False)
    - 具有L2正则化的线性回归
    - alpha: 惩罚项系数，正则化力度，也叫λ
        - λ取值:0~1 1~10
    - fit_intercept=True 选择是否添加偏置，添加使得模型更准确
    - solver:会根据数据自动化选择择优方式
        - SAG: 如果数据集，特征都比较大，选择该随机梯度下降优化
        - SGD：随机梯度下降 Stochastic gradient descent 一次迭代时只考虑一个训练样本，高效，容易实现，但是需要很多超参数，对于特征标准化是敏感的
        - Ridge方法相当于SGDRegressor(penalty='L2',loss='squared_loss'),只不过SGDRegressor实现了一个普通的随机梯度下降学习，是带L2的线性回归，推荐使用Rideg(实现类SAG)
            - 即线性回归添加了L2惩罚项，loss是最小二乘法
    - normalize:数据是否进行标准化
        - 如果是True，和在特征工程中先进行StandarScaler标准化数据是一样的，设置成True就不用再做标准化
        - 默认normalize=False: 可以在fit之前调用preprocessing.StandarScaler标准化数据
    - Ridge.coef_:回归权重
    - Ridge.intercept_:回归偏置
 
- sklearn.linear_model.RidgeCV(BaseRidegCV,RegressorMixin)
    - 具有L2正则化的线性回归，可以进行交叉验证
    - coef_:回归系数

## 观察正则化程度的变化  
- 观察正则化程度的变化，对结果的影响
    - 正则化力度越大，权重系数会越小
    - 正则化力度越小，权重系数会越大

![正则化程度的影响](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%AD%A3%E5%88%99%E5%8C%96%E7%A8%8B%E5%BA%A6%E7%9A%84%E5%8F%98%E5%8C%96.png)

横坐标:正则化力度α  纵坐标:权重系数weights 
- 正则化力度α越来越大，权重系数越来越接近于0
                        
### 波士顿房价预测案例
```
def linear_demo3():
    '''
    岭回归对波士顿房价进行预测
    :return:
    '''
    # 获取数据
    boston = load_boston()
    # 划分数据集
    x_train,x_test,y_train,y_test = train_test_split(boston.data,boston.target,random_state=22)
    # 特征工程 标准化
    transfer = StandardScaler()
    x_train = transfer.fit_transform(x_train)
    x_test = transfer.fit_transform(x_test)
    # 预估器流程, 学习率的算法constant，eta0=0.001 迭代次数10000
    estimator = Ridge(alpha=0.5,max_iter=10000)
    estimator.fit(x_train,y_train)
    # 得出模型
    print("岭回归权重系数为：\n", estimator.coef_)
    print("岭回归偏置为:\n",estimator.intercept_)
    # 模型评估
    y_predict = estimator.predict(x_test)
    print("岭回归预测房价：\n", y_predict)
    error = mean_squared_error(y_test, y_predict)
    print("岭回归均方误差为:\n", error)
    return None
    
    output:
    岭回归权重系数为：
 [-0.62710135  1.13221555 -0.07373898  0.74492864 -1.93983515  2.71141843
 -0.07982198 -3.27753496  2.44876703 -1.81107644 -1.74796456  0.88083243
 -3.91211699]
岭回归偏置为:
 22.6213720317
岭回归预测房价：
 [ 28.14980988  31.30005476  20.52972351  31.47072569  19.03738471
  18.2515444   20.58448636  18.46065814  18.47822278  32.9253545
  20.37688914  27.22040283  14.82433783  19.21909702  37.01094366
  18.31298687   7.73702695  17.57853316  30.19905513  23.61558173
  18.1326241   33.8248652   28.47676462  16.98519213  34.74283555
  26.21305152  34.80802748  26.62860932  18.63287977  13.28180355
  30.35485268  14.64609607  37.18577474   8.94068221  15.08489513
  16.10551255   7.22956076  19.15050136  39.5565419   28.26158947
  24.6308918   16.73535852  37.83410495   5.70268785  21.19002891
  24.62163908  18.88282355  19.94727347  15.19726616  26.29340687
   7.48806536  27.12758369  29.18672591  16.28031052   7.96550163
  35.44152785  32.33687405  20.89719296  16.42392256  20.87865837
  22.93165628  23.59993258  19.34765346  38.31082831  23.93493948
  18.96042932  12.60794029   6.13152953  41.45558456  21.09635345
  16.21825908  21.50546787  40.73024272  20.51662557  36.80209391
  27.04140791  19.86329311  19.62868177  24.59984417  21.18141704
  30.93357762  19.33695661  22.30651013  31.08556136  26.38132198
  20.24561514  28.80939986  20.8464224   26.0336187   19.31848012
  24.94485661  22.29733644  18.92673912  18.89328211  14.03554422
  17.42114021  24.18168589  15.83222588  20.05992382  26.52337056
  20.11082706  17.01706162  23.86414957  22.83910743  20.95125604
  36.14672616  14.70106802  20.62471484  32.45392546  33.20953724
  19.819153    26.51317386  20.93727618  16.44072777  20.76683347
  20.57365666  26.86331703  24.16771227  23.236999    13.79408517
  15.37608273   2.77999575  28.89574935  19.7879507   21.50537858
  27.54615553  28.52655513]
岭回归均方误差为:
 20.0626506883
```