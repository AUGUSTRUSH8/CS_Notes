# 特征抽取
- 也叫特征提取
- 机器学习算法 
    - 统计方法, 就是数学公式，但是**不能处理字符串**，就需要将文本类型的字符串转成成数值类型，即需要特征抽取
    - 文本类型转化成数值
    - 类别，**类型转成数值**，转换成one-hot编码，哑变量
- 为什么需要特征提取：
    - 将任意数据（如文本或图像）转换成可用于机器学习的数字特征（特征值化）

## 1特征提取API
- **sklearn.feature_extraction** 是一个类
    - feature 特征
    - extraction 提取
- **不同类型的数据有不同的转换方法**
    - 字典特征提取（特征离散化）
    - 文本特征提取
    - 图像特征提取（深度学习）
- **ont-hot编码**
    -  每个类别生成一个布尔列，这些列中只有一列可以为每个样本取值1，热编码

## 2字典特征提取API 
- 对字典数据进行特征值化

### 2.1字典特征提取API 
- **sklearn.feature_extraction.DictVectorizer(spars=True...)** 默认sparse为True
    - vector 向量 矢量 可以用一维数组来存储向量
        - 矩阵matrix 计算机中用二维数组存储 矩阵可以看成由向量构成
    - Dict Vectorizer 字典vectorizer转换成**向量**的形式，告诉计算机把**字典**转出成**数**值了，可以把每一个样本理解为一个向量，n个样本就是n个向量，可以看成是一个二维数组，也可以理解为矩阵
    - 系数矩阵: 默认sparse=True 节省空间，节省内存，提高加载运行效率
        - 如果需要构造二维矩阵 data.toarray()
    
- 字典特征提取
    - 数据离散化效果 
    - 对于特征当中存在类别信息的我们都会做**one-hot编码处理**，哑变量

### 2.2调用后如果使用
- 调用了**DictVectorizer**，相当于实例化了一个转换器对象，父类是一个**transfer**，转换器类，其中一个方法就是把字典换成成数值  
- 调用**实例化**对象DictVectorizer，里面有个方法叫fit_transform
- **DictVectorizer.fit_transform(x)** x:字典或者包含字典的迭代器   返回值：返回sparse矩阵 稀疏
- 稀疏矩阵将非0值按位置表现出来，节省内存，可以提高加载运行效率
- **DictVectorizer.fit_transform(x)** x:array数组或者sparse矩阵    返回值：转换之前数据格式
    - **DictVectorizer.get_feature_names()** 返回类别名称
      
应用数据：  
字典：  
```
data = [{'city': '北京', 'temperature': 100}, 
        {'city': '上海', 'temperature': 60}, 
        {'city': '深圳', 'temperature': 30}]
output:      
           [[0,1,0,100]
            [1,0,0,60]
            [0,0,1,30]]
```

- 每一个样本理解为一个向量，一共三行。三个样本，两个特征，是一个三行两列的二维数组/矩阵
原来每个样本有两个特征，再进行转化之后，字典特征抽取之后，样本量不变，特征数量变成了4个。当特征中有类别的时候，表示方法时字符串，想表示为数值并且数值没有比大小，我们就使用ont-hot变量，哑变量。
```python
from sklearn.feature_extraction import DictVectorizer
data = [{'city': '北京', 'temperature': 100}, {'city': '上海', 'temperature': 60}, {'city': '深圳', 'temperature': 30}]
#1 实例化一个转换器类
# sparse 稀疏矩阵
transform = DictVectorizer(sparse=False)
# 2 调用fit_transform()方法，里面传字典或者包含字典的迭代器
data_new = transform.fit_transform(data)
print("data_new:\n", data_new)
print("特征名字：",transform.get_feature_names())  
```
```
output:
data_new:
 [[   0.    1.    0.  100.]
 [   1.    0.    0.   60.]
 [   0.    0.    1.   30.]]
特征名字： ['city=上海', 'city=北京', 'city=深圳', 'temperature']

```
```python
from sklearn.feature_extraction import DictVectorizer
data = [{'city': '北京', 'temperature': 100}, {'city': '上海', 'temperature': 60}, {'city': '深圳', 'temperature': 30}]
#1 实例化一个转换器类
# sparse 稀疏矩阵
transform = DictVectorizer(sparse=True)
# 2 调用fit_transform()方法，里面传字典或者包含字典的迭代器
data_new = transform.fit_transform(data)
print("data_new:\n", data_new)
print("特征名字：",transform.get_feature_names())  
```
```
output:
data_new:
   (0, 1)	1.0
  (0, 3)	100.0
  (1, 0)	1.0
  (1, 3)	60.0
  (2, 2)	1.0
  (2, 3)	30.0
特征名字： ['city=上海', 'city=北京', 'city=深圳', 'temperature']
```
### 2.1字典提取应用场景
- 当我们面对数据集中有类别特征比较多
    - 将数据集的特征转换成字典类型
    - DictVectorizer转换
- 本身拿到的数据类型是字典类型

    
## 3文本特征提取
- 特征：特征词

### 3.1CountVectorizer API
- **sklearn.feature_extraction.text.CountVectorizer(stop_words=[])**
- 返回**词频矩阵** 统计每个样本**特征词**出现的次数
- stop_words 停用词，我们觉得某些词对最终分类没有用处 is/to，以列表的形式传递

#### 3.2调用方法
- **CountVectorizer.fit_transform(x)** x:文本或者包含文本字符串的可迭代对象  返回：返回sparse矩阵
    - CountVectorizer在设计的时候没有sparse这个参数，sparse矩阵转化为二维数组 可以用data.toarray()这个方法
    - 中文文档把**短语**当成一个特征值，处理中文要注意分词
- **CountVectorizer.inverse_transform(x)** x:array数组或者sparse矩阵 返回值：转换之前数据格
- **CountVectorizer.get_feature_names()** 返回值：单词列表
- **jieba.cut**
    - **import jieba**
    - 进行分词处理中文
    - 实例化CountVectorizer
    - 将分词结果变成字符串当做fit_transform的输入值

```python
from sklearn.feature_extraction.text import CountVectorizer
data = ["life is short,i like like python", "life is too long,i dislike python"]
#实例化一个转换器
transfer = CountVectorizer(stop_words = ['is','to'])
#调用fit_transform
new_data = transfer.fit_transform(data)
print(new_data,new_data.toarray())  
print("特征名字：",transfer.get_feature_names())
```
- print(new_data,new_data.toarray())    new_data.toarray() 可以转化成二维矩阵
```
output:
(0, 4)	1
  (0, 2)	2
  (0, 5)	1
  (0, 1)	1
  (1, 0)	1
  (1, 3)	1
  (1, 6)	1
  (1, 4)	1
  (1, 1)	1 [[0 1 2 0 1 1 0]
 [1 1 0 1 1 0 1]]
特征名字： ['dislike', 'life', 'like', 'long', 'python', 'short', 'too']
```
- 因为中文的特性，需要句子中的词语要分开
```python
from sklearn.feature_extraction.text import CountVectorizer
#中文文档的特征抽取
data = ["一闪 一闪 亮晶晶，漫天 都 是 小星星"]
transfer = CountVectorizer()
data_new = transfer.fit_transform(data)
print(data_new, data_new.toarray())
# 这个把短语当成一个特征值，处理中文要注意分词
print(transfer.get_feature_names())  
```
```
output:
  (0, 2)	1
  (0, 3)	1
  (0, 1)	1
  (0, 0)	2 [[2 1 1 1]]
['一闪', '亮晶晶', '小星星', '漫天']
```


```python
from sklearn.feature_extraction.text import TfidfVectorizer
import jieba

def count_Chinese_demo():
    '''
    中文文本特征提取，自动分词
    :return:
    '''
    data = ["一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。",
            "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。",
            "如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"]
    # 首先要分词处理
    data_new=[]
    for i in data:
        ### 注意是i 不是data
        data_new.append(cut_word_demo(i))
    # data_new是分好词的string
    print(data_new)
    tranfer = CountVectorizer(stop_words=["一种",'因为'])
    data_final = tranfer.fit_transform(data_new)
    print("data_final", data_final.toarray())
    print(data_final)
    print("特征名字：", tranfer.get_feature_names())

    return None


def cut_word_demo(text):
    '''
    用于中文分词
    :return:
    '''
    #jieba.cut(text) 返回一个生成器 generator object Tokenizer.cut
    # 需要强转为list的格式 list(jieba.cut(text))，但是最终效果我们希望是一个字符串，再转换成字符串' '.join(list(jieba.cut(text)))
    text1 = ' '.join(list(jieba.cut(text)))
    #print(text1)
    #print("...")
    # 返回的是一个字符串
    return ' '.join(list(jieba.cut(text)))

if __name__ == "__main__":
    count_Chinese_demo()
```
```
output:
['一种 还是 一种 今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。', '我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。', '如果 只用 一种 方式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。']
data_final [[0 1 0 0 0 2 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 2 0 1 0 2 1 0 0 0 1 1 0 0 1 0]
 [0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 3 0 0 0 0 1 0 0 0 0 2 0 0 0 0 0 1 0 1]
 [1 0 0 4 3 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 2 1 0 0 1 0 0 0]]
  (0, 19)	1
  (0, 1)	1
  (0, 26)	1
  (0, 18)	1
  (0, 23)	1
  (0, 13)	1
  (0, 30)	1
  (0, 31)	1
  (0, 11)	1
  (0, 21)	2
  (0, 25)	2
  (0, 5)	2
  (0, 34)	1
  (1, 33)	1
  (1, 16)	1
  (1, 35)	1
  (1, 8)	1
  (1, 2)	1
  (1, 7)	1
  (1, 6)	1
  (1, 22)	1
  (1, 27)	2
  (1, 17)	3
  (2, 32)	1
  (2, 14)	1
  (2, 9)	1
  (2, 29)	1
  (2, 12)	1
  (2, 28)	2
  (2, 0)	1
  (2, 4)	3
  (2, 24)	1
  (2, 3)	4
  (2, 20)	1
  (2, 10)	1
  (2, 15)	1
  (2, 17)	1
特征名字： ['不会', '不要', '之前', '了解', '事物', '今天', '光是在', '几百万年', '发出', '取决于', '只用', '后天', '含义', '大部分', '如何', '如果', '宇宙', '我们', '所以', '放弃', '方式', '明天', '星系', '晚上', '某样', '残酷', '每个', '看到', '真正', '秘密', '绝对', '美好', '联系', '过去', '还是', '这样']
```

### 3.3TfidfVectorizer 
- **关键词**：在某个类别的文章中，出现的次数很多，但是在别的类别的文章中出现的很少
- TF-IDF的主要思想是：如果某个词或者短语在一篇文章中出现的概率并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类
- **TF-IDF作用：用以评估字词对于一个文件集或一个语料库中的其中一份文件的重要程度**
- 公式：
    - **词频**：term frequency，tf 值的是某一个给定的词语在该文件中出现的频率
    - **逆向文档频率**：inverse document frequency idf 是一个词语普遍重要性的度量。某一特定词语的idf，可以由总文件数目除以包含该该词语之文件的数目，再将得到的商取10为底的对数得到
    - **tfidfij = tfij * idfi**
    - 例子： 两个词 '经济''非常' 1000篇文章作为语料库 100篇文章都有'非常' 10篇文章有'经济'，现在有两篇文章AB，AB各有100个词; A中10次经济，B中10次非常。计算TF-IDF的值。
        - A:tf=10/100=0.1     B:tf=10/100=0.1
        - A:idf log(1000/10)=2   B:idf log(1000/100)=1
        - A: TF-IDF 0.2 B: TF-IDF 0.1

### 3.4TfidfVectorizer API
- **sklearn.feature_extraction.text.TfidfVectorizer(stop_words=None...)**
    - 返回词的权重矩阵
        - TfidfVectorizer.fit_transform(x)
            - x:文本或者包含文本字符串的可迭代对象
            - 返回值: 返回sparse矩阵
        - TfidfVectorizer.inverse_transform(x)
            - x: array数组或者sparse矩阵
            - 返回值: 转换之前数据格式
        - TfidfVectorizer.get_feature_names()
            - 返回值：单词列表


```python
from sklearn.feature_extraction.text import TfidfVectorizer
import jieba
def tfidf_demo():
    '''
    用TF-IDF的方法进行文本特征抽取
    :return:
    '''
    data = ["一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。",
            "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。",
            "如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"]
    ## 首先要分词处理
    data_new = []
    for i in data:
        ### 注意是i 不是data
        data_new.append(cut_word_demo(i))
    #print(data_new)
    tranfer = TfidfVectorizer(stop_words=["一种", '因为'])
    data_final = tranfer.fit_transform(data_new)
    print(data_final)
    print("data_final", data_final.toarray())
    print("特征名字：", tranfer.get_feature_names())
    return None
    
def cut_word_demo(text):
    '''
    用于中文分词
    :return:
    '''
    # 先强转为list的格式，在转换成字符串
    text1 = ' '.join(list(jieba.cut(text)))
    #print(text1)
    #print("...")
    # 返回的是一个字符串
    return ' '.join(list(jieba.cut(text)))

if __name__ == "__main__":
    tfidf_demo()
```

```
output:
  (0, 34)	0.213200716356
  (0, 5)	0.426401432711
  (0, 25)	0.426401432711
  (0, 21)	0.426401432711
  (0, 11)	0.213200716356
  (0, 31)	0.213200716356
  (0, 30)	0.213200716356
  (0, 13)	0.213200716356
  (0, 23)	0.213200716356
  (0, 18)	0.213200716356
  (0, 26)	0.213200716356
  (0, 1)	0.213200716356
  (0, 19)	0.213200716356
  (1, 17)	0.550047687471
  (1, 27)	0.482164405401
  (1, 22)	0.241082202701
  (1, 6)	0.241082202701
  (1, 7)	0.241082202701
  (1, 2)	0.241082202701
  (1, 8)	0.241082202701
  (1, 35)	0.241082202701
  (1, 16)	0.241082202701
  (1, 33)	0.241082202701
  (2, 17)	0.120888454083
  (2, 15)	0.158953789581
  (2, 10)	0.158953789581
  (2, 20)	0.158953789581
  (2, 3)	0.635815158326
  (2, 24)	0.158953789581
  (2, 4)	0.476861368744
  (2, 0)	0.158953789581
  (2, 28)	0.317907579163
  (2, 12)	0.158953789581
  (2, 29)	0.158953789581
  (2, 9)	0.158953789581
  (2, 14)	0.158953789581
  (2, 32)	0.158953789581
data_final [[ 0.          0.21320072  0.          0.          0.          0.42640143
   0.          0.          0.          0.          0.          0.21320072
   0.          0.21320072  0.          0.          0.          0.
   0.21320072  0.21320072  0.          0.42640143  0.          0.21320072
   0.          0.42640143  0.21320072  0.          0.          0.
   0.21320072  0.21320072  0.          0.          0.21320072  0.        ]
 [ 0.          0.          0.2410822   0.          0.          0.
   0.2410822   0.2410822   0.2410822   0.          0.          0.          0.
   0.          0.          0.          0.2410822   0.55004769  0.          0.
   0.          0.          0.2410822   0.          0.          0.          0.
   0.48216441  0.          0.          0.          0.          0.
   0.2410822   0.          0.2410822 ]
 [ 0.15895379  0.          0.          0.63581516  0.47686137  0.          0.
   0.          0.          0.15895379  0.15895379  0.          0.15895379
   0.          0.15895379  0.15895379  0.          0.12088845  0.          0.
   0.15895379  0.          0.          0.          0.15895379  0.          0.
   0.          0.31790758  0.15895379  0.          0.          0.15895379
   0.          0.          0.        ]]
特征名字： ['不会', '不要', '之前', '了解', '事物', '今天', '光是在', '几百万年', '发出', '取决于', '只用', '后天', '含义', '大部分', '如何', '如果', '宇宙', '我们', '所以', '放弃', '方式', '明天', '星系', '晚上', '某样', '残酷', '每个', '看到', '真正', '秘密', '绝对', '美好', '联系', '过去', '还是', '这样']

```