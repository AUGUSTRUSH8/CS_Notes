## 特征工程介绍
- 什么是特征工程
    - 是是用专业背景知识和技巧处理数据，能得特征能在机器学习算法上发挥更好的作用的过程
    - 目前用的工具 sklearn
- 为什么需要特征工程 Feature Engineering
    - 数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已
- 影响效果的原因
    - 算法
    - 特征工程
- 特征工程的位置与数据处理的比较
    - pandas：一个数据读取非常方便以及基本的处理格式的工具 用于数据清洗，数据处理（缺失值处理）
    - sklearn： 对于特征的处理特供了强大的接口 用于特征工程
- 特征工程包含内容
    - 特征抽取
    - 特征预处理
    - 特征降维


## 特征抽取
- 特征提取
- 机器学习算法 
    - 统计方法, 就是数学公式，但是不能处理字符串，就需要将文本类型的字符串转成成数值类型，即需要特征抽取
        - 文本类型转化成数值
        - 类别，类型转成数值，转换成one-hot编码，哑变量
- 为什么需要特征提取：
    - 将任意数据（如文本或图像）转换成可用于机器学习的数字特征（特征值化）
- 特征提取API
    - sklearn.feature_extraction 是一个类
        - feature 特征
        - extraction 提取
    - 不同类型的数据有不同的转换方法
        - 字典特征提取（特征离散化）
        - 文本特征提取
        - 图像特征提取（深度学习）
     

         
### 字典特征提取  

对字典数据进行特征值化

- sklearn.feature_extraction.DictVectorizer(spars=True...) 默认sparse为True
    - vector 向量 矢量 可以用一维数组来存储向量
        - 矩阵matrix 计算机中用二维数组存储 矩阵可以看成由向量构成
    - Dict Vectorizer 字典vectorizer转换成向量的形式，告诉计算机把字典转出成数值了，可以把每一个样本理解为一个向量，n个样本就是n个向量，可以看成是一个二维数组，也可以理解为矩阵
    - 调用了DictVectorizer，相当于实例化了一个转换器对象，父类是一个transfer，转换器类，其中一个方法就是把字典换成成数值  
    - 调用实例化好的对象DictVectorizer，里面有个方法叫fit_transform
    - DictVectorizer.fit_transform(x) x:字典或者包含字典的迭代器   返回值：返回sparse矩阵 稀疏
        - 稀疏矩阵将非0值按位置表现出来，节省内存，可以提高加载运行效率
    - DictVectorizer.fit_transform(x) x:array数组或者sparse矩阵    返回值：转换之前数据格式
        - DictVectorizer.get_feature_names() 返回类别名称
      
字典特征提取
- 对于特征当中存在类别信息的我们都会做one-hot编码处理，哑变量

应用数据：  
字典：  
```
data = [{'city': '北京', 'temperature': 100}, 
        {'city': '上海', 'temperature': 60}, 
        {'city': '深圳', 'temperature': 30}]
output:      
           [[0,1,0,100]
            [1,0,0,60]
            [0,0,1,30]]
```

每一个样本理解为一个向量，一共三行。三个样本，两个特征，是一个三行两列的二维数组/矩阵
原来每个样本有两个特征，再进行转化之后，字典特征抽取之后，样本量不变，特征数量变成了4个。当特征中有类别的时候，表示方法时字符串，想表示为数值并且数值没有比大小，我们就使用ont-hot变量，哑变量。
```python
from sklearn.feature_extraction import DictVectorizer
def dict_demo():
    '''
    字典特征抽取
    :return:
    '''
    data = [{'city': '北京', 'temperature': 100}, {'city': '上海', 'temperature': 60}, {'city': '深圳', 'temperature': 30}]
    #1 实例化一个转换器类
    # sparse 稀疏矩阵
    transform = DictVectorizer(sparse=False)
    # 2 调用fit_transform()方法，里面传字典或者包含字典的迭代器
    data_new = transform.fit_transform(data)
    print("data_new:\n", data_new)
    print("特征名字：",transform.get_feature_names())
    return None
```
#### 字典提取应用场景
- 当我们面对数据集中有类别特征比较多
    - 将数据集的特征转换成字典类型
    - DictVectorizer转换
- 本身拿到的数据类型是字典类型
    
### 文本特征提取
- 特征：特征词
#### 第一个方法
- CountVectorizer
- sklearn.feature_extraction.text.CountVectorizer(stop_words=[])
- 返回词频矩阵 统计每个样本特征词出现的次数
- stop_words 停用词，我们觉得某些词对最终分类没有用处 is/to，以列表的形式传递
- CountVectorizer.fit_transform(x) x:文本或者包含文本字符串的可迭代对象  返回：返回sparse矩阵
    - CountVectorizer在设计的时候没有sparse这个参数，sparse矩阵转化为二维数组 可以用data.toarray()这个方法
    - 中文文档把短语当成一个特征值，处理中文要注意分词
- CountVectorizer.inverse_transform(x) x:array数组或者sparse矩阵 返回值：转换之前数据格
- CountVectorizer.get_feature_names() 返回值：单词列表
- jieba.cut
    - 进行分词处理中文
    - 实例化CountVectorizer
    - 将分词结果变成字符串当做fit_transform的输入值

```python
from sklearn.feature_extraction.text import CountVectorizer
def count_demo():
    '''
    文本特征抽取
    :return:
    '''
    data = ["life is short,i like like python", "life is too long,i dislike python"]
    #实例化一个转换器
    transfer = CountVectorizer(stop_words = ['is','to'])
    #调用fit_transform
    new_data = transfer.fit_transform(data)
    print(new_data,new_data.toarray())
    print("特征名字：",transfer.get_feature_names())
    return None
```
```python
from sklearn.feature_extraction.text import CountVectorizer
def Chinese_demo():
    '''
    中文文档的特征抽取
    :return:
    '''
    data = ["一闪 一闪 亮晶晶，漫天 都 是 小星星"]
    transfer = CountVectorizer()
    data_new = transfer.fit_transform(data)
    print(data_new, data_new.toarray())
    # 这个把短语当成一个特征值，处理中文要注意分词
    print(transfer.get_feature_names())
    return  None

def count_Chinese_demo():
    '''
    中文文本特征提取，自动分词
    :return:
    '''
    data = ["一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。",
            "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。",
            "如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"]
    ## 首先要分词处理
    data_new=[]
    for i in data:
        ### 注意是i 不是data
        data_new.append(cut_word_demo(i))
    print(data_new)
    tranfer = CountVectorizer(stop_words=["一种",'因为'])
    data_final = tranfer.fit_transform(data_new)
    print("data_final", data_final.toarray())
    print(data_final)
    print("特征名字：", tranfer.get_feature_names())

    return None

def cut_word_demo(text):
    '''
    用于中文分词
    :return:
    '''
    # 先强转为list的格式，在转换成字符串
    text1 = ' '.join(list(jieba.cut(text)))
    #print(text1)
    #print("...")
    # 返回的是一个字符串
    return ' '.join(list(jieba.cut(text)))
```
```python

def tfidf_demo():
    '''
    用TF-IDF的方法进行文本特征抽取
    :return:
    '''
    data = ["一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。",
            "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。",
            "如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"]
    ## 首先要分词处理
    data_new = []
    for i in data:
        ### 注意是i 不是data
        data_new.append(cut_word_demo(i))
    #print(data_new)
    tranfer = TfidfVectorizer(stop_words=["一种", '因为'])
    data_final = tranfer.fit_transform(data_new)
    print(data_final)
    print("data_final", data_final.toarray())
    print("特征名字：", tranfer.get_feature_names())

    return None
```

#### 第二个方法    
- TfidfVectorizer   
- 关键词：在某个类别的文章中，出现的次数很多，但是在别的类别的文章中出现的很少
- TF-IDF的主要思想是：如果某个词或者短语在一篇文章中出现的概率并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类
- TF-IDF作用：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度
- 公式：
    - 词频：term frequency，tf 值的是某一个给定的词语在该文件中出现的频率
    - 逆向文档频率：inverse document frequency idf 是一个词语普遍重要性的度量。某一特定词语的idf，可以由总文件数目除以包含该该词语之文件的数目，再将得到的商取10为底的对数得到
    - tfidfij = tfij * idfi
    - 例子： 两个词 '经济''非常' 1000篇文章作为语料库 100篇文章都有非常 10篇文章有经济，现在有两篇文章AB，AB各100个词; A中10次经济，B中10次非常。计算TF-IDF的值。
        - A:tf=10/100     B:tf=10/100
        - A:idf log(1000/10)=2   B:idf log(1000/100)=1
        - A: TF-IDF 0.2 B: TF-IDF 0.1
- API：sklearn.feature_extraction.text.TfidfVectorizer(stop_words=None...)
    - 返回词的权重矩阵
        - TfidfVectorizer.fit_transform(x)
            - x:文本或者包含文本字符串的可迭代对象
            - 返回值: 返回sparse矩阵
        - TfidfVectorizer.inverse_transform(x)
            - x: array数组或者sparse矩阵
            - 返回值: 转换之前数据格式
        - TfidfVectorizer.get_feature_names()
            - 返回值：单词列表
```python
from sklearn.feature_extraction.text import TfidfVectorizer
def tfidf_demo():
    '''
    用TF-IDF的方法进行文本特征抽取
    :return:
    '''
    data = ["一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。",
            "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。",
            "如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"]
    ## 首先要分词处理
    data_new = []
    for i in data:
        ### 注意是i 不是data
        data_new.append(cut_word_demo(i))
    #print(data_new)
    tranfer = TfidfVectorizer(stop_words=["一种", '因为'])
    data_final = tranfer.fit_transform(data_new)
    print(data_final)
    print("data_final", data_final.toarray())
    print("特征名字：", tranfer.get_feature_names())

    return None
```

