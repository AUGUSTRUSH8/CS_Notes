# 随机森林
## 1集成学习方法 Ensemble learning
- 集成学习通过建立几个模型组合来解决单一预测问题。他的工作原理就是生成多个分类器/模型。各自独立地学习和作出预测。这些预测最后结合成组合预测，因此优于任何一个单分类的作出预测
- 目的: 让机器学习效果更好

### 1.1Bagging模型
- bootstrap aggregation 并行训练一堆分类器
- 最典型代表: 随机森林 random forest simplified
- 随机: 数据采样随机，特征选择随机
    - 之所以要进行随机，要保证泛化能力，如果每棵树都一样，那样就没有意义了
- 森林: 很多个决策树并行放在一起
- bagging: 训练多个分类器取平均   
![集成算法bagging](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E9%9B%86%E6%88%90%E7%AE%97%E6%B3%95bagging.png)

- 前人的经验，现在提到集成模型，可以默认为是**树模型**
- 理论上越多数的效果会越好，但是实际上基本超过一定数量就差不多上下浮动了

#### 1.1.1为什么采用bootstrap抽样
随机有放回抽样 这样每一个训练集都是独有的
- 为什么要随机抽样训练集
    - 如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的
- 为什么要有放回的抽样
    - 如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是有偏的，都是绝对片面的，每棵树训练出来都是有很大的差异，而随机森林最后分类取决于多棵树(弱分类器)的投票表决


### 1.2boosting: 从弱学习器可以加强，通过加权进行训练  
![集成算法boosting](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E9%9B%86%E6%88%90%E7%AE%97%E6%B3%95boosting.png)  

### 1.3stacking: 聚合多个分类或回归模型(可以分阶段来做)

## 2什么是随机森林
- 集成学习方法中的一个，在机器学习中，随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别数输出的类别的**众数**而定
- 由于二重随机性，使得每个树基本上都不会一样，最终的结果也会不一样
- 例如，如果你训练5个树，其中4个树的结果是True，一个树的结果是False，那么最终结果就是True

## 3随机森林原理过程
- 训练集随机，特征值随机
- 用N来表示训练用树(样本)的个数，M表示特征数目
    - 一次随机选出一个样本，重复N次，有可能出现重复的样本
    - 随机去选m个特征，m<<M，建立决策树，可以起到降维的结果
- 采取bootstrap抽样


## 4API
- **sklearn.ensemble.RandomForestClassifier(n_estimators=10,criterion='gini',max_depth=None,bootstrap=True,random_state=None,min_sample_split=2)**
- n_estimators: integer,optional(default=10) 森林里面数目的数量
- criterion:string，可选(default='gini') 分割特征的测量方法
- max_depth: integer或None，可选(默认=无) 树的最大深度 5，8，15，25，30
- bootstrap: boolean,optional(default=True) 是否在构建树是放回抽样
- max_features='auto',每个决策树最大特征数量 m
    - if auto, then max_features=sqrt(n_features)
    - if sqrt, then max_features(n_features) same as auto
    - if log2 then max_features=log2(n_features)
    - if None, then max_features=n_features m=M 达不到降维的效果，每棵树的时间很长
- min_samples_split:节点划分最少样本数
- min_samples_leaf:叶子节点的最小样本数
- 超参数:n_estimators,max_depth,min_samples_split,min_samples_leaf

## 5随机森林案例
- 泰坦尼克号案例分析

## 6随机森林总结
- 能够处理很高维度(feature很多)的数据，并且不用做特征选择；能够有效的运行在大数据集上，处理具有高维特征的输入样本，而且不需要降维
- 在训练完后，能够给出哪些feature比较重要，能够评估各个特征在分类问题上的重要性
- 容易做成并行化方法，速度比较快
- 可以进行可视化展示，便于分析
- 在当前所有算法中，具有极好的准确率
