## 特征降维

### 什么是降维
- ndarray 
        - 维数: 嵌套的层数
        - 0维 具体的数，标量
        - 1维 向量
        - 2维 矩阵
        - 3维 嵌套三次 多个2维数组嵌套而成
        - n维
- 降维是指在某些限定条件下，降低随机变量(特征)个数，得到一组"不相关"主变量的过程
    - 降低的是列的个数，特征的个数
    - 降低随机变量的个数
    - 效果要求特征与特征之间不相关
- 相关特性： correlated feature
    - 相对湿度与降雨量之间的相关（相关性大）
    - 相关特征太多会导致信息冗余
- 因为在进行训练的时候，我们都是使用特征进行学习，如果特征本身存在问题或者特征之间相关性较强，对于算法学习预测会影响很大
- 降维的两种方式
    - 特征选择
    - 主成分分析

### 特征选择
- 数据中包含冗余或者相关变量(或称特征，属性，指标等)，旨在从原有特征中找出主要特征
- 方法：
    - Filter(过滤式):主要探究特征本身特点，特征与特征和目标值之间关联
        - 方差选择法：低方差特征过滤 
            - 方差少，数据比较集中
        - 相关系数
            - 可以衡量两个特征之间是否有很强的相关性
    - Embedded(嵌入式):算法自动选择特征（特征与目标值之间的关联）
        - 决策树：信息熵，信息增益
        - 正则化：L1,L2
        - 深度学习：卷积等
- API:sklearn.feature_selection

#### 过滤式
- 低方差特征过滤
    - 删除低方差的一些特征，再结合方差的大小来考虑这个方式的角度
        - 特征方差小：某个特征大多样本的值比较相近 适合删除
        - 特征方差大：某个特征很多样本的值都有差别 适合保留
- API
    - sklearn.feature_selection.VarianceThreshold(threshold=0.0)
        - 删除所有低方差特征
        - Variance.fit_transform(x)
            - x:numpy array 格式的数据[n_sample,n_features]
            - 返回值:训练集差异低于threshold的特征将会被删除，默认值是保留所有非零方差特征，即删除所有样本重具有相同值的特征
        - 初始化VarianceThreshold 指定方差，调用fit_transform
```python
from sklearn.feature_selection import VarianceThreshold
def variance_demo():
    '''
    过滤低方差特征
    :return:
    '''
    data = pd.read_csv("factor_returns.csv")
    data = data.iloc[:,1:-2]
    transfer = VarianceThreshold(threshold=10)
    data_new = transfer.fit_transform(data)
    print(data_new,data_new.shape)
    # 计算某两个变量之间的相关系数
    r1 = pearsonr(data["pe_ratio"],data["pb_ratio"])
    print("相关系数",r1)

    r2 = pearsonr(data["revenue"],data["total_expense"])
    print("相关系数",r2)
    return  None
```

#### 相关系数
- 皮尔逊相关系数(Pearson Correiation Coefficient)
    - 反映变量之间相关关系密切程度的统计指标
- 特点：
- 相关系数的值介于-1与+1之间，即-1<=r<=+1
    - 当r>0时，表示两变量正相关，r<0,表示两个变量为负相关
    - 当|r|=1时，表示两变量为完全相关，当r=0，表示两变量之间无相关关系
    - 当0<|r|<1时，表示两变量存在一定程度的相关，当|r|越接近1，两变量间线性关系越密切；当|r|越接近0，表示两变量的线性关系越弱
    - 一般可以按三级划分：当|r|<0.4为低度相关；0.4<=|r|<0.7为显著性相关；0.7<=|r|<1为高度线性相关
- API
    - from scipy.stats import pearsonr
        - x:(N,) array_like
        - y:(N,) array_like Return:(Pearson's correlation coefficient,p-value)
- 特征与特征之间相关性很高：
    - 选择其中一个作为代表
    - 按一定权重加权求和
    - 主成分分析 自动处理将相关性比较强的特征

### 主成分分析PCA
- 可以理解一种特征提取的方式，降维并尽可能的保持原有信息，减少信息的损失
- 定义：高维数据转化成低维数据的过程，在此过程中可以回舍弃原有数据，创造新的数据
- 作用：是数据维度压缩，尽可能降低原数据的维数(复杂度)，损失少量信息
- 应用：回归分析或者聚类分析当中
- 二维降到一维
    - 找到一个合适的直线，通过一个矩阵运算得出主成分分析的结果
- sklearn.decomposition.PCA(n_components=None)
    - 将数据分解为较低维数空间
    - n_components:
        - 小数：表示保留百分之多少的信息
        - 整数：减少到多少特征
    - PCA.fit_transform(x) 
        - x: numpy array格式的数据
        - [n_samples, n_features]
    - 返回值：转换后指定维度的array
```python
from sklearn.decomposition import PCA
def PCA_demo():
    '''
    PCA降维
    :return:
    '''
    data = [[2, 8, 4, 5], [6, 3, 0, 8], [5, 4, 9, 1]]
    #实例化一个转化器类
    transfer = PCA(n_components=0.95)
    #调用方法
    data_new = transfer.fit_transform(data)
    print(data_new)
    return None
```
