# 特征降维

## 1什么是降维
- ndarray 
        - 维数: 嵌套的层数
        - 0维 具体的数，标量
        - 1维 向量
        - 2维 矩阵
        - 3维 嵌套三次 多个2维数组嵌套而成
        - n维
- 降维是指在某些限定条件下，**降低随机变量(特征)个数**，得到一组"不相关"**主变量**的过程
    - 降低的是**列的个数**，**特征的个数**
    - 降低随机变量的个数
    - 效果要求**特征**与**特征**之间不相关

![降维](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4PCA/%E9%99%8D%E7%BB%B4.png)


- 相关特性： correlated feature
    - 相对湿度与降雨量之间的相关（相关性大）
    - 相关特征太多会导致信息冗余
- 因为在进行训练的时候，我们都是使用特征进行学习，如果特征本身存在问题或者特征之间相关性较强，对于算法学习预测会影响很大

## 2降维的两种方式
- 特征选择
- 主成分分析

## 3特征选择
- 数据中包含冗余或者相关变量(或称特征，属性，指标等)，旨在从原有特征中找出主要特征
- **API:sklearn.feature_selection**

### 3.1Filter(过滤式)
- 主要探究特征**本身特点**，**特征与特征和目标值之间**关联
    - **方差选择法**：**低方差特征过滤**
        - **方差小**，**数据比较集中**
        - 自己这一列数据里面的数据互相比较，方差小说明这列数据可以不考虑
    - **相关系数**
        - 可以衡量两个特征之间是否有很强的相关性

### 3.2Embedded(嵌入式)
- 算法自动选择特征（特征与目标值之间的关联）
    - 决策树：信息熵，信息增益
    - 正则化：L1,L2
    - 深度学习：卷积等


## 4过滤式
### 4.1低方差特征过滤
    - 删除低方差的一些特征，再结合方差的大小来考虑这个方式的角度
        - 特征方差小：某个特征大多样本的值比较相近 适合删除
        - 特征方差大：某个特征很多样本的值都有差别 适合保留

#### 4.1.1低方差特征过滤API
- **sklearn.feature_selection.VarianceThreshold(threshold=0.0)**
    - threshold=0.0 设置一个临界值，低于临界值就删除，默认方差为0
    - 删除所有低方差特征

##### 4.1.2调用
- Variance.fit_transform(x)
    - x:numpy array 格式的数据[n_sample,n_features]
    - 返回值:训练集差异低于threshold的特征将会被删除，默认值是保留所有非零方差特征，即删除所有样本重具有相同值的特征

##### 4.1.3步骤
- 初始化VarianceThreshold 指定方差，
- 调用fit_transform

```python
from sklearn.feature_selection import VarianceThreshold
data = pd.read_csv("factor_returns.csv")
print('data原有的shape',data.shape)
data = data.iloc[:,1:-2]
transfer = VarianceThreshold(threshold=10)
data_new = transfer.fit_transform(data)
print(data_new)
print('data现在的shape',data_new.shape)
```
- 原来是9个特征，现在是7个特征
```
output:
data原有的shape (2318, 12)
[[  5.95720000e+00   8.52525509e+10   8.00800000e-01 ...,   1.21144486e+12
    2.07014010e+10   1.08825400e+10]
 [  7.02890000e+00   8.41133582e+10   1.64630000e+00 ...,   3.00252062e+11
    2.93083692e+10   2.37834769e+10]
 [ -2.62746100e+02   5.17045520e+08  -5.67800000e-01 ...,   7.70517753e+08
    1.16798290e+07   1.20300800e+07]
 ..., 
 [  3.95523000e+01   1.70243430e+10   3.34400000e+00 ...,   2.42081699e+10
    1.78908166e+10   1.74929478e+10]
 [  5.25408000e+01   3.28790988e+10   2.74440000e+00 ...,   3.88380258e+10
    6.46539204e+09   6.00900728e+09]
 [  1.42203000e+01   5.91108572e+10   2.03830000e+00 ...,   2.02066110e+11
    4.50987171e+10   4.13284212e+10]]
data现在的shape (2318, 7)
```

### 4.2相关系数
- 皮尔逊相关系数(Pearson Correiation Coefficient)

![皮尔逊相关系数](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4PCA/%E7%9A%AE%E5%B0%94%E9%80%8A%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0.png)

- 反映变量之间相关关系密切程度的统计指标
- 特点：
- 相关系数的值介于-1与+1之间，即-1<=r<=+1
    - 当r>0时，表示两变量正相关，r<0,表示两个变量为负相关
    - 当|r|=1时，表示两变量为完全相关，当r=0，表示两变量之间无相关关系
    - 当0<|r|<1时，表示两变量存在一定程度的相关，当|r|越接近1，两变量间线性关系越密切；当|r|越接近0，表示两变量的线性关系越弱
    - 一般可以按三级划分：当|r|<0.4为低度相关；0.4<=|r|<0.7为显著性相关；0.7<=|r|<1为高度线性相关


#### 4.2.1相关系数API
- **from scipy.stats import pearsonr**
    - x:(N,) array_like
    - y:(N,) array_like Return:(Pearson's correlation coefficient,p-value)

- 特征与特征之间相关性很高：
    - 选择其中一个作为代表
    - 按一定权重加权求和 作为一个新的特征，删除之前的特征
    - 主成分分析 自动处理将相关性比较强的特征
```python
from sklearn.feature_selection import VarianceThreshold
from scipy.stats import pearsonr
data = pd.read_csv("factor_returns.csv")
print('data原有的shape',data.shape)
data = data.iloc[:,1:-2]
transfer = VarianceThreshold(threshold=10)
data_new = transfer.fit_transform(data)
print(data_new)
print('data现在的shape',data_new.shape)
# 计算某两个变量之间的相关系数
r1 = pearsonr(data["pe_ratio"],data["pb_ratio"])
print("相关系数 r1",r1)
r2 = pearsonr(data["revenue"],data["total_expense"])
print("相关系数 r2",r2)
```

```
output:
相关系数 r1 (-0.0043893227799362737, 0.83272054965913767)
相关系数 r2 (0.99584504131361107, 0.0)
```


## 4主成分分析PCA

![主成分分析](https://raw.githubusercontent.com/mayu1031/CS_Notes/master/doc/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4PCA/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90.png)
- 可以理解一种特征提取的方式，降维并尽可能的保持原有信息，减少信息的损失
- 定义：**高维数据转化成低维数据的过程**，在此过程中**可以回舍弃原有数据**，**创造新的数据**
- 作用：**是数据维度压缩，尽可能降低原数据的维数(复杂度)，损失少量信息**
- 应用：回归分析或者聚类分析当中
- 二维降到一维
    - 找到一个合适的直线，通过一个矩阵运算得出主成分分析的结果

### 4.1主成分分析PCA API
- **sklearn.decomposition.PCA(n_components=None)**
    - 将数据分解为较低维数空间
    - n_components:
        - 小数：表示保留百分之多少的信息
        - 整数：减少到多少特征

### 4.2调用
- **PCA.fit_transform(x)**
    - x: numpy array格式的数据
    - [n_samples, n_features]
- 返回值：转换后指定维度的array

```python
from sklearn.decomposition import PCA
# 3行4列
data = [[2, 8, 4, 5], [6, 3, 0, 8], [5, 4, 9, 1]]
#实例化一个转化器类
transfer = PCA(n_components=0.95)
#调用方法
data_new = transfer.fit_transform(data)
print(data_new)
```
- 4个特征降到类2个特征
```
output:
# 3行2列
[[  1.22879107e-15   3.82970843e+00]
 [  5.74456265e+00  -1.91485422e+00]
 [ -5.74456265e+00  -1.91485422e+00]]
```
